[
  {
    "path": "posts/2024-02-28-lmcategorical/",
    "title": "Simple Linear Regression with Categorical Predictor in R",
    "description": "Exploring how one categorical predictor affects a numeric outcome, is a fancy way to say: comparing several groups. We could use ANOVA for it, but simple linear regression delivers more results. Let's find out how.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2024-03-06",
    "categories": [
      "videos",
      "statistics",
      "visualization"
    ],
    "contents": "\n\nContents\nThis post as ca. ‚Ä¶ minutes video\nClean the data, otherwise: shit in shit out!\nBuild linear equation\nCheck all model assumptions visually\nVisualize predictions\nVisualize estimates\nSummary function\nGet publication ready table\nGet effect sizes\nReport model results\nWhat‚Äôs next?\nShould we categorize numerical predictors?\nAdvantages\nDisadvantages\n\n\nThis post as ca. ‚Ä¶ minutes video\n\n\n\n\n\n\n\nClean the data, otherwise: shit in shit out!\n\n\n\nFirst of all, it‚Äôs crucial to visualize the data before modeling, because the real-world data often contains contaminations üí©. For instance, in Wage dataset from the ISLR package, I would remove the high-income individuals earning over 250K per year, because they are outliers, and totally not because I am jealous! üòâ. The data-cleaning step is vital and often overlooked. Remember the phrase: ‚ÄòGarbage in, garbage out‚Äô? If we skip data cleaning, our model‚Äôs performance could be seriously compromised, and we‚Äôll see it in a moment. For now let‚Äôs take 100 random people form the industry and build our linear model around them.\n\n\nlibrary(tidyverse)      # laod & thank me later ;)\nlibrary(ISLR)           # provides Wage dataset\ntheme_set(theme_test()) # beautifies plots\n\nset.seed(1)             # for reproducibility\nd <- Wage %>% \n  filter(wage < 250) %>% \n  group_by(education) %>% \n  sample_n(100)\n\n\nBuild linear equation\nTo build our linear model, we‚Äôll use the intuitive ‚Äòlm‚Äô (show pack of cigarettes ; —Å —Ä–µ–∫–ª–∞–º–æ–π –∏–º–ø–æ—Ç–µ–Ω—Ü–∏–∏ :) function. Within ‚Äòlm‚Äô, we only need two arguments: the formula and the data. Here‚Äôs how it breaks down:\nOn the left side of the formula, we‚Äôll place the variable we‚Äôre interested in predicting. Unfortunately, this variable goes by several names: response variable, outcome, dependent variable, target, and more. It can get confusing!\nOn the right side, we‚Äôll place the predictor variable. Predictors also have various synonyms: independent variable, explanatory variable, regressor or covariate (in linear regression), feature (in Machine Learning), and even risk factor (in epidemiology).\nUgh, so many names for the same thing! Drives me nuts. Makes stats seem way harder than it actually is.\n\n\n# build the model\nm <- lm(formula = wage ~ education, data = d)\n\n\nCheck all model assumptions visually\nAfter building the model, we need to make sure the assumptions are satisfied. Otherwise, we couldn‚Äôt trust our model! The {performance} package offers intuitive and powerful functions to do that, for instance\n‚Äúcheck_posterior_predictions()‚Äù model fits the data well\n‚Äúcheck_normality()‚Äù looks whether the residuals are normally distributed and\n‚Äúcheck_homogeneity()‚Äù compares variances between groups.\n\n\n# check model assumptions visually\nlibrary(performance)   # extra video on my channel\na <- check_posterior_predictions(m) %>% plot()\nb <- check_normality(m) %>% plot()\nc <- check_homogeneity(m) %>% plot()\n\nlibrary(patchwork)     # extra video on my channel\na + b + c\n\n\n\nLooking at the assumptions, I could say that the model fits the data pretty well! The residuals are normally distributed since they are inside the confidence intervals, which means our data is normally distributed. And finally, the variance is relatively similar.\nBy the way: with categorical predictor we don‚Äôt need to check the linearity and outliers assumptions, and we can relax the independence of observations assumptions, since we don‚Äôt have repeated measures.\nNow, when we model uncleaned data, the model fit would be far from reality, because those rich folks I am totally not jealous about ;) would\ncompromised the model fit\nskew the normality of residuals and\nadd huge heterogenety between groups, in other words would make the groups variance differ,\n‚Ä¶ so that I would stop trusting my model. Therefore, it‚Äôs really important to clean the data before modeling.\n\n\nm2 <- lm(formula = wage ~ education, data = Wage)\n\na <- check_posterior_predictions(m2) %>% plot()\nb <- check_normality(m2) %>% plot()\nc <- check_homogeneity(m2) %>% plot()\n\na + b + c\n\n\n\nOh, and by the way, if you have a lot of data, it‚Äôs important to check assumptions visually, like we just did, instead of conducting statistical tests. Because, the tests would often show significant results, meaning that assumptions seemingly would not be satisfied, while they are actually satisfied. Here is an example of the Shapiro-Wilk normality test, which magically finds non-normally distributed residuals, even though they are totally fine!\n\n\n# don't trust statistical tests to much\ncheck_normality(m)\n\nWarning: Non-normality of residuals detected (p = 0.041).\n\nNow, since the assumptions of our model are satisfied, we can visualize model results, and by results, I mean visualize model predictions.\nVisualize predictions\nFor that, we‚Äôll use another very intuitive function called ‚Äúplot_model()‚Äù from the {sjPlot} package and provide three arguments:\nthe model name,\nthe type of predictions ‚Äî we‚Äôll use ‚Äúeffect‚Äù to get an effect plot, and\nthe name of a predictor we want to visualize.\n\n\n# visualize predictions\nlibrary(sjPlot)    # extra video on my channel\n\nplot_model(m, type = \"eff\", terms = \"education\") # show.data = T\n\n\n\nAnd voil√†, our plot shows that salaries increase with increasing levels of education. Nice right? But this plot misses two important details: it doesn‚Äôt display the differences in salaries between groups, and it doesn‚Äôt demonstrate whether these differences are significant.\nVisualize estimates\nBut we can quickly solve both problems, when we use\n‚Äúshow.values = TRUE‚Äù instead of ‚Äútype =‚Äùeff‚Äù in the ‚Äúplot_model()‚Äù function and\nwidth argument instead of predictor name.\n\n\n# visualize estimates\nplot_model(m, show.values = TRUE, width = 0.2)\n\n\n\nNamely, it tells us that we have a 1080$ increase in salary per year, and this increase is highly significant because the estimate doesn‚Äôt cross zero. And while the significance stars are often enough, sometimes we need an exact p-value. To obtain this, we‚Äôll use the ‚Äútab_model()‚Äù function from the same {sjPlot} package. This function produces a nice, publication-ready table that not only provides exact p-values but also reveals the equation of our linear model:\nSummary function\n\n\nsummary(m)\n\n\nCall:\nlm(formula = wage ~ education, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-84.46 -16.56  -2.13  16.04  95.09 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   86.186      2.810  30.674  < 2e-16 ***\neducation2. HS Grad            9.949      3.974   2.504   0.0126 *  \neducation3. Some College      18.359      3.974   4.620 4.89e-06 ***\neducation4. College Grad      29.353      3.974   7.387 6.43e-13 ***\neducation5. Advanced Degree   46.185      3.974  11.623  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 28.1 on 495 degrees of freedom\nMultiple R-squared:  0.2458,    Adjusted R-squared:  0.2397 \nF-statistic: 40.34 on 4 and 495 DF,  p-value: < 2.2e-16\n\nWe could use a well known summary table for that, but the output, although useful, is not really pleasing to the human eye and is not suitable for publication.\nThere are tons of videos about summary function out there. So, I don‚Äôt explain this in detail here. And I honestly don‚Äôt know why it is soo famous and soo many videos are made about it. And here is what summary function misses to deliver:\nHave you ever wondered, why a ‚Äúsummary‚Äù function compares all categories of a categorical predictor to only the reference category, without comparing categories to each other?\nAnd what about those mysterious slopes with standard errors we get as model coefficients instead of the averages per category with 95% CIs that we actually want?\nMoreover, ‚Äúsummary‚Äù function doesn‚Äôt adjust p-values for multiple comparisons, which increases the probability of discovering nonsense by making too many type-I errors.\nSummary function doesn‚Äôt plot the results of a model and\nmakes it almost impossible to interpret interactions!\nSo, if you‚Äôve ever been frustrated due to similar issues, you‚Äôre definitely not alone! The ‚Äúsummary‚Äù function doesn‚Äôt actually provide a very useful summary, and that‚Äôs why we need {emmeans} package, which solves all those problems.\nPost-Hoc tests.\nA much better choice is the ‚Äúemmeans‚Äù function from the very powerful {emmeans} package. It not only provides predictions for any education you desire, but also reports 95% confidence intervals and offers a wide range of additional capabilities.\n\n\nlibrary(emmeans)   # extra 2 videos on my channel ;)\nresults <- emmeans(m, pairwise ~ education, infer = T)\nresults\n\n$emmeans\n education          emmean   SE  df lower.CL upper.CL t.ratio p.value\n 1. < HS Grad         86.2 2.81 495     80.7     91.7  30.674  <.0001\n 2. HS Grad           96.1 2.81 495     90.6    101.7  34.215  <.0001\n 3. Some College     104.5 2.81 495     99.0    110.1  37.208  <.0001\n 4. College Grad     115.5 2.81 495    110.0    121.1  41.121  <.0001\n 5. Advanced Degree  132.4 2.81 495    126.9    137.9  47.112  <.0001\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                             estimate   SE  df lower.CL\n 1. < HS Grad - 2. HS Grad               -9.95 3.97 495    -20.8\n 1. < HS Grad - 3. Some College         -18.36 3.97 495    -29.2\n 1. < HS Grad - 4. College Grad         -29.35 3.97 495    -40.2\n 1. < HS Grad - 5. Advanced Degree      -46.18 3.97 495    -57.1\n 2. HS Grad - 3. Some College            -8.41 3.97 495    -19.3\n 2. HS Grad - 4. College Grad           -19.40 3.97 495    -30.3\n 2. HS Grad - 5. Advanced Degree        -36.24 3.97 495    -47.1\n 3. Some College - 4. College Grad      -10.99 3.97 495    -21.9\n 3. Some College - 5. Advanced Degree   -27.83 3.97 495    -38.7\n 4. College Grad - 5. Advanced Degree   -16.83 3.97 495    -27.7\n upper.CL t.ratio p.value\n    0.930  -2.504  0.0914\n   -7.480  -4.620  <.0001\n  -18.474  -7.387  <.0001\n  -35.306 -11.623  <.0001\n    2.469  -2.117  0.2146\n   -8.525  -4.883  <.0001\n  -25.357  -9.119  <.0001\n   -0.114  -2.767  0.0462\n  -16.947  -7.003  <.0001\n   -5.953  -4.236  0.0003\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 5 estimates \nP value adjustment: tukey method for comparing a family of 5 estimates \n\nWe could even quickly plot contrasts in order to see where the difference is the biggest. For instance, naturally the biggest difference is between two lowest categories, HS Grad and below, vs.¬†the highest categorie, Advanced Degree.\n\n\nresults$contrasts %>% plot() \n\n\n\nBut as much as I love {emmeans} package, it is also not perfect. While it can produce a publication ready with some coding effort, it‚Äôs not as easy and quick, as with two other options we‚Äôll learn next.\nGet publication ready table\nThe first one is ‚Äútab_model‚Äù function from {sjPlot} package. It takes only three intuitive arguments to produce a basic table, where all categories are compared to the reference levels.\n\n\ntab_model(m, \n          show.reflvl = T, \n          show.intercept = F, \n          p.style = \"numeric_stars\")\n\n\n¬†\n\n\nwage\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n< HS Grad\n\n\nReference\n\n\n\n\n\n\nHS Grad\n\n\n9.95 *\n\n2.14¬†‚Äì¬†17.76\n\n\n0.013\n\n\nSome College\n\n\n18.36 ***\n\n10.55¬†‚Äì¬†26.17\n\n\n<0.001\n\n\nCollege Grad\n\n\n29.35 ***\n\n21.55¬†‚Äì¬†37.16\n\n\n<0.001\n\n\nAdvanced Degree\n\n\n46.18 ***\n\n38.38¬†‚Äì¬†53.99\n\n\n<0.001\n\n\nObservations\n\n\n500\n\n\nR2 / R2 adjusted\n\n\n0.246 / 0.240\n\n\np<0.05¬†¬†¬†** p<0.01¬†¬†¬†*** p<0.001\n\n\nBut I never understood, why people avoid more inference by limiting themselfes to only one reference category, while this inference was already calculated by the model. What if we want to compare all categories to each other pairwisely? So, that the second option is to produce a table with contrasts using the ‚Äútbl_regression‚Äù function from {gtsummary} package, which also take only few intuitive arguments. Where Beta is the difference in thousands of dollars.\n\n\nlibrary(gtsummary)\nfancy_table <- tbl_regression(m, add_pairwise_contrasts = T) %>% \n   add_significance_stars(hide_p = F, hide_se = T, hide_ci = F)\n\nfancy_table\n\n\nCharacteristic\n      Beta1\n      95% CI2\n      p-value\n    education\n\n\n¬†¬†¬†¬†2. HS Grad - 1. < HS Grad\n10\n-0.93, 21\n0.091¬†¬†¬†¬†3. Some College - 1. < HS Grad\n18***\n7.5, 29\n¬†¬†¬†¬†3. Some College - 2. HS Grad\n8.4\n-2.5, 19\n0.2¬†¬†¬†¬†4. College Grad - 1. < HS Grad\n29***\n18, 40\n¬†¬†¬†¬†4. College Grad - 2. HS Grad\n19***\n8.5, 30\n¬†¬†¬†¬†4. College Grad - 3. Some College\n11*\n0.11, 22\n0.046¬†¬†¬†¬†5. Advanced Degree - 1. < HS Grad\n46***\n35, 57\n¬†¬†¬†¬†5. Advanced Degree - 2. HS Grad\n36***\n25, 47\n¬†¬†¬†¬†5. Advanced Degree - 3. Some College\n28***\n17, 39\n¬†¬†¬†¬†5. Advanced Degree - 4. College Grad\n17***\n6.0, 28\n1 *p<0.05; **p<0.01; ***p<0.001\n    2 CI = Confidence Interval\n    \n\nlibrary(flextable)\nfancy_table %>%\n  as_flex_table() %>%\n  save_as_docx(path = \"fancy_table.docx\") \n\n\nWhen you need to publish the equation of the model, you could ‚Äúextract_eq‚Äù it from the model via ‚Äúextract_eq‚Äù function from {equatiomatic} package.\n\n\nlibrary(equatiomatic)\nextract_eq(m)\n\n\\[\n\\operatorname{wage} = \\alpha + \\beta_{1}(\\operatorname{education}_{\\operatorname{2.\\ HS\\ Grad}}) + \\beta_{2}(\\operatorname{education}_{\\operatorname{3.\\ Some\\ College}}) + \\beta_{3}(\\operatorname{education}_{\\operatorname{4.\\ College\\ Grad}}) + \\beta_{4}(\\operatorname{education}_{\\operatorname{5.\\ Advanced\\ Degree}}) + \\epsilon\n\\]\n\nThis equation describes the straight-line relationship between the x-axis and the y-axis:\n\\[ y = Œ± + Œ≤x \\]\nOr, in our case, between ‚Äúeducation‚Äù and ‚Äúsalary‚Äù:\n\\[ wage = Estimate_{(Intercept)} + Estimate_{education} * education \\]\n‚Ä¶ and it helps us to interpret our model. Specifically, an increase in education of one year results in an average salary increase of 1080$. It‚Äôs important to note that this increase can vary ‚Äî sometimes it‚Äôs as low as 540$, and other times as high as 1620$, depending on people‚Äôs job roles. The model indicates that there is a 95% chance that the salary increase falls within this interval. That‚Äôs why it‚Äôs called - the 95% Confidence Interval.\nIf this average annual increase of 1080$ occurs consistently, we obtain the slope of the line, represented by the beta (Œ≤) coefficient in the model formula. Put simply, the slope indicates the average change in ‚Äúy‚Äù for every one-unit increase in ‚Äúx‚Äù. As with any slope, it needs a starting point. And the best start of any slope is usually zero. When our line crosses the y-axis (zero on the x-axis), we find the intercept (Œ±) in the model output. In our case, this implies an unrealistic scenario where our salary would be 57,500$ at birth, which doesn‚Äôt make any sense, and that‚Äôs why the intercept of a model is often not interpreted.\nThe true value of the formula lies in its ability to predict future salaries. By plugging any ‚Äúeducation‚Äù value (x) into the model equation, we can estimate the corresponding ‚Äúsalary‚Äù value (y). This means we can ask the model to tell us how much we‚Äôll earn when we‚Äôre 40 or 50.\nGet effect sizes\nNow, that we understand how to interpret our model, the next crucial question we need to address is - how good our model is. Your clients or scientific reviewers will definetely ask this question. What they typically want to know is - how well our model fits the data, which is summarized by the coefficient of determination, denoted as \\(R^2\\).\n\\(R^2\\) quantifies how much variance in the data our model explains. It ranges from 0 to 1, with a simple rule: the higher the value, the better the fit. However, interpreting \\(R^2\\) is context-dependent. For instance:\nin physics, a model with \\(R^2\\) near 1 is usually desirable, while\nin biology, even an \\(R^2\\) of 0.2 might indicate a good model fit.\nDespite this context sensitivity, some guidelines proposed by smart individuals (such as Cohen) exist. And we can ask the {effectsize} package to interpret any value of \\(R^2\\) for us.\n\n\n# get effect sizes\nlibrary(effectsize)\n?interpret_r2\n\ninterpret_r2(0.246, rules = \"cohen1988\") \n\n[1] \"moderate\"\n(Rules: cohen1988)\n\n# rules = \"cohen1988\" is a default\n\n\nIn our case, the \\(R^2\\) value of 0.139 indicates a moderate relationship between education and salary. But why moderate and not strong? Well, that could be because education is most likely not the most crucial predictor for salary üòâ. I mean, when I sit on the sofa, do nothing, and just get older, my salary won‚Äôt grow! So, what is an important predictor? Education is!\n\n\nggplot(Wage, aes(x = education, y = wage, color = jobclass))+\n  geom_boxplot()\n\n\n\nSpecifically, folks without a High School diploma start at 70,000$ as early as 18 years old, while those with an ‚ÄúAdvanced Degree‚Äù begin with a salary of approximately 140,000$ at the education of 25, after completing their studies. However, I‚Äôll cover it in a separate video in this series, where I demonstrate how to interpret a model with a categorical predictor. Until then if you‚Äôre enjoying the video so far, please, consider hitting the like button!\nReport model results\nFinally, the last thing I found difficult was accurately describing the model with sufficient detail for others to understand and reproduce my results. If you have similar problem, the {report} package solves it ‚Äî even if you only use one function to literally ‚Äúreport‚Äù your model.\nNamely, the ‚Äúreport‚Äù function:\nidentifies the type of model you used,\nshows how well the model fits the data (\\(R^2\\)),\ninterprets effect sizes,\ndescribes whether predictors are significant and which direction the slopes go and\neven reports how 95% confidence intervals (CIs) and p-values were calculated.\nI personally found it very useful, but I‚Äôd love to know what you think? So, feel free to share your thoughts in the comments section below.\n\n\nlibrary(report)   # extra video on my channel\nreport(m)\n\nWe fitted a linear model (estimated using OLS) to predict wage with\neducation (formula: wage ~ education). The model explains a\nstatistically significant and moderate proportion of variance (R2 =\n0.25, F(4, 495) = 40.34, p < .001, adj. R2 = 0.24). The model's\nintercept, corresponding to education = 1. < HS Grad, is at 86.19\n(95% CI [80.67, 91.71], t(495) = 30.67, p < .001). Within this model:\n\n  - The effect of education [2. HS Grad] is statistically significant\nand positive (beta = 9.95, 95% CI [2.14, 17.76], t(495) = 2.50, p =\n0.013; Std. beta = 0.31, 95% CI [0.07, 0.55])\n  - The effect of education [3. Some College] is statistically\nsignificant and positive (beta = 18.36, 95% CI [10.55, 26.17], t(495)\n= 4.62, p < .001; Std. beta = 0.57, 95% CI [0.33, 0.81])\n  - The effect of education [4. College Grad] is statistically\nsignificant and positive (beta = 29.35, 95% CI [21.55, 37.16], t(495)\n= 7.39, p < .001; Std. beta = 0.91, 95% CI [0.67, 1.15])\n  - The effect of education [5. Advanced Degree] is statistically\nsignificant and positive (beta = 46.18, 95% CI [38.38, 53.99], t(495)\n= 11.62, p < .001; Std. beta = 1.43, 95% CI [1.19, 1.68])\n\nStandardized parameters were obtained by fitting the model on a\nstandardized version of the dataset. 95% Confidence Intervals (CIs)\nand p-values were computed using a Wald t-distribution approximation.\n\n\n\nmodel_performance(m)\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |   RMSE |  Sigma\n--------------------------------------------------------------------\n4761.586 | 4761.756 | 4786.874 | 0.246 |     0.240 | 27.956 | 28.097\n\nWhat‚Äôs next?\nWhat do we do, when assumptions are not satisfied? Well, while there are several options, like robust or bootstrapping regression, I personally enjoy the quantile regression a lot. So, if you wanna be equepped for more situations and be statistically robust, make sure to check out this video on quantile regression:\n\n\nreport(d)\n\nThe data contains 500 observations, grouped by education, of the\nfollowing 11 variables:\n\n- 1. < HS Grad (n = 100):\n  - year: n = 100, Mean = 2006.01, SD = 1.98, Median = 2006.00, MAD =\n2.97, range: [2003, 2009], Skewness = 0.02, Kurtosis = -1.32, 0%\nmissing\n  - age: n = 100, Mean = 41.27, SD = 11.56, Median = 41.00, MAD =\n10.38, range: [18, 73], Skewness = 0.38, Kurtosis = -7.93e-03, 0%\nmissing\n  - maritl: 5 levels, namely 1. Never Married (n = 21, 21.00%), 2.\nMarried (n = 64, 64.00%), 3. Widowed (n = 0, 0.00%), 4. Divorced (n =\n7, 7.00%) and 5. Separated (n = 8, 8.00%)\n  - race: 4 levels, namely 1. White (n = 82, 82.00%), 2. Black (n = 10,\n10.00%), 3. Asian (n = 6, 6.00%) and 4. Other (n = 2, 2.00%)\n  - region: 9 levels, namely 1. New England (n = 0, 0.00%), 2. Middle\nAtlantic (n = 100, 100.00%), 3. East North Central (n = 0, 0.00%), 4.\nWest North Central (n = 0, 0.00%), 5. South Atlantic (n = 0, 0.00%),\n6. East South Central (n = 0, 0.00%), 7. West South Central (n = 0,\n0.00%), 8. Mountain (n = 0, 0.00%) and 9. Pacific (n = 0, 0.00%)\n  - jobclass: 2 levels, namely 1. Industrial (n = 68, 68.00%) and 2.\nInformation (n = 32, 32.00%)\n  - health: 2 levels, namely 1. <=Good (n = 41, 41.00%) and 2. >=Very\nGood (n = 59, 59.00%)\n  - health_ins: 2 levels, namely 1. Yes (n = 49, 49.00%) and 2. No (n =\n51, 51.00%)\n  - logwage: n = 100, Mean = 4.42, SD = 0.30, Median = 4.40, MAD =\n0.24, range: [3.04, 5.03], Skewness = -1.41, Kurtosis = 5.43, 0%\nmissing\n  - wage: n = 100, Mean = 86.19, SD = 22.70, Median = 81.28, MAD =\n18.77, range: [20.93, 152.22], Skewness = 0.25, Kurtosis = 0.70, 0%\nmissing\n\n- 2. HS Grad (n = 100):\n  - year: n = 100, Mean = 2006.00, SD = 2.02, Median = 2006.00, MAD =\n2.97, range: [2003, 2009], Skewness = -0.02, Kurtosis = -1.29, 0%\nmissing\n  - age: n = 100, Mean = 42.85, SD = 11.76, Median = 43.00, MAD =\n11.86, range: [19, 66], Skewness = -0.05, Kurtosis = -0.85, 0%\nmissing\n  - maritl: 5 levels, namely 1. Never Married (n = 19, 19.00%), 2.\nMarried (n = 70, 70.00%), 3. Widowed (n = 2, 2.00%), 4. Divorced (n =\n8, 8.00%) and 5. Separated (n = 1, 1.00%)\n  - race: 4 levels, namely 1. White (n = 88, 88.00%), 2. Black (n = 8,\n8.00%), 3. Asian (n = 3, 3.00%) and 4. Other (n = 1, 1.00%)\n  - region: 9 levels, namely 1. New England (n = 0, 0.00%), 2. Middle\nAtlantic (n = 100, 100.00%), 3. East North Central (n = 0, 0.00%), 4.\nWest North Central (n = 0, 0.00%), 5. South Atlantic (n = 0, 0.00%),\n6. East South Central (n = 0, 0.00%), 7. West South Central (n = 0,\n0.00%), 8. Mountain (n = 0, 0.00%) and 9. Pacific (n = 0, 0.00%)\n  - jobclass: 2 levels, namely 1. Industrial (n = 62, 62.00%) and 2.\nInformation (n = 38, 38.00%)\n  - health: 2 levels, namely 1. <=Good (n = 38, 38.00%) and 2. >=Very\nGood (n = 62, 62.00%)\n  - health_ins: 2 levels, namely 1. Yes (n = 70, 70.00%) and 2. No (n =\n30, 30.00%)\n  - logwage: n = 100, Mean = 4.53, SD = 0.26, Median = 4.54, MAD =\n0.24, range: [3.90, 5.08], Skewness = -0.15, Kurtosis = -0.44, 0%\nmissing\n  - wage: n = 100, Mean = 96.13, SD = 24.91, Median = 94.00, MAD =\n23.72, range: [49.56, 160.64], Skewness = 0.44, Kurtosis = -0.22, 0%\nmissing\n\n- 3. Some College (n = 100):\n  - year: n = 100, Mean = 2005.77, SD = 1.94, Median = 2006.00, MAD =\n2.22, range: [2003, 2009], Skewness = 0.09, Kurtosis = -1.09, 0%\nmissing\n  - age: n = 100, Mean = 39.42, SD = 11.40, Median = 39.00, MAD =\n13.34, range: [18, 64], Skewness = 0.29, Kurtosis = -0.76, 0% missing\n  - maritl: 5 levels, namely 1. Never Married (n = 29, 29.00%), 2.\nMarried (n = 59, 59.00%), 3. Widowed (n = 0, 0.00%), 4. Divorced (n =\n10, 10.00%) and 5. Separated (n = 2, 2.00%)\n  - race: 4 levels, namely 1. White (n = 87, 87.00%), 2. Black (n = 11,\n11.00%), 3. Asian (n = 1, 1.00%) and 4. Other (n = 1, 1.00%)\n  - region: 9 levels, namely 1. New England (n = 0, 0.00%), 2. Middle\nAtlantic (n = 100, 100.00%), 3. East North Central (n = 0, 0.00%), 4.\nWest North Central (n = 0, 0.00%), 5. South Atlantic (n = 0, 0.00%),\n6. East South Central (n = 0, 0.00%), 7. West South Central (n = 0,\n0.00%), 8. Mountain (n = 0, 0.00%) and 9. Pacific (n = 0, 0.00%)\n  - jobclass: 2 levels, namely 1. Industrial (n = 55, 55.00%) and 2.\nInformation (n = 45, 45.00%)\n  - health: 2 levels, namely 1. <=Good (n = 22, 22.00%) and 2. >=Very\nGood (n = 78, 78.00%)\n  - health_ins: 2 levels, namely 1. Yes (n = 67, 67.00%) and 2. No (n =\n33, 33.00%)\n  - logwage: n = 100, Mean = 4.60, SD = 0.35, Median = 4.62, MAD =\n0.23, range: [3, 5.18], Skewness = -2.01, Kurtosis = 6.96, 0% missing\n  - wage: n = 100, Mean = 104.55, SD = 29.35, Median = 101.82, MAD =\n24.83, range: [20.09, 176.99], Skewness = -0.08, Kurtosis = 0.84, 0%\nmissing\n\n- 4. College Grad (n = 100):\n  - year: n = 100, Mean = 2005.69, SD = 1.99, Median = 2005.50, MAD =\n2.22, range: [2003, 2009], Skewness = 0.14, Kurtosis = -1.25, 0%\nmissing\n  - age: n = 100, Mean = 41.88, SD = 10.68, Median = 43.00, MAD =\n11.86, range: [22, 64], Skewness = 0.03, Kurtosis = -0.86, 0% missing\n  - maritl: 5 levels, namely 1. Never Married (n = 26, 26.00%), 2.\nMarried (n = 64, 64.00%), 3. Widowed (n = 1, 1.00%), 4. Divorced (n =\n9, 9.00%) and 5. Separated (n = 0, 0.00%)\n  - race: 4 levels, namely 1. White (n = 84, 84.00%), 2. Black (n = 6,\n6.00%), 3. Asian (n = 8, 8.00%) and 4. Other (n = 2, 2.00%)\n  - region: 9 levels, namely 1. New England (n = 0, 0.00%), 2. Middle\nAtlantic (n = 100, 100.00%), 3. East North Central (n = 0, 0.00%), 4.\nWest North Central (n = 0, 0.00%), 5. South Atlantic (n = 0, 0.00%),\n6. East South Central (n = 0, 0.00%), 7. West South Central (n = 0,\n0.00%), 8. Mountain (n = 0, 0.00%) and 9. Pacific (n = 0, 0.00%)\n  - jobclass: 2 levels, namely 1. Industrial (n = 34, 34.00%) and 2.\nInformation (n = 66, 66.00%)\n  - health: 2 levels, namely 1. <=Good (n = 23, 23.00%) and 2. >=Very\nGood (n = 77, 77.00%)\n  - health_ins: 2 levels, namely 1. Yes (n = 77, 77.00%) and 2. No (n =\n23, 23.00%)\n  - logwage: n = 100, Mean = 4.71, SD = 0.29, Median = 4.74, MAD =\n0.27, range: [3.90, 5.30], Skewness = -0.70, Kurtosis = 0.33, 0%\nmissing\n  - wage: n = 100, Mean = 115.54, SD = 30.77, Median = 114.48, MAD =\n30.12, range: [49.56, 200.54], Skewness = 0.06, Kurtosis = -0.15, 0%\nmissing\n\n- 5. Advanced Degree (n = 100):\n  - year: n = 100, Mean = 2006.02, SD = 2.08, Median = 2006.00, MAD =\n2.97, range: [2003, 2009], Skewness = 0.01, Kurtosis = -1.32, 0%\nmissing\n  - age: n = 100, Mean = 43.73, SD = 10.29, Median = 42.50, MAD =\n12.60, range: [27, 72], Skewness = 0.27, Kurtosis = -0.74, 0% missing\n  - maritl: 5 levels, namely 1. Never Married (n = 21, 21.00%), 2.\nMarried (n = 72, 72.00%), 3. Widowed (n = 0, 0.00%), 4. Divorced (n =\n7, 7.00%) and 5. Separated (n = 0, 0.00%)\n  - race: 4 levels, namely 1. White (n = 76, 76.00%), 2. Black (n = 9,\n9.00%), 3. Asian (n = 15, 15.00%) and 4. Other (n = 0, 0.00%)\n  - region: 9 levels, namely 1. New England (n = 0, 0.00%), 2. Middle\nAtlantic (n = 100, 100.00%), 3. East North Central (n = 0, 0.00%), 4.\nWest North Central (n = 0, 0.00%), 5. South Atlantic (n = 0, 0.00%),\n6. East South Central (n = 0, 0.00%), 7. West South Central (n = 0,\n0.00%), 8. Mountain (n = 0, 0.00%) and 9. Pacific (n = 0, 0.00%)\n  - jobclass: 2 levels, namely 1. Industrial (n = 31, 31.00%) and 2.\nInformation (n = 69, 69.00%)\n  - health: 2 levels, namely 1. <=Good (n = 13, 13.00%) and 2. >=Very\nGood (n = 87, 87.00%)\n  - health_ins: 2 levels, namely 1. Yes (n = 75, 75.00%) and 2. No (n =\n25, 25.00%)\n  - logwage: n = 100, Mean = 4.86, SD = 0.25, Median = 4.90, MAD =\n0.20, range: [4.03, 5.43], Skewness = -0.64, Kurtosis = 0.86, 0%\nmissing\n  - wage: n = 100, Mean = 132.37, SD = 31.67, Median = 133.97, MAD =\n24.62, range: [56.45, 227.46], Skewness = 0.21, Kurtosis = 0.32, 0%\nmissing\n\nShould we categorize numerical predictors?\nAdvantages\nIncreases interpretability: Sometimes it‚Äôs easier to understand the impact of different predictor levels on the outcome. For instance, instead of saying ‚Äúfor every 1-unit increase in blood pressure,‚Äù we can say ‚Äúfor patients with high blood pressure.‚Äù\nHandles nonlinearity: Categorization captures nonlinear relationships between predictors and outcomes if one exists. By categorizing based on these smooth functions, we can identify risk categories (e.g., low, average, high) more effectively. Segmenting the data into groups can potentially improve model fit and predictive power.\nDecision Support: Categorization simplifies decision-making. For example, a physician might categorize a patient‚Äôs cholesterol level as ‚Äúnormal,‚Äù ‚Äúborderline,‚Äù or ‚Äúhigh.‚Äù\nModel Stability: Categorization can stabilize model estimates. Continuous predictors may introduce noise due to small fluctuations. By grouping similar values together, we reduce the impact of minor variations and enhance model stability.\nOutlier Control: Similar to decision trees, categorization can reduce the impact of outliers in the data. By grouping outliers with other data points, their influence on the model can be mitigated.\nRelaxes Linearity Assumption\nHandling Missing Values: Categorization allows handling missing data by creating a separate category.\nDisadvantages\nInformation Loss: The original numeric relationship is replaced by discrete categories, which may not fully capture the underlying variability.\nSubjective Boundaries (Category Choice): Choosing category boundaries is subjective and can impact model results. Techniques like quantile-based methods, k-means clustering, or domain knowledge-based approaches can be used to create informative categories. However, choosing different binning schemes can lead to varying results.\nInefficient Use of Data: Binning reduces the effective sample size within each category. Smaller sample sizes can lead to less precise estimates.\nIncreased Model Complexity: Categorization introduces additional parameters (dummy variables) into the model. More parameters can lead to overfitting, especially with limited data.\nIn summary, while categorization can simplify modeling, it comes with trade-offs. Consider the context, research question, and data characteristics before deciding whether to categorize numeric predictors in your statistical models.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2024-02-28-lmcategorical/lmcategorical_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2024-03-06T17:28:37+01:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-02-03-lmnumeric/",
    "title": "Master Simple Linear Regression with Numeric Predictor in R",
    "description": "Simple linear regression demonstrates how one numeric predictor affects a numeric outcome. For example, it can reveal whether age actually translates to higher paychecks. So, let's learn (1) how to build a linear regression in R, (2) how to check ALL model assumptions with a ONE simple and intuitive command, (3) how to visualize and interpret the results, and much more.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2024-02-26",
    "categories": [
      "videos",
      "statistics",
      "visualization"
    ],
    "contents": "\n\nContents\nThis post as ca. 12 minutes video\nClean the data, otherwise: shit in shit out!\nBuild linear equation\nCheck all model assumptions visually\nVisualize predictions\nVisualize estimates\nMake specific predictions\nGet effect sizes\nReport model results\nWhat‚Äôs next?\n\nThis post as ca. 12 minutes video\n\n\n\n\n\n\n\nClean the data, otherwise: shit in shit out!\n\n\n\nThe data in this picture comes from the Wage dataset in the ISLR package. It clearly shows that real-world data often contains contaminations üí©. Therefore, it‚Äôs crucial to look at the data first before rushing into modeling. For instance, in this scenario, I would clean the data by removing the high-income individuals earning over 200K per year, because they are outliers, and totally not because I am jealous! üòâ Additionally, I‚Äôd exclude older individuals who are nearing retirement, as their salaries tend to decline. The data-cleaning step is vital and often overlooked. Remember the phrase: ‚ÄòGarbage in, garbage out‚Äô? If we skip data cleaning, our model‚Äôs performance could be seriously compromised, and we‚Äôll see it in a moment. For now let‚Äôs take 100 random people form the industry and build our linear model in R.\n\n\nlibrary(tidyverse)      # laod & thank me later ;)\nlibrary(ISLR)           # provides Wage dataset\ntheme_set(theme_test()) # beautifies plots\n\nset.seed(1)             # for reproducibility\nd <- Wage %>% \n  filter(wage < 200 & age < 60 & jobclass == \"1. Industrial\") %>% \n  sample_n(100)\n\n\nBuild linear equation\nTo build our linear model, we‚Äôll use the intuitive ‚Äòlm‚Äô (show pack of cigarettes ; —Å —Ä–µ–∫–ª–∞–º–æ–π –∏–º–ø–æ—Ç–µ–Ω—Ü–∏–∏ :) function. Within ‚Äòlm‚Äô, we only need two arguments: the formula and the data. Here‚Äôs how it breaks down:\nOn the left side of the formula, we‚Äôll place the variable we‚Äôre interested in predicting. Unfortunately, this variable goes by several names: response variable, outcome, dependent variable, target, and more. It can get confusing!\nOn the fright side, we‚Äôll place the predictor variable. Predictors also have various synonyms: independent variable, explanatory variable, regressor or covariate (in linear regression), feature (in Machine Learning), and even risk factor (in epidemiology).\nUgh, so many names for the same thing! Drives me nuts. Makes stats seem way harder than it actually is.\n\n\n# build the model\nm <- lm(formula = wage ~ age, data = d)\n\n\nCheck all model assumptions visually\nAfter building the model, we need to make sure the assumptions are satisfied. Otherwise, we couldn‚Äôt trust our model! The ‚Äúcheck_model()‚Äù function from the {performance} package is so intuitive and powerful that, once you‚Äôve used it, you cannot unlearn it! Believe me! I use it every time I model.\n\n\n# check model assumptions visually\nlibrary(performance)   # extra video on my channel\ncheck_model(m)\n\n\n\nLooking at the assumptions, I could say that the model fits the data pretty well! The linearity assumption looks fine; the line is not completely straight, but there is no dramatic non-linearity to see. The variance is homogeneous. There are no influential points or outliers. And finally, the residuals are normally distributed since they are inside the confidence intervals, which means our data is normally distributed.\nNow, when we model uncleaned data, the model fit would be far from reality, and we‚Äôll even get a warning that our model misses the maximum values ‚Äî you know, those rich folks I am totally not jealous about ;). Besides, the normality of residuals would also be compromised, so I would stop trusting my model. Therefore, it‚Äôs really important to clean the data before modeling.\n\n\nm2 <- lm(formula = wage ~ age, data = Wage)\nlibrary(patchwork)\ncheck_posterior_predictions(m2)\n\nWarning: Maximum value of original data is not included in the\n  replicated data.\n  Model may not capture the variation of the data.\n\na <- check_posterior_predictions(m2) %>% plot() \nb <- check_normality(m2) %>% plot()\n\na + b\n\n\n\nOh, by the way, if you have a lot of data, it‚Äôs important to check assumptions visually, like we just did, instead of conducting statistical tests. For a lot of data, the tests would often show significant results, meaning that assumptions seemingly would not be satisfied, while they are actually satisfied. Here is an example of the Shapiro-Wilk normality test, which magically finds non-normally distributed residuals, even though they are totally fine!\n\n\n# don't trust statistical tests to much\ncheck_normality(m)\n\nWarning: Non-normality of residuals detected (p = 0.019).\n\nNow, since the assumptions of our model are satisfied, we can visualize model results, and by results, I mean visualize model predictions.\nVisualize predictions\nFor that, we‚Äôll use another very intuitive function called ‚Äúplot_model()‚Äù from the {sjPlot} package and provide three arguments:\nthe model name,\nthe type of predictions ‚Äî we‚Äôll use ‚Äúeffect‚Äù to get an effect plot, and\nthe name of a predictor we want to visualize.\n\n\n# visualize predictions\nlibrary(sjPlot)    # extra video on my channel\n\nplot_model(m, type = \"eff\", terms = \"age\") # show.data = T\n\n\n\nAnd if we really want to, we could easily show the data on our plot:\n\n\nplot_model(m, type = \"eff\", terms = \"age\",  show.data = TRUE)\n\n\n\nAnd voil√†, our plot shows that salaries increase with age. This plot is nice, but it misses two important details: it doesn‚Äôt exactly show how quickly the salary grows, and it doesn‚Äôt demonstrate whether this increase is significant.\nVisualize estimates\nThe ‚Äúplot_model()‚Äù function solves both problems, when we use ‚Äúshow.values = TRUE‚Äù instead of ‚Äútype =‚Äùeff‚Äù.\n\n\n# visualize estimates\nplot_model(m, show.values = TRUE, terms = \"age\")\n\n\n\nNamely, it tells us that we have a 1080$ increase in salary per year, and this increase is highly significant because the estimate doesn‚Äôt cross zero. And while the significance stars are often enough, sometimes we need an exact p-value. To obtain this, we‚Äôll use the ‚Äútab_model()‚Äù function from the same {sjPlot} package. This function produces a nice, publication-ready table that not only provides exact p-values but also reveals the equation of our linear model:\n\n\ntab_model(m)\n\n\n¬†\n\n\nwage\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n57.50\n\n\n35.24¬†‚Äì¬†79.76\n\n\n<0.001\n\n\nage\n\n\n1.08\n\n\n0.54¬†‚Äì¬†1.62\n\n\n<0.001\n\n\nObservations\n\n\n100\n\n\nR2 / R2 adjusted\n\n\n0.139 / 0.130\n\n\nThis equation describes the straight-line relationship between the x-axis and the y-axis:\n\\[ y = Œ± + Œ≤x \\]\nOr, in our case, between ‚Äúage‚Äù and ‚Äúsalary‚Äù:\n\\[ wage = Estimate_{(Intercept)} + Estimate_{age} * age \\]\n‚Ä¶ and it helps us to interpret our model. Specifically, an increase in age of one year results in an average salary increase of 1080$. It‚Äôs important to note that this increase can vary ‚Äî sometimes it‚Äôs as low as 540$, and other times as high as 1620$, depending on people‚Äôs job roles. The model indicates that there is a 95% chance that the salary increase falls within this interval. That‚Äôs why it‚Äôs called - the 95% Confidence Interval.\nIf this average annual increase of 1080$ occurs consistently, we obtain the slope of the line, represented by the beta (Œ≤) coefficient in the model formula. Put simply, the slope indicates the average change in ‚Äúy‚Äù for every one-unit increase in ‚Äúx‚Äù. As with any slope, it needs a starting point. And the best start of any slope is usually zero. When our line crosses the y-axis (zero on the x-axis), we find the intercept (Œ±) in the model output. In our case, this implies an unrealistic scenario where our salary would be 57,500$ at birth, which doesn‚Äôt make any sense, and that‚Äôs why the intercept of a model is often not interpreted.\nThe true value of the formula lies in its ability to predict future salaries. By plugging any ‚Äúage‚Äù value (x) into the model equation, we can estimate the corresponding ‚Äúsalary‚Äù value (y). This means we can ask the model to tell us how much we‚Äôll earn when we‚Äôre 40 or 50.\nMake specific predictions\nFor that we‚Äôll simply replace our alpha with the value of the Intercept 57.5 and the age with 40 or 50:\n57.50 + 1.08 * 40 = 100.7\n57.50 + 1.08 * 50 = 111.5\nAssuming our lives remain relatively stable, at 40 years old, we could be earning over 100,000$, and by 50, our paychecks could exceed 111,000$. Essentially, we‚Äôve just forecast our future salaries based on the available data. Naturally, R provides several convenient functions for making such predictions, the most well-known being the ‚Äúpredict‚Äù function. This function allows you to input your model and a data frame containing the variable you want predictions for. However, I personally find this function somewhat unintuitive.\n\n\n# get particular predictions\npredict(m, data.frame(age = c(40, 50, 60)) )\n\n       1        2        3 \n100.8430 111.6785 122.5140 \n\nA much better choice is the ‚Äúemmeans‚Äù function from the very powerful {emmeans} package. It not only provides predictions for any age you desire, but also reports 95% confidence intervals and offers a wide range of additional capabilities.\n\n\nlibrary(emmeans)   # extra 2 videos on my channel ;)\nemmeans(m, ~ age, at = list(age = c(40, 50, 60)))\n\n age emmean   SE df lower.CL upper.CL\n  40    101 2.65 98     95.6      106\n  50    112 3.80 98    104.1      119\n  60    123 6.06 98    110.5      135\n\nConfidence level used: 0.95 \n\nAsking for predictions beyond 60 years old isn‚Äôt recommended, as we intentionally limited our data to include only individuals up to 60. Here, it‚Äôs crucial to differentiate between interpolation and extrapolation. Interpolation means - predicting values within existing data points, and it is very useful. However, extrapolation, which means going beyond the data range, is risky and unreliable. Predicting salary at birth (age zero) is a perfect example for a nonsensical extrapolation, because at the age of zero we can‚Äôt earn anything.\nGet effect sizes\nNow, that we understand how to interpret our model, the next crucial question we need to address is - how good our model is. Your clients or scientific reviewers will definetely ask this question. What they typically want to know is - how well our model fits the data, which is summarized by the coefficient of determination, denoted as \\(R^2\\).\n\\(R^2\\) quantifies how much variance in the data our model explains. It ranges from 0 to 1, with a simple rule: the higher the value, the better the fit. However, interpreting \\(R^2\\) is context-dependent. For instance:\nin physics, a model with \\(R^2\\) near 1 is usually desirable, while\nin biology, even an \\(R^2\\) of 0.2 might indicate a good model fit.\nDespite this context sensitivity, some guidelines proposed by smart individuals (such as Cohen) exist. And we can ask the {effectsize} package to interpret any value of \\(R^2\\) for us.\n\n\n# get effect sizes\nlibrary(effectsize)\n?interpret_r2\n\ninterpret_r2(0.139, rules = \"cohen1988\") \n\n[1] \"moderate\"\n(Rules: cohen1988)\n\n# rules = \"cohen1988\" is a default\n\n\nIn our case, the \\(R^2\\) value of 0.139 indicates a moderate relationship between age and salary. But why moderate and not strong? Well, that could be because age is most likely not the most crucial predictor for salary üòâ. I mean, when I sit on the sofa, do nothing, and just get older, my salary won‚Äôt grow! So, what is an important predictor? Education is!\n\n\nggplot(Wage, aes(x = age, y = wage, color = education))+\n  geom_smooth(method = \"lm\")\n\n\n\nSpecifically, folks without a High School diploma start at 70,000$ as early as 18 years old, while those with an ‚ÄúAdvanced Degree‚Äù begin with a salary of approximately 140,000$ at the age of 25, after completing their studies. However, I‚Äôll cover it in a separate video in this series, where I demonstrate how to interpret a model with a categorical predictor. Until then if you‚Äôre enjoying the video so far, please, consider hitting the like button!\nReport model results\nFinally, the last thing I found difficult was accurately describing the model with sufficient detail for others to understand and reproduce my results. If you have similar problem, the {report} package solves it ‚Äî even if you only use one function to literally ‚Äúreport‚Äù your model.\nNamely, the ‚Äúreport‚Äù function:\nidentifies the type of model you used,\nshows how well the model fits the data (\\(R^2\\)),\ninterprets effect sizes,\ndescribes whether predictors are significant and which direction the slopes go and\neven reports how 95% confidence intervals (CIs) and p-values were calculated.\nI personally found it very useful, but I‚Äôd love to know what you think? So, feel free to share your thoughts in the comments section below.\n\n\nlibrary(report)   # extra video on my channel\nreport(m)\n\nWe fitted a linear model (estimated using OLS) to predict wage with\nage (formula: wage ~ age). The model explains a statistically\nsignificant and moderate proportion of variance (R2 = 0.14, F(1, 98)\n= 15.80, p < .001, adj. R2 = 0.13). The model's intercept,\ncorresponding to age = 0, is at 57.50 (95% CI [35.24, 79.76], t(98) =\n5.13, p < .001). Within this model:\n\n  - The effect of age is statistically significant and positive (beta =\n1.08, 95% CI [0.54, 1.62], t(98) = 3.97, p < .001; Std. beta = 0.37,\n95% CI [0.19, 0.56])\n\nStandardized parameters were obtained by fitting the model on a\nstandardized version of the dataset. 95% Confidence Intervals (CIs)\nand p-values were computed using a Wald t-distribution approximation.\n\nWhat‚Äôs next?\nMoreover, the {report} package can help you correctly describe most classic statistical tests and models. It can even describe your entire dataset, so make sure to check it out.\n\n\nreport(d)\n\nThe data contains 100 observations of the following 11 variables:\n\n  - year: n = 100, Mean = 2005.83, SD = 2.02, Median = 2006.00, MAD =\n2.97, range: [2003, 2009], Skewness = 0.15, Kurtosis = -1.25, 0%\nmissing\n  - age: n = 100, Mean = 39.99, SD = 9.76, Median = 40.50, MAD = 11.86,\nrange: [18, 56], Skewness = -0.17, Kurtosis = -0.99, 0% missing\n  - maritl: 5 levels, namely 1. Never Married (n = 20, 20.00%), 2.\nMarried (n = 74, 74.00%), 3. Widowed (n = 2, 2.00%), 4. Divorced (n =\n4, 4.00%) and 5. Separated (n = 0, 0.00%)\n  - race: 4 levels, namely 1. White (n = 83, 83.00%), 2. Black (n = 4,\n4.00%), 3. Asian (n = 11, 11.00%) and 4. Other (n = 2, 2.00%)\n  - education: 5 levels, namely 1. < HS Grad (n = 14, 14.00%), 2. HS\nGrad (n = 37, 37.00%), 3. Some College (n = 22, 22.00%), 4. College\nGrad (n = 22, 22.00%) and 5. Advanced Degree (n = 5, 5.00%)\n  - region: 9 levels, namely 1. New England (n = 0, 0.00%), 2. Middle\nAtlantic (n = 100, 100.00%), 3. East North Central (n = 0, 0.00%), 4.\nWest North Central (n = 0, 0.00%), 5. South Atlantic (n = 0, 0.00%),\n6. East South Central (n = 0, 0.00%), 7. West South Central (n = 0,\n0.00%), 8. Mountain (n = 0, 0.00%) and 9. Pacific (n = 0, 0.00%)\n  - jobclass: 2 levels, namely 1. Industrial (n = 100, 100.00%) and 2.\nInformation (n = 0, 0.00%)\n  - health: 2 levels, namely 1. <=Good (n = 30, 30.00%) and 2. >=Very\nGood (n = 70, 70.00%)\n  - health_ins: 2 levels, namely 1. Yes (n = 61, 61.00%) and 2. No (n =\n39, 39.00%)\n  - logwage: n = 100, Mean = 4.57, SD = 0.29, Median = 4.57, MAD =\n0.31, range: [3.92, 5.18], Skewness = -0.17, Kurtosis = -0.58, 0%\nmissing\n  - wage: n = 100, Mean = 100.83, SD = 28.39, Median = 96.76, MAD =\n29.46, range: [50.41, 176.99], Skewness = 0.40, Kurtosis = -0.50, 0%\nmissing\n\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2024-02-03-lmnumeric/thumbnail_lmnp.png",
    "last_modified": "2024-02-26T16:56:43+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2024-01-04-qr2/",
    "title": "Unleash Quantile Regression Results",
    "description": "In the previous episode, I presented four reasons why Quantile Regression (QR) is a better alternative to classic linear regression. However, I discovered that reporting QR results can be quite demanding. To make the process easier, I created better plots for model estimates and predictions, a comprehensive table of model results, including contrasts between groups and p-values. I found this code so useful that I thought you guys might benefit from it too. Besides, I really enjoyed programming it :)",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2024-01-14",
    "categories": [
      "videos",
      "statistics",
      "visualization"
    ],
    "contents": "\n\nContents\nThis post as ca. 18 minutes video\nGet 3 most importance quantiles\nGet all quantiles\nGet tidy results of multivariable models\nCreate a fancy plot step by step\n1. Filter out one predictor\n2. Plot all quantiles\n3. Put intercept on top of the plot\n4. Put linear model results on the plot\n5. Put three most important quantiles on the plot\n\nCreate a function to automate plot creation\nUse this function\nEffect size of all concfounders\n\nUnleash the power of contrasts with {emmeans}\nGet table of predictions for every tau\nPlot predictions of every tau\nInteractions also work (not part of the video, since too much)\nPrepare predictions of all models to add OLS results\nPlot predictions of all models OLS & QR\nGet fancy table for your publication\n\nWhat‚Äôs nest\nVisualizing contrasts (it might be an overkill, so, it‚Äôs not part of the video, but why not?)\n\nThis post as ca. 18 minutes video\n\n\n\n\n\n\n\nGet 3 most importance quantiles\nWe‚Äôll use the Wage dataset from the ISLR package to study the influence of job type, age, and education on American salaries. We first create four simple models: one classic linear regression that calculates average wages, and three quantile regressions. The low quantile (tau = 0.1) calculates wages of low earners, the median quantile (tau = 0.5) gets median wages, and the high quantile (tau = 0.9) describes wages of top 10% earners. We will then use the {sjPlot} package to plot estimates of these four models for all predictors at the same time.\n\n\nlibrary(tidyverse)      # use & thank me later ;)\nlibrary(ISLR)           # provides Wage dataset\ntheme_set(theme_test()) # beautifies plots\n\nlibrary(quantreg) # extra video on my channel\nlr   <- lm(wage ~ jobclass + age, Wage)\nqr10 <- rq(wage ~ jobclass + age, Wage, tau = 0.1)\nqr50 <- rq(wage ~ jobclass + age, Wage, tau = 0.5)\nqr90 <- rq(wage ~ jobclass + age, Wage, tau = 0.9)\n\nlibrary(sjPlot)   # extra video on my channel\nplot_models(lr, qr10, qr50, qr90, \n            show.values = TRUE,\n            m.labels = c(\"LR\", \"QR 10%\", \"QR 50%\", \"QR 90%\"), \n            legend.title = \"Model type\")\n\n\n\nWhile this plot allows us to compare results among models, different predictors usually have very different magnitudes of estimates. High estimates are displayed in a better way, while low estimates are underrepresented and hard to see. This problem could be solved with a single line of code by using the ‚Äúrm.terms‚Äù (remove terms) argument. However, when we have many predictors and many categories per predictor, this solution becomes very inconvenient. To be honest, it can become a huge pain in the neck.\n\n\nplot_models(lr, qr10, qr50, qr90, \n            rm.terms = \"jobclass [2. Information]\",\n            show.values = TRUE,\n            m.labels = c(\"LR\", \"QR 10%\", \"QR 50%\", \"QR 90%\"), \n            legend.title = \"Model type\")\n\n\n\nSo, if you are only interested in a few important quantiles and have models with a low number of predictors, then the plot_model() function is the way to go. However, if you want to explore the entire distribution of quantiles, creating a new model for every quantile would be necessary. This approach would result in too much code and the plot might look cluttered with too many estimates.\nGet all quantiles\nA more elegant solution for obtaining more quantiles is to use the ‚Äútau‚Äù argument with a sequence of quantiles of your choice. This way, we can easily create multiple models at once, say 20. Using the ‚Äúsummary‚Äù and ‚Äúplot‚Äù functions would allow us to plot all predictors for all quantiles simultaneously. However, this visualization is hardly customizable and we cannot include estimates or p-values like {sjPlot} did.\n\n\nqr_all <- rq(wage ~ jobclass + age, Wage, \n             tau  = seq(.05, .95, by = 0.05))\n\nsummary(qr_all) %>% plot()\n\n\n\nTo address this issue, I developed the idea of combining the visual results of {sjPlot} and {quantreg} in order to create beautiful and highly customizable plot. To do that, we need two tidy datasets ‚Äì one containing the quantile regression model results and the other comprising the linear regression model results. This approach is necessary because, despite the fact that the linear regression results are plotted in red by the {quantreg} package, which means that they have been calculated, I could not extract them from the model. If you know how, please put the solution in the comments below for the whole community.\n\n\n\nGet tidy results of multivariable models\n\n\nsummary(lr)\n\n\nCall:\nlm(formula = wage ~ jobclass + age, data = Wage)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-107.234  -24.751   -6.311   16.308  197.278 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             76.6298     2.8320   27.06   <2e-16 ***\njobclass2. Information  15.9214     1.4732   10.81   <2e-16 ***\nage                      0.6447     0.0638   10.11   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.16 on 2997 degrees of freedom\nMultiple R-squared:  0.07435,   Adjusted R-squared:  0.07373 \nF-statistic: 120.4 on 2 and 2997 DF,  p-value: < 2.2e-16\n\nWe need to tidy up model results because the classic summary() function of any model does not produce them. For that, we will use, suprise suprise, the tidy() function from the {rstatix} package. Here, it is important to use the se.type = ‚Äúnid‚Äù argument because otherwise, the default option of tidy() (se.type = ‚Äúrank‚Äù) would produce slightly different confidence intervals (CIs) compared to {sjPlot} CIs, which would then not match on the plot. Having only Standard-Error will allow us to calculate 95% CIs on our terms! So, we would feel more powerful and in control. (some ironic picture of powerful personality, like Held in unterhose)\n\n\nlr_preds <- lr %>% \n  rstatix::tidy(se.type = \"nid\") %>% # se.type = \"rank\"\n  mutate(LCL = estimate - 1.96 * std.error, \n         UCL = estimate + 1.96 * std.error) %>% \n  rstatix::add_significance(\"p.value\") %>% \n  filter(term != \"(Intercept)\") %>% \n  select(-std.error, -statistic) %>%\n  rename(p = p.value.signif)\n\nqr_preds <- qr_all %>% \n  rstatix::tidy(se.type = \"nid\") %>% \n  mutate(LCL = estimate - 1.96 * std.error, \n         UCL = estimate + 1.96 * std.error) %>% \n  filter(!grepl(\"Intercept\", term)) %>% \n  rstatix::add_significance(\"p.value\") %>% \n  select(-std.error, -statistic) %>%\n  rename(p = p.value.signif)\n\n\nWe then remove the intercept because there is a better way to deal with it and we‚Äôll come to it in a moment. To do so, we can filter the intercept out using the exclamation mark. However, intercepts are often written in a strange way, with brackets for example, which can be confusing. Therefore, we can use the ‚Äúgrepl‚Äù function to get rid of it. The ‚Äúgrepl‚Äù function searches for matches to the pattern we specify. In our case, the word ‚Äúintercept‚Äù itself is the pattern. (Note that ‚Äúgrepl‚Äù also takes missing values in our variable ‚Äúterm‚Äù as not matching a non-missing pattern.)\nFinally, we can use the ‚Äúadd_significance‚Äù function from the {rstatix} package to add significance stars as a new column. Here‚Äôs a quick interpretation of the stars:\n\nStarsP-value Interpretationcharactercharacternsp ‚â• 0.05*p < 0.05**p < 0.01***p < 0.001****p = 0n: 5\n\nBy the way, the double colon is a convenient way to use only one particular function from a specific package. For instance, in this case, I don‚Äôt need to use the {rstatix} package anymore. Using the double colon takes less memory on a computer and produces fewer conflicts. For example, the ‚Äútidy‚Äù function is present in three different packages. But I digress, so, let‚Äôs get back to the topic at hand.\n\n\n?tidy()\n\nHelp on topic 'tidy' was found in the following packages:\n\n  Package               Library\n  rstatix               /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n  broom                 /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n  generics              /Library/Frameworks/R.framework/Versions/4.3-x86_64/Resources/library\n\n\nUsing the first match ...\n\nFinally, we remove unnecessary columns from our tidy dataset and rename the p-value column. We are now ready to produce our beautiful and informative plot.\nCreate a fancy plot step by step\nTo ensure maximum clarity and learning, let‚Äôs program that fancy plot step-by-step.\n1. Filter out one predictor\nFirst, we‚Äôll focus on one predictor at a time. Let‚Äôs filter out the predictor ‚Äòage‚Äô and produce two smaller tidy datasets, one for linear regression and another for quantile regression.\n\n\ndata_lm = lr_preds %>% filter(term == \"age\")\ndata_qr = qr_preds %>% filter(term == \"age\")\n\n\n2. Plot all quantiles\nTo plot the estimates of QR and their 95% CIs, we‚Äôll use a ‚Äúggplot‚Äù command with classic ‚Äúgeom_point‚Äù, ‚Äúgeom_line‚Äù and ‚Äúgeom_ribbon‚Äù functions. We‚Äôll display all possible tau values on the x axis and finally, we‚Äôll flip the coordinates in order to make this plot similar to {sjPlots} solution. It looks much nicer than the grey plot, doesn‚Äôt it? :)\n\n\nq_plot <- ggplot(data = data_qr, aes(x = tau, y = estimate))+\n  geom_point(color = \"green\", size = 3)+  \n  geom_line( color = \"green\", size = 1)+ \n  geom_ribbon(aes(ymin = LCL, ymax = UCL), fill = \"green\", alpha = 0.25)+\n  scale_x_reverse(breaks = seq(from = 0, to = 1, by = 0.1))+\n  coord_flip()\n\nq_plot\n\n\n\n3. Put intercept on top of the plot\nAnother disadvantage of the grey plot is that it sometimes doesn‚Äôt show the intercept for some predictors. For example, it doesn‚Äôt show the intercept for ‚Äúage,‚Äù but it does for ‚Äújobclass.‚Äù We‚Äôll fix this by using the ‚Äúgeom_hline‚Äù function to always put the Intercept on our plot. This will display the strength of the evidence against the null hypothesis. The further our estimates are from the intercept, the stronger evidence we have against the null hypothesis.\n\n\n# yintercept instead of xintercept because the plot is flipped\nq_plot2 <- q_plot +\n  geom_hline(yintercept = 0, alpha = .3, size = 1)\n\nq_plot2\n\n\n\n4. Put linear model results on the plot\nThe next step is to put linear model results on our plot with ‚Äúgeom_line‚Äù and ‚Äúannotate‚Äù functions. Here, we could have used the numbers from the dataset directly. But imagine that you need to do this for 10 or 20 predictors, and you‚Äôll see that we need to somehow automate the process of putting estimates on the plot. That‚Äôs what we do next.\n\n\ndata_lm\n\n# A tibble: 1 √ó 6\n  term  estimate  p.value   LCL   UCL p    \n  <chr>    <dbl>    <dbl> <dbl> <dbl> <chr>\n1 age      0.645 1.24e-23 0.520 0.770 **** \n\nNamely, we use our small dataset with only ‚Äúage‚Äù predictor and:\nFirst, we use the estimate as a line.\nThen, we use the ‚Äúannotate‚Äù function to create a rectangle with Lower and Upper Confidence Limits for y-coordinates and Infinity for x-coordinates.\nThe opacity, color, and size of our lines would allow us to make our plot more appealing than the {quantreg} plot.\nFinally, we‚Äôll put the estimate and the p-value stars as a piece of text on our plot in the location of our choice. We can control the location via ‚Äúx‚Äù and ‚Äúy‚Äù arguments and adjust it horizontally via ‚Äúhjust‚Äù or vertically via ‚Äúvjust‚Äù when we need to. The ‚Äúpaste‚Äù command helps us to combine a real text with a number and significance stars into a single piece of information.\n\n\n# OLS\nq_plot3 <- q_plot2 +\n  geom_hline(yintercept = data_lm$estimate, \n             alpha = .2, color = \"red\", size = 1)+\n  annotate(geom = \"rect\", alpha = .1, fill=\"red\",\n           ymin = data_lm$LCL, ymax = data_lm$UCL, \n           xmin = -Inf,        xmax = Inf)+\n  annotate(geom = \"text\", color=\"red\",\n           x = 0, y = data_lm$estimate, hjust = -.1,\n           label = paste(\"OLS = \", round(data_lm$estimate, 2), data_lm$p ))\n\nq_plot3\n\n\n\n5. Put three most important quantiles on the plot\n\n\ndata_qr\n\n# A tibble: 19 √ó 7\n   term  estimate  p.value   tau   LCL   UCL p    \n   <chr>    <dbl>    <dbl> <dbl> <dbl> <dbl> <chr>\n 1 age      0.371 2.09e- 4  0.05 0.175 0.566 ***  \n 2 age      0.360 1.82e- 8  0.1  0.235 0.485 **** \n 3 age      0.401 2.16e-12  0.15 0.289 0.512 **** \n 4 age      0.411 1.53e-12  0.2  0.297 0.524 **** \n 5 age      0.472 2.00e-15  0.25 0.356 0.587 **** \n 6 age      0.513 0         0.3  0.398 0.627 **** \n 7 age      0.531 0         0.35 0.418 0.645 **** \n 8 age      0.584 0         0.4  0.472 0.696 **** \n 9 age      0.602 0         0.45 0.488 0.715 **** \n10 age      0.652 0         0.5  0.536 0.767 **** \n11 age      0.705 0         0.55 0.579 0.831 **** \n12 age      0.713 0         0.6  0.583 0.843 **** \n13 age      0.756 0         0.65 0.622 0.890 **** \n14 age      0.850 0         0.7  0.708 0.991 **** \n15 age      0.891 0         0.75 0.728 1.05  **** \n16 age      0.919 0         0.8  0.732 1.10  **** \n17 age      0.974 0         0.85 0.762 1.19  **** \n18 age      1.12  6.20e-13  0.9  0.813 1.42  **** \n19 age      1.89  0         0.95 1.59  2.20  **** \n\nSimilar to the results of linear regression, we can easily put the results of the three most important quantiles on our plot. There are only two small differences:\nFirst, we use ‚Äúgeom_pointrange‚Äù instead of ‚Äúgeom_line‚Äù and ‚Äúgeom_rect‚Äù.\nSecondly, since our dataset for age now contains 19 different quantiles, we need to specify which line of the estimate or confidence intervals we should use. In our case, we‚Äôd use 2 for the first quantile (tau = 0.1), 10 for the median regression (tau = 0.5), and 18 for the ninth quantile (tau = 0.9).\n\n\n  # tau = .1\nq_plot4 <- q_plot3 +\n  annotate(geom = \"pointrange\", \n           x    = .1, color = \"black\", size = 1, linewidth = 1,\n           y    = data_qr$estimate[2], \n           ymin = data_qr$LCL[2], \n           ymax = data_qr$UCL[2])+\n  annotate(geom = \"text\", \n           x = .1 - 0.05, \n           y = data_qr$estimate[2], hjust = -.14, \n           label = paste( round(data_qr$estimate[2], 2), data_qr$p[2] ))+\n  \n  # tau = .5\n  annotate(geom = \"pointrange\", \n           x    = .5, color = \"black\", size = 1, linewidth = 1,\n           y    = data_qr$estimate[10], \n           ymin = data_qr$LCL[10], \n           ymax = data_qr$UCL[10])+\n  annotate(geom = \"text\", \n           x = .5 - 0.05, \n           y =  data_qr$estimate[10], hjust = -.14, \n           label = paste( round(data_qr$estimate[10], 2), data_qr$p[10] ))+\n  \n  # tau = .9\n  annotate(geom = \"pointrange\", \n           x    = .9, color = \"black\", size = 1, linewidth = 1,\n           y    = data_qr$estimate[18], \n           ymin = data_qr$LCL[18], \n           ymax = data_qr$UCL[18])+\n  annotate(geom=\"text\", \n           x = .9 - 0.05, \n           y = data_qr$estimate[18], hjust = -.14,\n           label = paste( round(data_qr$estimate[18], 2), data_qr$p[18] ))\n  \nq_plot4\n\n\n\nPutting more than 3 quantiles on this plot would unnecessarily clutter it. But using the 3 most important ones seems optimal to me. And while estimates are useful, since we don‚Äôt need to stair at the axis below, we don‚Äôt really need the significance stars. And here‚Äôs why: a simple trick to quickly extract inference from this plot is to look whether the confidence intervals cross the grey line. If they do, we usually can‚Äôt reject the Null Hypothesis. If they don‚Äôt, we can. Interestingly, the trick with confidence intervals works for comparing models too. So, when red and green CIs don‚Äôt overlap, the models differ significantly. In this concrete example, that would mean that the classic linear regression, which says that an increase in age of one year would increase the salary by $640 on average, overpromises wage rise for low earners and underpromises for folks who started their career with high salary from the very beginning. Despite the fact that {quantreg} package doesn‚Äôt compare the taus among each other, even when different quantile models don‚Äôt overlap, I would see them as significantly different.\nSo, this picture combines the capabilities of {sjPlot} and of {quantreg} packages (halliluja!). But that is just a beginning, because we now can put all the building blocks together and create a function that automates this procedure for any predictor in a multivariable model.\nCreate a function to automate plot creation\nIf you‚Äôre wondering why I didn‚Äôt give you the function at the very beginning, the answer is simple: you might have learned less! Besides, copy-pasting functions, which I have often done, can create problems later because I don‚Äôt understand every part of the code well enough. And finally, it wouldn‚Äôt fit on the screen :)\nTo create a function, we simply combine the 5 steps we just coded. We only need to make a few changes to the already existing code, namely:\nThe first line, where we name our function and put the arguments we need. We only need three arguments: the first one is the predictor we want to visualize, and the other two are just pieces of text which we would put on the axis.\nTidying up datasets goes without any change.\nUsing the ‚Äúpredictor‚Äù argument instead of the name of the concrete predictor ‚Äúage‚Äù will allow us to choose predictors we want to see in the plot.\nPlotting estimates, Intercept, OLS, and Quantiles also goes without any change.\nand finally, those two arguments of our function, ‚Äúintercept‚Äù and ‚Äúestimates‚Äù, allow us to automate labeling. And that‚Äôs it!\n\n\ncool_QR_plot = function(predictor, intercept, estimates) {\n  \n# tidy up results\nlr_preds <- lr %>% \n  rstatix::tidy(se.type = \"nid\") %>% # se.type = \"rank\"\n  mutate(LCL = estimate - 1.96 * std.error, \n         UCL = estimate + 1.96 * std.error) %>% \n  filter(term != \"(Intercept)\") %>% \n  rstatix::add_significance(\"p.value\") %>% \n  select(-std.error, -statistic) %>%\n  rename(p = p.value.signif)\n\nqr_preds <- qr_all %>% \n  rstatix::tidy(se.type = \"nid\") %>% \n  mutate(LCL = estimate - 1.96 * std.error, \n         UCL = estimate + 1.96 * std.error) %>% \n  filter(!grepl(\"Intercept\", term)) %>% \n  rstatix::add_significance(\"p.value\") %>% \n  select(-std.error, -statistic) %>%\n  rename(p = p.value.signif)\n  \n# isolate one predictor\ndata_lm = lr_preds %>% filter(term == predictor)\ndata_qr = qr_preds %>% filter(term == predictor)\n\n# plot them\nggplot(data = data_qr, aes(x = tau, y = estimate))+\n  geom_point(color = \"green\", size = 3)+  # color #27408b\n  geom_line( color = \"green\", size = 1)+ \n  geom_ribbon(aes(ymin = LCL, ymax = UCL), alpha = 0.25, fill = \"green\")+\n  coord_flip()+\n  scale_x_reverse(breaks = seq(from = 0, to = 1, by = 0.1))+\n  \n  # Intercept\n  geom_hline(yintercept = 0, alpha = .3, size = 1)+\n  \n  # OLS\n  geom_hline(yintercept = data_lm$estimate, alpha = .2, color = \"red\", size = 1)+\n  annotate(geom = \"rect\", alpha = .1, fill=\"red\",\n           ymin = data_lm$LCL, ymax = data_lm$UCL, \n           xmin = -Inf,        xmax = Inf)+\n  annotate(geom = \"text\", color = \"red\",\n           x = 0, y = data_lm$estimate, hjust = -.1,\n          label = paste(\"OLS = \", round(data_lm$estimate, 2), data_lm$p ))+\n  \n  # tau = .1\n  annotate(geom = \"pointrange\",  \n           x    = .1, color = \"black\", size = 1, linewidth = 1,\n           y    = data_qr$estimate[2], \n           ymin = data_qr$LCL[2], \n           ymax = data_qr$UCL[2])+\n  annotate(geom = \"text\", \n           x = .1 - 0.05, \n           y = data_qr$estimate[2], hjust = -.14, \n           label = paste( round(data_qr$estimate[2], 2), data_qr$p[2] ))+\n  \n  # tau = .5\n  annotate(geom = \"pointrange\",  \n           x    = .5, color = \"black\", size = 1, linewidth = 1,\n           y    = data_qr$estimate[10], \n           ymin = data_qr$LCL[10], \n           ymax = data_qr$UCL[10])+\n  annotate(geom = \"text\", \n           x = .5 - 0.05, \n           y =  data_qr$estimate[10], hjust = -.14, \n           label = paste( round(data_qr$estimate[10], 2), data_qr$p[10] ))+\n  \n  # tau = .9\n  annotate(geom = \"pointrange\", \n           x    = .9, color = \"black\", size = 1, linewidth = 1,\n           y    = data_qr$estimate[18], \n           ymin = data_qr$LCL[18], \n           ymax = data_qr$UCL[18])+\n  annotate(geom=\"text\", \n           x = .9 - 0.05, \n           y = data_qr$estimate[18], hjust = -.14,\n           label = paste( round(data_qr$estimate[18], 2), data_qr$p[18] ))+\n  \n  # labels\n  labs(y = paste( \"Wage change:\", intercept, \"(gray) and\", estimates, \"(colored)\"),\n       x = \"Quantile (tau)\")\n\n}\n\n\nUse this function\nNow that we have this function, we‚Äôre able to produce these plots for all our numerical predictors or for each category of our categorical predictor.\n\n\ncool_QR_plot(\"age\", \"age = 0\", \"increasing age\")\n\n\ncool_QR_plot(\"jobclass2. Information\", \"jobclass = Industrial\", \"IT\")\n\n\n\nTo demonstrate how efficiently we can generate comprehensive multi-plots from complex multivariable models, consider this example. We‚Äôll first enhance our models by incorporating the ‚Äúeducation‚Äù predictor with its five distinct categories.\n\n\nlr   <- lm(wage ~ jobclass + age + education, Wage)\nqr10 <- rq(wage ~ jobclass + age + education, Wage, tau = 0.1)\nqr50 <- rq(wage ~ jobclass + age + education, Wage, tau = 0.5)\nqr90 <- rq(wage ~ jobclass + age + education, Wage, tau = 0.9)\n\nqr_all <- rq(wage ~ jobclass + age + education, Wage, \n             tau  = seq(.05, .95, by = 0.05))\n\n\nNext, we produce separate plots for each numerical predictor and each category of our categorical predictors and save them into separate objects.\n\n\njob <- cool_QR_plot(\"jobclass2. Information\", \"jobclass = Industrial\", \"IT\")\n\nage <- cool_QR_plot(\"age\", \"age = 0\", \"increasing age\")\n\nedu_hs <- cool_QR_plot(\"education2. HS Grad\", \"Educ. = < HS Grad\", \"HS\")\n\nedu_sc <- cool_QR_plot(\"education3. Some College\", \"Educ. = < HS\", \"‚âà College\")\n\nedu_coll <- cool_QR_plot(\"education4. College Grad\", \"Educ. = < HS\", \"College\")\n\nedu_high <- cool_QR_plot(\"education5. Advanced Degree\", \"Educ. = < HS\", \"Uni | PhD\")\n\n\nAnd finally, we put together all the individual plots into a comprehensive multiplot using the {patchwork} package. I will not go into the code here, since I just published a video on using the {patchwork} package. Instead, I‚Äôd like to highlight the ‚Äújob‚Äù plot, which now presents a completely different perspective compared to the original one.\n\n\nlibrary(patchwork) # extra video on my channel\n\njob + age + edu_hs + edu_sc + edu_coll + edu_high +\n  plot_layout(ncol = 3, nrow = 2) +\n  plot_annotation(\n    title    = \"Salaries predicted by profession, age and education\",\n    subtitle = \"Should you invest in yourself?\",\n    caption  = \"Data source: ISLR package\",\n    tag_levels = \"A\",  # 'a', '1', ‚Å†'i‚Å†, or 'I'\n  )\n\n\n\nEffect size of all concfounders\nThe impact of the ‚Äújob‚Äù predictor on the response variable changed significantly because we introduced a much stronger predictor for ‚Äúwage‚Äù ‚Äì ‚Äúeducation‚Äù. The {effectsize} package and the ‚Äúeta_squared‚Äù function demonstrate that the effect of ‚Äúeducation‚Äù is 4 times stronger than the effect of ‚Äújobclass‚Äù. Thus, education matters. (Like, whaaattt???) This phenomenon often arises due to a confounding factor, a topic that deserves a separate discussion. On that note, I‚Äôd be curious to know how many of you guys are familiar with the concept of confounding. Please share your thoughts in the comments section below.\n\n\nlibrary(effectsize)\neta_squared(lr, alternative = \"two.sided\")\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\njobclass  |           0.05 | [0.04, 0.07]\nage       |           0.04 | [0.03, 0.06]\neducation |           0.20 | [0.18, 0.23]\n\nNow, when we look at our fancy multiplot, we‚Äôll see four distinct subplots dedicated to the ‚Äúeducation‚Äù predictor, each comparing a unique education category to the reference level, namely ‚Äúno high school education‚Äù. While comparing solely to the reference level may sufficient in some cases, I personally prefer to conduct pairwise comparisons between all categories to extract the maximum amount of information from the model. This can be easily done with one of my absolute favorite R packages, the {emmeans} package.\nUnleash the power of contrasts with {emmeans}\nGet table of predictions for every tau\nWhat the {emmeans} package does is that it enables us to compare education categories pairwisely for every tau in the quantile regression. However, since seeing so many results can be overwhelming, we can restrict the taus to only the most important ones through the user-friendly ‚Äútau‚Äù argument. If you prefer to have the 95% confidence intervals instead of standard errors, you can add the ‚Äúinfer = T‚Äù argument to the ‚Äúemmeans‚Äù function.\n\n\nlibrary(emmeans) # extra video on my channel ... it's really useful!\n\nemmeans(qr_all, \n        pairwise ~ education, \n        by  = \"tau\", \n        tau = c(.1, .9),\n        weights = \"prop\") # infer = T\n\n$emmeans\ntau = 0.1:\n education          emmean    SE   df lower.CL upper.CL\n 1. < HS Grad         61.6  2.15 2993     57.4     65.8\n 2. HS Grad           67.7  1.03 2993     65.7     69.7\n 3. Some College      77.0  2.26 2993     72.6     81.5\n 4. College Grad      80.9  2.10 2993     76.8     85.1\n 5. Advanced Degree   96.1  3.35 2993     89.6    102.7\n\ntau = 0.9:\n education          emmean    SE   df lower.CL upper.CL\n 1. < HS Grad        114.4  2.02 2993    110.5    118.4\n 2. HS Grad          128.2  1.52 2993    125.2    131.2\n 3. Some College     141.1  1.85 2993    137.4    144.7\n 4. College Grad     168.7  3.30 2993    162.2    175.2\n 5. Advanced Degree  255.7 20.43 2993    215.7    295.8\n\nResults are averaged over the levels of: jobclass \nConfidence level used: 0.95 \n\n$contrasts\ntau = 0.1:\n contrast                             estimate    SE   df t.ratio\n 1. < HS Grad - 2. HS Grad               -6.08  2.36 2993  -2.574\n 1. < HS Grad - 3. Some College         -15.39  3.11 2993  -4.950\n 1. < HS Grad - 4. College Grad         -19.30  3.02 2993  -6.386\n 1. < HS Grad - 5. Advanced Degree      -34.50  4.01 2993  -8.598\n 2. HS Grad - 3. Some College            -9.31  2.47 2993  -3.766\n 2. HS Grad - 4. College Grad           -13.22  2.35 2993  -5.625\n 2. HS Grad - 5. Advanced Degree        -28.42  3.53 2993  -8.060\n 3. Some College - 4. College Grad       -3.90  3.07 2993  -1.271\n 3. Some College - 5. Advanced Degree   -19.10  4.03 2993  -4.736\n 4. College Grad - 5. Advanced Degree   -15.20  3.92 2993  -3.879\n p.value\n  0.0754\n  <.0001\n  <.0001\n  <.0001\n  0.0016\n  <.0001\n  <.0001\n  0.7091\n  <.0001\n  0.0010\n\ntau = 0.9:\n contrast                             estimate    SE   df t.ratio\n 1. < HS Grad - 2. HS Grad              -13.74  2.09 2993  -6.590\n 1. < HS Grad - 3. Some College         -26.61  2.48 2993 -10.710\n 1. < HS Grad - 4. College Grad         -54.25  3.75 2993 -14.454\n 1. < HS Grad - 5. Advanced Degree     -141.29 20.51 2993  -6.888\n 2. HS Grad - 3. Some College           -12.87  2.23 2993  -5.783\n 2. HS Grad - 4. College Grad           -40.51  3.55 2993 -11.394\n 2. HS Grad - 5. Advanced Degree       -127.54 20.47 2993  -6.230\n 3. Some College - 4. College Grad      -27.64  3.73 2993  -7.418\n 3. Some College - 5. Advanced Degree  -114.67 20.50 2993  -5.594\n 4. College Grad - 5. Advanced Degree   -87.04 20.68 2993  -4.208\n p.value\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  0.0003\n\nResults are averaged over the levels of: jobclass \nP value adjustment: tukey method for comparing a family of 5 estimates \n\nThe output of the ‚Äúemmeans‚Äù function consists of two tables. The upper table provides predicted values for every group of the categorical predictor and for every tau. The second table provides contrasts or differences between groups for every tau. To make these tables even more useful, let‚Äôs first visualize the predicted values and then produce the best table of contrasts.\nPlot predictions of every tau\nVisualizing predictions for each quantile is remarkably straightforward with the {emmeans} package and the ‚Äúemmip‚Äù function. All we need are 1) the fitted model, 2) the predictor of interest, 3) the inclusion of confidence intervals (via ‚ÄúCIs = TRUE‚Äù), and 4) the specification of the quantiles we aim to plot. In practice, plotting all quantiles is often unnecessary and will just clutter our visualization. That‚Äôs why I often go for a maximum of 3 quantiles. For numeric variables, we must also provide a list of values for which we want to see predictions; otherwise, the ‚Äúemmip‚Äù function will only plot the predictions for the average ‚Äúage‚Äù for each quantile.\n\n\nemmip(qr_all, tau ~ education, CIs = T, tau = c(.1, .5, .9)) +\n\nemmip(qr_all, tau ~ age, CIs = T, tau = c(.1, .5, .9),\n      at = list(age = c(25, 45, 65))) +\n  \nemmip(qr_all, tau ~ age, CIs = T) \n\n\n\nThe three taus are more than enough to tell a compelling story. Namely, common people without education earn similarly to the lowest 10% earners with a college degree, but still less than the lowest 10% of earners with the highest education. But even without education, you can be successful and earn more than some highly educated folks in the lowest quantile. However, education opens more doors to growth in salary, as can be seen by the difference between the lowest and highest quantiles. The difference between the lowest and highest salaries without education is much smaller than the difference between the lowest and highest salaries with the highest education. The classic linear regression would never have been able to deliver so much inference, so that quantile regression simply rocks! And if you also find it as useful as me, consider hitting the like button.\nInteractions also work (not part of the video, since too much)\n\n\nqr_all_int <- rq(wage ~ jobclass*education, Wage, tau = seq(.25, .75, by = 0.25))\n\nemmeans(qr_all_int, pairwise ~ education | jobclass, type = \"response\", by = \"tau\", tau = c(.25, .75)) #  infer = T for 95% CIs\n\n$emmeans\njobclass = 1. Industrial, tau = 0.25:\n education          emmean   SE   df lower.CL upper.CL\n 1. < HS Grad         69.1 1.54 2990     66.1     72.1\n 2. HS Grad           76.9 1.37 2990     74.2     79.6\n 3. Some College      88.0 1.83 2990     84.4     91.6\n 4. College Grad      94.1 3.28 2990     87.6    100.5\n 5. Advanced Degree  104.9 6.79 2990     91.6    118.2\n\njobclass = 2. Information, tau = 0.25:\n education          emmean   SE   df lower.CL upper.CL\n 1. < HS Grad         72.2 2.98 2990     66.3     78.0\n 2. HS Grad           79.9 1.50 2990     76.9     82.8\n 3. Some College      90.5 1.61 2990     87.3     93.6\n 4. College Grad      99.7 1.45 2990     96.9    102.5\n 5. Advanced Degree  118.9 1.99 2990    115.0    122.8\n\njobclass = 1. Industrial, tau = 0.75:\n education          emmean   SE   df lower.CL upper.CL\n 1. < HS Grad         93.1 3.43 2990     86.3     99.8\n 2. HS Grad          109.8 1.76 2990    106.4    113.3\n 3. Some College     120.6 2.46 2990    115.8    125.4\n 4. College Grad     138.3 2.74 2990    132.9    143.7\n 5. Advanced Degree  158.3 5.38 2990    147.7    168.8\n\njobclass = 2. Information, tau = 0.75:\n education          emmean   SE   df lower.CL upper.CL\n 1. < HS Grad        101.8 5.83 2990     90.4    113.3\n 2. HS Grad          111.7 1.18 2990    109.4    114.0\n 3. Some College     121.4 1.97 2990    117.6    125.3\n 4. College Grad     145.8 1.52 2990    142.8    148.8\n 5. Advanced Degree  177.0 5.05 2990    167.1    186.9\n\nConfidence level used: 0.95 \n\n$contrasts\njobclass = 1. Industrial, tau = 0.25:\n contrast                             estimate   SE   df t.ratio\n 1. < HS Grad - 2. HS Grad               -7.80 2.06 2990  -3.776\n 1. < HS Grad - 3. Some College         -18.88 2.39 2990  -7.899\n 1. < HS Grad - 4. College Grad         -24.97 3.63 2990  -6.885\n 1. < HS Grad - 5. Advanced Degree      -35.82 6.97 2990  -5.142\n 2. HS Grad - 3. Some College           -11.09 2.28 2990  -4.857\n 2. HS Grad - 4. College Grad           -17.18 3.56 2990  -4.829\n 2. HS Grad - 5. Advanced Degree        -28.03 6.93 2990  -4.044\n 3. Some College - 4. College Grad       -6.09 3.76 2990  -1.622\n 3. Some College - 5. Advanced Degree   -16.94 7.03 2990  -2.408\n 4. College Grad - 5. Advanced Degree   -10.85 7.55 2990  -1.438\n p.value\n  0.0015\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  0.0005\n  0.4833\n  0.1134\n  0.6032\n\njobclass = 2. Information, tau = 0.25:\n contrast                             estimate   SE   df t.ratio\n 1. < HS Grad - 2. HS Grad               -7.70 3.33 2990  -2.311\n 1. < HS Grad - 3. Some College         -18.33 3.38 2990  -5.417\n 1. < HS Grad - 4. College Grad         -27.54 3.31 2990  -8.322\n 1. < HS Grad - 5. Advanced Degree      -46.73 3.58 2990 -13.056\n 2. HS Grad - 3. Some College           -10.63 2.20 2990  -4.824\n 2. HS Grad - 4. College Grad           -19.83 2.09 2990  -9.507\n 2. HS Grad - 5. Advanced Degree        -39.03 2.49 2990 -15.656\n 3. Some College - 4. College Grad       -9.21 2.17 2990  -4.252\n 3. Some College - 5. Advanced Degree   -28.40 2.56 2990 -11.098\n 4. College Grad - 5. Advanced Degree   -19.19 2.46 2990  -7.804\n p.value\n  0.1416\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  0.0002\n  <.0001\n  <.0001\n\njobclass = 1. Industrial, tau = 0.75:\n contrast                             estimate   SE   df t.ratio\n 1. < HS Grad - 2. HS Grad              -16.78 3.86 2990  -4.351\n 1. < HS Grad - 3. Some College         -27.54 4.22 2990  -6.522\n 1. < HS Grad - 4. College Grad         -45.25 4.39 2990 -10.301\n 1. < HS Grad - 5. Advanced Degree      -65.24 6.38 2990 -10.222\n 2. HS Grad - 3. Some College           -10.76 3.02 2990  -3.559\n 2. HS Grad - 4. College Grad           -28.47 3.26 2990  -8.744\n 2. HS Grad - 5. Advanced Degree        -48.46 5.66 2990  -8.561\n 3. Some College - 4. College Grad      -17.71 3.68 2990  -4.811\n 3. Some College - 5. Advanced Degree   -37.71 5.92 2990  -6.374\n 4. College Grad - 5. Advanced Degree   -20.00 6.04 2990  -3.311\n p.value\n  0.0001\n  <.0001\n  <.0001\n  <.0001\n  0.0035\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  0.0083\n\njobclass = 2. Information, tau = 0.75:\n contrast                             estimate   SE   df t.ratio\n 1. < HS Grad - 2. HS Grad               -9.90 5.95 2990  -1.664\n 1. < HS Grad - 3. Some College         -19.61 6.15 2990  -3.187\n 1. < HS Grad - 4. College Grad         -43.98 6.02 2990  -7.301\n 1. < HS Grad - 5. Advanced Degree      -75.17 7.71 2990  -9.745\n 2. HS Grad - 3. Some College            -9.71 2.29 2990  -4.236\n 2. HS Grad - 4. College Grad           -34.08 1.92 2990 -17.735\n 2. HS Grad - 5. Advanced Degree        -65.27 5.19 2990 -12.584\n 3. Some College - 4. College Grad      -24.37 2.49 2990  -9.806\n 3. Some College - 5. Advanced Degree   -55.56 5.42 2990 -10.249\n 4. College Grad - 5. Advanced Degree   -31.18 5.27 2990  -5.912\n p.value\n  0.4564\n  0.0126\n  <.0001\n  <.0001\n  0.0002\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n\nP value adjustment: tukey method for comparing a family of 5 estimates \n\nemmip(qr_all_int, tau ~ education | jobclass, type = \"response\", CIs = T, tau = c(.25, .75))\n\n\n\nPrepare predictions of all models to add OLS results\nSpeaking of classic linear regression. The only downside to these predictions is that they don‚Äôt include the comparison to linear regression. But we can easily fix this by again using the ‚Äúemmeans‚Äù package to extract predictions from each model for each predictor and each ‚Äútau‚Äù separately, and then combine them into a new data frame called, let‚Äôs say, ‚Äúpreds‚Äù.\n\n\npreds <- bind_rows(\n\n  bind_rows(\n    \n    # OLS\n    emmeans(lr, ~ education, weights = \"prop\") %>% \n      as_tibble() %>% \n      mutate(predictor = \"education\") %>% \n      rename(strata = education),\n    \n    emmeans(lr, ~ jobclass, weights = \"prop\") %>% \n      as_tibble() %>% \n      mutate(predictor = \"jobclass\") %>% \n      rename(strata = jobclass),\n    \n    emmeans(lr, ~ age, weights = \"prop\", at = list(age = c(25, 50, 75))) %>% \n      as_tibble() %>% \n      mutate(predictor = \"age\") %>% \n      rename(strata = age) %>% \n      mutate(strata = factor(strata))\n    ) %>% \n      mutate(model = \"OLS\"),\n  \n  \n  bind_rows(\n    \n    # QR = 10%\n    emmeans(qr10, ~ education, weights = \"prop\") %>% \n      as_tibble() %>% \n      mutate(predictor = \"education\") %>% \n      rename(strata = education),\n    \n    emmeans(qr10, ~ jobclass, weights = \"prop\") %>% \n      as_tibble() %>% \n      mutate(predictor = \"jobclass\") %>% \n      rename(strata = jobclass),\n    \n    emmeans(qr10, ~ age, weights = \"prop\", at = list(age = c(25, 50, 75))) %>% \n      as_tibble() %>% \n      mutate(predictor = \"age\") %>% \n      rename(strata = age) %>% \n      mutate(strata = factor(strata))\n    ) %>% \n      mutate(model = \"QR 10%\"),\n  \n  bind_rows(\n    \n    # QR = 50%\n    emmeans(qr50, ~ education, weights = \"prop\") %>% \n      as_tibble() %>% \n      mutate(predictor = \"education\") %>% \n      rename(strata = education),\n    \n    emmeans(qr50, ~ jobclass, weights = \"prop\") %>% \n      as_tibble() %>% \n      mutate(predictor = \"jobclass\") %>% \n      rename(strata = jobclass),\n    \n    emmeans(qr50, ~ age, weights = \"prop\", at = list(age = c(25, 50, 75))) %>% \n      as_tibble() %>% \n      mutate(predictor = \"age\") %>% \n      rename(strata = age) %>% \n      mutate(strata = factor(strata))\n    ) %>% \n      mutate(model = \"QR 50%\"),\n  \n  \n  bind_rows(\n    \n    # QR = 90%\n    emmeans(qr90, ~ education, weights = \"prop\") %>% \n      as_tibble() %>% \n      mutate(predictor = \"education\") %>% \n      rename(strata = education),\n    \n    emmeans(qr90, ~ jobclass, weights = \"prop\") %>% \n      as_tibble() %>% \n      mutate(predictor = \"jobclass\") %>% \n      rename(strata = jobclass),\n    \n    emmeans(qr90, ~ age, weights = \"prop\", at = list(age = c(25, 50, 75))) %>% \n      as_tibble() %>% \n      mutate(predictor = \"age\") %>% \n      rename(strata = age) %>% \n      mutate(strata = factor(strata))\n    ) %>% \n      mutate(model = \"QR 90%\")\n  \n) %>% \n  select(-SE, -df) %>% \n  rename(`Predicted wage` = emmean)\n\n\nPlot predictions of all models OLS & QR\nWe can then use a classic ‚Äúggplot‚Äù function with ‚Äúgeom_errorbar‚Äù and ‚Äúgeom_point‚Äù to:\nfirst, create individual plots containing all 4 models for each predictor,\nthen, combine these plots into a single figure using {patchwork},\nand finally, save this multi-plot for publication in a format, quality, and size of our choice.\nCool, right?\n\n\na <- ggplot(preds %>% filter(predictor == \"education\"), \n       aes(x = strata, y = `Predicted wage`, color = model))+\n  geom_errorbar(\n  aes(ymin = lower.CL, ymax = upper.CL), \n  position = position_dodge(0.5), width = 0.5\n)+\n  geom_point(position = position_dodge(0.5), width = 0.5, size=1, shape=21, fill=\"white\")+\n  theme(legend.position = \"top\")+\n  labs(x = \"Education\")\n\nb <- ggplot(preds %>% filter(predictor == \"jobclass\"), \n       aes(x = strata, y = `Predicted wage`, color = model))+\n  geom_errorbar(\n  aes(ymin = lower.CL, ymax = upper.CL), \n  position = position_dodge(0.3), width = 0.5\n)+\n  geom_point(position = position_dodge(0.3), width = 0.5, size=1, shape=21, fill=\"white\")+\n  theme(legend.position = \"top\")+\n  labs(x = \"Job Type\")\n\nc <- ggplot(preds %>% filter(predictor == \"age\"), \n       aes(x = strata, y = `Predicted wage`, color = model))+\n  geom_errorbar(\n  aes(ymin = lower.CL, ymax = upper.CL), \n  position = position_dodge(0.5), width = 0.5\n)+\n  geom_point(position = position_dodge(0.5), width = 0.5, size=1, shape=21, fill=\"white\")+\n  theme(legend.position = \"top\")+\n  labs(x = \"Age\")\n\na + b + c + plot_annotation(tag_levels = \"A\")\n\n\nggsave(\n  \"preds_all_models.png\",\n  device = png,\n  plot = last_plot(),\n  dpi = 999,\n  width = 16,\n  height = 5\n)\n\n\nGet fancy table for your publication\nBut despite all the coolness of prediction plots, we sometimes need tables with pairwise comparisons to provide p-values. Putting p-values on plots would clutter them, so tables are sometimes a better option. And the best way to get such tables in my opinion is to use the ‚Äútbl_regression‚Äù function from the {gtsummary} package. Using just a few simple arguments inside the ‚Äútbl_regression‚Äù function will produce a perfect table for any model. For example, we can create four tables, one for the classic linear regression and three for the quantile regressions. Then, we can easily put them side by side using the ‚Äútbl_merge‚Äù function and save them in either Microsoft Word or PNG format for our publication. Isn‚Äôt that amazing?‚Äù\n\n\nlibrary(gtsummary) # extra video on my channel\n\nqr90_table <- tbl_regression(\n        qr90, \n        add_pairwise_contrasts = T,\n        emmeans_args = list(tau = .90),\n        pvalue_fun   = ~style_pvalue(.x, digits = 3),\n        estimate_fun = ~style_number(.x, digits = 2)) %>% \n        bold_p(t = 0.05)\n\nqr90_table\n\n\nCharacteristic\n      Beta\n      95% CI1\n      p-value\n    jobclass\n\n\n¬†¬†¬†¬†2. Information - 1. Industrial\n3.00\n-0.72, 6.72\n0.114age\n0.81\n0.67, 0.95\neducation\n\n\n¬†¬†¬†¬†2. HS Grad - 1. < HS Grad\n13.74\n8.05, 19.43\n¬†¬†¬†¬†3. Some College - 1. < HS Grad\n26.61\n19.83, 33.40\n¬†¬†¬†¬†3. Some College - 2. HS Grad\n12.87\n6.80, 18.95\n¬†¬†¬†¬†4. College Grad - 1. < HS Grad\n54.25\n44.00, 64.49\n¬†¬†¬†¬†4. College Grad - 2. HS Grad\n40.51\n30.80, 50.21\n¬†¬†¬†¬†4. College Grad - 3. Some College\n27.64\n17.47, 37.80\n¬†¬†¬†¬†5. Advanced Degree - 1. < HS Grad\n141.29\n85.30, 197.27\n¬†¬†¬†¬†5. Advanced Degree - 2. HS Grad\n127.54\n71.66, 183.42\n¬†¬†¬†¬†5. Advanced Degree - 3. Some College\n114.67\n58.72, 170.63\n¬†¬†¬†¬†5. Advanced Degree - 4. College Grad\n87.04\n30.59, 143.49\n1 CI = Confidence Interval\n    \n\nqr50_table <- tbl_regression(\n        qr50, \n        add_pairwise_contrasts = T,\n        emmeans_args = list(tau = .50),\n        pvalue_fun   = ~style_pvalue(.x, digits = 3),\n        estimate_fun = ~style_number(.x, digits = 2)) %>% \n        bold_p(t = 0.05)\n\nqr10_table <- tbl_regression(\n        qr10, \n        add_pairwise_contrasts = T,\n        emmeans_args = list(tau = .10),\n        pvalue_fun   = ~style_pvalue(.x, digits = 3),\n        estimate_fun = ~style_number(.x, digits = 2)) %>% \n        bold_p(t = 0.05)\n\nlr_table <- tbl_regression(\n        lr, \n        add_pairwise_contrasts = T,\n        pvalue_fun   = ~style_pvalue(.x, digits = 3),\n        estimate_fun = ~style_number(.x, digits = 2)) %>% \n        bold_p(t = 0.05)\n\n\n\n\nfancy_table <- tbl_merge(\n    tbls = list(\n      lr_table, qr10_table, qr50_table, qr90_table\n    ),\n    tab_spanner = c(\"OLS\", \"QR 10%\", \"QR 50%\", \"QR 90%\")\n  )\n\n\n\n\ntheme_gtsummary_compact()\n\nlibrary(flextable)\nfancy_table %>%\n  as_flex_table() %>% \n  save_as_docx(path = \"fancy_table.docx\")\n\nreset_gtsummary_theme()\n\nfancy_table %>%\n  as_flex_table() %>% \n  save_as_image(path = \"fancy_table.png\", res = 999)\n\n[1] \"fancy_table.png\"\n\n\n\n\nWhat‚Äôs nest\nAdditionally, you can also utilize those {gtsummary} tables directly in your writing, where you can report any result inside of the body of text. This works for both, numeric predictors, like our ‚Äúage‚Äù 0.81 (95% CI 0.67, 0.95; p<0.001) and categorical variables, for example the first education contrast of the highest quantile 13.74 (95% CI 8.05, 19.43; p<0.001). This approach saves you from typos and ensures consistent reporting. And there are actually so many cool things the {gtsummary} package can do, that I dedicated an entire video to it, so, feel free to check it out if you‚Äôre interested.\nInput:\n‚Äúr inline_text(qr90_table, variable = education, level =‚Äù2. HS Grad - 1. < HS Grad‚Äù)‚Äù\nOutput:\n13.74 (95% CI 8.05, 19.43; p<0.001)\nVisualizing contrasts (it might be an overkill, so, it‚Äôs not part of the video, but why not?)\nAnd since adding pairwise contrasts provides a huge benefit, we can quickly visualize those contrasts with {emmeans} package.\n\n\nref_grid(qr_all) %>%\n  emmeans(~ education|tau) %>%\n  pairs(weights=\"prop\") %>%\n    emmip(~ tau|contrast, CIs = T) +\n  labs(x = \"Quantile (tau)\", y = \"Education difference\")+\n  coord_flip()\n\n\n\nRe-scaling all the facets of the plot and not using annotations.\n\n\nplot_contrasts <- ref_grid(qr_all) %>% \n  emmeans(~ education|tau) %>% \n  pairs(weights=\"prop\", infer = c(T,T)) %>% \n  data.table::as.data.table() %>% \n  rstatix::add_significance(\"p.value\") %>% \n  rename(p = p.value.signif) %>% \n  select(-SE, -df, -t.ratio, -p.value) %>% \n  rename(LCL = lower.CL, UCL = upper.CL)\n\n# without annotations\nggplot(data = plot_contrasts, aes(x = tau, y = estimate))+\n  geom_point(color = \"green\", size = 3)+  # color #27408b\n  geom_line( color = \"green\", size = 1)+\n  facet_wrap(~contrast, scales = \"free\")+ \n  geom_ribbon(aes(ymin = LCL, ymax = UCL), alpha = 0.25, fill = \"green\")+\n  coord_flip()+\n  scale_x_reverse(breaks = seq(from = 0, to = 1, by = 0.1))+\n\n  # Intercept\n  geom_hline(yintercept = 0, alpha = .3, size = 1)\n\n\n\nPlotting separate contrasts with annotations by without facets.\n\n\n# with annotations by without facets\ncontrats_1 <- plot_contrasts %>% \n  filter(contrast == \"1. < HS Grad - 2. HS Grad\")\n\nggplot(data = contrats_1, aes(x = tau, y = estimate))+\n  geom_point(color = \"green\", size = 3)+  # color #27408b\n  geom_line( color = \"green\", size = 1)+\n  geom_ribbon(aes(ymin = LCL, ymax = UCL), alpha = 0.25, fill = \"green\")+\n  coord_flip()+\n  scale_x_reverse(breaks = seq(from = 0, to = 1, by = 0.1))+\n\n  # Intercept\n  geom_hline(yintercept = 0, alpha = .3, size = 1)+\n  \n  # annotate\n  annotate(geom = \"text\", \n           x = contrats_1$tau, \n           y = contrats_1$estimate, hjust = -.19, \n           label = paste( round(contrats_1$estimate, 2), contrats_1$p ))\n\n\n\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2024-01-04-qr2/thumbnail_QR2.png",
    "last_modified": "2024-01-14T23:38:50+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2023-12-16-patchwork/",
    "title": "Make Multiplots Like a Pro with {patchwork} | R package reviews",
    "description": "The Patchwork package makes it incredibly easy to combine separate plots into the same graphic by using the simplest mathematical operators, such as plus (+), slash (/), parentheses and much more.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-12-22",
    "categories": [
      "videos",
      "statistics",
      "visualization"
    ],
    "contents": "\n\nContents\nThis post as ca. 10 minutes video\nCreate some toy plots\nPlot Assembly\n1. |\n2. /\n2 by 2 or 3 by 1\nside + side by side\nside + stacked\n\n3. parenthesis\ngetting creative\n\n4. building blocks\n5. üîé ‚Äúzoom in‚Äù\n\nPlot design\n6. plot_layout()\nnasty legends\n\n7. work with fancy plots\n8. desing a layout with ‚Äúarea‚Äù\n9. add ‚Äúnon-pictures‚Äù to your multiplot\nplot_spacer()\ntext\ntable\n\n10. overlapping by ‚Äúinset_element()‚Äù\n11. plot_annotation()\n12. use {ggplot} options\n\nSave your fancy picture\nWhat‚Äôs nest\n\nThis post as ca. 10 minutes video\n\n\n\n\n\n\n\nCreate some toy plots\nSince you are here, I am sure you already have your beautiful plots. So, I‚Äôll quickly create some plots for this demo, but I won‚Äôt focus on them.\n\n\nlibrary(tidyverse)    # use & thank me later ;)\nlibrary(ISLR)         # provides Wage dataset\ntheme_set(theme_bw()) # beautifies plots\n\n# bar plot\np1 <- ggplot(Wage) + \n  geom_bar(aes(education))\n\n# box plot\np2 <- ggplot(Wage) + \n  geom_boxplot(aes(education, wage))\n\n# scatter plot\np3 <- ggplot(Wage) + \n    geom_point(aes(age, wage))\n\n# density plot\np4 <- ggplot(Wage, aes(x=wage)) +\n    geom_histogram(aes(y=after_stat(density)))+\n    geom_density()\n\n# stacked bar plot\np5 <- ggplot(data=Wage, aes(x=education, fill=health_ins)) +\n    geom_bar(position=\"fill\")+\n    scale_y_continuous(labels=scales::percent)+\n  theme(legend.position = \"top\")\n\n# grouped box plot\np6 <- ggplot(Wage) + \n    geom_boxplot(aes(health_ins, wage, fill = jobclass))+\n  theme(legend.position = \"top\")\n\n\nPlot Assembly\n1. |\nInstead, let‚Äôs get straight to the plot assembly. For example, to place multiple plots side by side, we can use a plus (+) or a vertical bar (|).\n\n\nlibrary(patchwork)    # combines plots\np4 | p5 | p6\n\n\n\n2. /\nFor stacking plots on top of each other, we‚Äôll simply use slashes between plots.\n\n\np1 / p2 / p3\n\n\n\n2 by 2 or 3 by 1\nMoreover, we can easily combine both vertical dash and slash in the same command to produce two by two or three by one multiplots. It‚Äôs that simple!\n\n\n(p1 | p2) / (p3 | p4)\n\n\n\n\n\n(p5 | p1 | p2) / p3\n\n\n\nside + side by side\nHowever, the lower picture in this layout is so unnaturally wide that my eyes start to hurt when I look at it for a little longer. Sometimes, when we have plots of different widths, we need to place a wide plot on one half of the page and two slim plots on the other half of the page. To achieve this, we can divide the page into two halves using a vertical bar and place two plots in one of the halfs, either to the right or left of the vertical dash.\n\n\np2 | (p3 + p1) \n\n\n(p4 + p6) | p5\n\n\n\nside + stacked\nIf one of the plots is significantly taller than the other two, we can use half of the page for the tall plot and stack the two flat plots vertically on top of each other in the other half of the page.\n\n\n## left + stacked\np1 | (p2 / p3)\n\n\n## stacked + right\n(p4 / p6) | p5\n\n\n\n3. parenthesis\nAnd as you can see, using parentheses is important because they can completely change the layout of your plot. For instance, let‚Äôs consider two multiplots that we have already created where we put plots side by side and put a slash between them. When we don‚Äôt use parentheses, the vertical dashes would split our multiplot horizontally into three equally sized spaces, where the middle space would consist of four small plots. However, when we use parentheses, we can split our multiplot into two vertical spaces.\n\n\np2 | (p3 + p6) / (p4 + p5) | p1\n\n\n(p2 | (p3 + p6)) / ((p4 + p5) | p1)\n\n\n\ngetting creative\nAnd we can get ridiculously creative with the design of our plots.\n\n\n(p1 / p2 | p3) / p4\n\n\n\n\n\np5 | ( (p6 | p4) / (p1 | p2))\n\n\n\n\n\np3 / (p4 | p5 | p6) / p1\n\n\n\n4. building blocks\nHowever, even with simple mathematical operations, the use of many names and symbols can quickly become cumbersome. To simplify the code even further, we can create building blocks. For instance, when we create two building blocks, each consisting of three side-by-side plots, stacking those two blocks would put six plots on the same graph without excessive use of parentheses.\n\n\nblock1 = p1 | (p2 / p3)\nblock2 = (p4 / p5) | p6\n\nblock1 / block2\n\n\n\n5. üîé ‚Äúzoom in‚Äù\nSimilarly, we can use the {ggforce} package to zoom in on a specific plot, displaying the zoomed-in part as a separate picture beside the original one. We can then use a slash to place this complex figure below three other informative graphics. Additionally, we can easily control the size of the zoom by setting the ‚Äòzoom.size‚Äô argument higher than 1 for a wider zoom or lower than 1 for a more narrow zoom.\n\n\nlibrary(ggforce)\n(p6 | p4 | p5) / p3 + facet_zoom(ylim = c(150, 200), \n                                 zoom.size = 2)\n\n\n(p1 | p2 | p4) / p3 + facet_zoom(ylim = c(150, 200), \n                                 zoom.size = 0.5)\n\n\n\nPlot design\n6. plot_layout()\n\n\n# produce new plot to work with legends\np1 <- ggplot(Wage) + \n  geom_bar(aes(education, fill = health_ins))\np2 <- ggplot(Wage) + \n  geom_boxplot(aes(education, wage, fill = jobclass))\np3 <- ggplot(Wage) + \n  geom_point(aes(age, wage, color = education))\np4 <- ggplot(data=Wage, aes(x=education, fill=health_ins)) +\n    geom_bar(position=\"fill\")+\n    scale_y_continuous(labels=scales::percent)\np5 <- ggplot(Wage) + \n    geom_boxplot(aes(health_ins, wage, fill = jobclass))+facet_wrap(~jobclass)\np6 <- ggplot(Wage, aes(x=wage, fill = jobclass)) +\n  geom_histogram(aes(y=..density..), color = \"white\")+\n  geom_density(aes(color = jobclass))\n\n\nBut such control is optional. If you don‚Äôt feel like controlling anything, you can relax because {patchwork} will try to keep the multiplot as square as possible, deciding for you how many rows and columns your final plot has.\n\n\np1 + p2 + p3 + p4 + p5 + p6\n\n\n\nHowever, if you know exactly how many rows and columns you want, the ‚Äúplot_layout‚Äù command allows you to easily determine the dimensions of your plot, for instance using 2 columns instead of 3.\n\n\np1 + p2 + p3 + p4 + p5 + p6 + \n  plot_layout(ncol = 2, nrow = 3)\n\n\n\nnasty legends\nThe only issue we have here is with the legends (think of a ‚ÄúLegend‚Äù movie). They take up too much valuable space and repeat themselves redundantly. To solve these problems, we can add the guides = ‚Äúcollect‚Äù argument inside our layout. This would reduce the number of legends from 6 to 4 by removing two redundant ones and placing the remaining legends of different plot types on one side of the multiplot, freeing up space. However, similar legends from different plot types would still remain on the picture, so no important information would be lost.\n\n\np1 + p2 + p3 + p4 + p5 + p6 + \n  plot_layout(\n    ncol = 2, nrow = 3,\n    guides = \"collect\")\n\n\n\n7. work with fancy plots\nAnd while several ggplots of the same size fit nicely together, combining more sophisticated plots might produce odd shapes and proportions. For example, let‚Äôs say we produce plots of anova, chi-square test, model predictions, and the table of model results and try to put them all on the same graph.\n\n\nset.seed(111)         # for reproducibility \nd <- Wage %>%         # make the data set smaller\n  filter(!(education %in% c(\"2. HS Grad\", \"4. College Grad\"))) %>% \n  sample_n(111) %>% \n  mutate(wage = round(wage))\n\n# boxplot\nlibrary(ggstatsplot)  # makes cool stat-plots\na <- ggbetweenstats(  # extra video on a channel\n  data = d, \n  x    = education, \n  y    = wage)\n\n# barplot\nb <- ggbarstats(      # extra video on a channel\n  data = d, \n  x    = jobclass, \n  y    = education, \n  label = \"both\")+\n  theme(legend.position = \"top\")\n\n\n# model results\nm <- lm(wage ~ education, d)\nlibrary(sjPlot)       # extra video on a channel\nc <- plot_model(m, \n                type  = \"eff\", \n                terms = \"education\", \n                title = \"\")\n\n# model results as table\nlibrary(gtsummary)    # extra video on a channel\n# remotes::install_github(\"ddsjoberg/bstfun\")\nlibrary(bstfun)\ntable <- tbl_regression(\n         m, \n         add_pairwise_contrasts = T) %>%\n  bstfun::as_ggplot()\n\n\nJust look at this disproportional plot: the ANOVA picture contains a lot of information that is difficult to see, while the model prediction plot takes up a lot of space despite being almost empty.\n\n\na + table + c + b \n\n\n\nTo control the proportions of plots on the grid, we can add the ‚Äúwidths‚Äù and ‚Äúheights‚Äù arguments inside the ‚Äúplot_layout()‚Äù function. Setting all values to 1 would create completely equal proportions for all 4 plots. However, the model predictions plot and the table still take up too much space despite not showing much, while the ANOVA plot could use a bit more room. Let‚Äôs improve that.\n\n\n# equalize plots\na + b + c + table +\n    plot_layout(\n        ncol = 2, nrow = 2,\n        widths  = c(1, 1), \n        heights = c(1, 1))\n\n\n\nBy setting the ‚Äúwidths‚Äù argument to 2:1, the left column will be twice as wide as the right column. Similarly, setting the ‚Äúheights‚Äù argument to 2:1 would make the upper row of pictures twice as high as the lower row. Since both dimensions would reduce the size of the table by a factor of two, our table would take up only one-fourth of the area of the ANOVA plot.\n\n\n# ratio 2 to 1 for columns and rows\na + b + c + table + \n    plot_layout(\n        ncol = 2, nrow = 2,\n        widths  = c(2, 1), \n        heights = c(2, 1))\n\n\n\n8. desing a layout with ‚Äúarea‚Äù\nSpeaking of area: ‚Äúarea‚Äù is a small helper function used to specify a single area in a rectangular grid that should contain a plot. The area() function has four intuitive arguments: top, left, bottom, and right, which allow you to determine the exact position of a plot on the grid: area(t, l, b = t, r = l).\nThe usefulness of such layout design will be apparent in a moment, so please bear with me. Let‚Äôs start by placing one plot in the first row and first column, the second plot in the second row and second column, and the third plot in the third row and third column using only the top and left arguments. We can then use the design argument to apply our layout design to our multiplot.\n\n\nlayout <- c(\n    area(1, 1),\n    area(2, 2),\n    area(3, 3)\n)\n\nplot(layout) | p1 + p2 + p3 + plot_layout(design = layout)\n\n\n\nAdding other separate plots to the grid is now completely intuitive.\n\n\nlayout <- c(\n    area(1, 1),\n    area(1, 3),\n    area(2, 2),\n    area(3, 1),\n    area(3, 3)\n)\n\nplot(layout) | p1 + p2 + p3 + p4 + p5 + plot_layout(design = layout)\n\n\n\nHowever, the real power of designing your own layout comes out when you start to use bottom and right placements in the area. For instance, defining 1 for the bottom and 3 for the right placement for the first plot would keep our plot on the first row but stretch it out over three columns, which is useful when we have wide plots.\nSimilarly, using area(2, 1, 3, 1) would start our second plot on the second row and first column. Using 3 for the bottom would pull it to the third row, while 1 for the right would keep the plot in the first column, which is useful for tall plots.\nYou get the idea. In this way, you can design your plot to your heart‚Äôs content and even leave some empty space in the middle for a signature, photo, or simply for a dramatic effect.\n\n\nlayout <- c(\n    area(1, 1, 1, 3),\n    area(2, 1, 3, 1),\n    area(3, 2, 3, 3),\n    area(2, 3)\n)\n\nfancy_plot <- plot(layout) | p1 + p2 + p3 + p4 + plot_layout(design = layout)\n\nfancy_plot\n\n\n\n9. add ‚Äúnon-pictures‚Äù to your multiplot\nBy the way, we can easily\nadd an empty space with the ‚Äúplot_spacer‚Äù command,\nadd some text using the ‚ÄútextGrob‚Äù function from the {grid} package, or\nadd a simple table via the ‚ÄútableGrob‚Äù function from the {gridExtra} package.\nplot_spacer()\n\n\na + plot_spacer() + b\n\n\n\ntext\n\n\np5 + grid::textGrob(\"'It is the mark of \\n \na truly intelligent person \\n\nto be moved \\n\nby statistics.' \n~ George Bernard Shaw\") + p6\n\n\n\ntable\n\n\na + gridExtra::tableGrob(mtcars[1:10, c('mpg', 'disp')])\n\n\n\n10. overlapping by ‚Äúinset_element()‚Äù\nMoreover, if we have limited space, we can overlay one picture on top of the other. For instance, we could place a table with contrasts between the 3 errorbars to show how big the difference between them is and whether this difference is significant.\n\n\nc + \n  inset_element(p = table,\n                left   = 0.01,\n                bottom = 0.5,\n                right  = 0.5,\n                top    = 1) \n\n\n\n11. plot_annotation()\nWhile it‚Äôs common to arrange multiple plots side by side, there are occasions when we desire the final result to present a cohesive and visually appealing narrative. This is where the ‚Äòplot_annotation()‚Äô function becomes invaluable. With this function, we can not only set titles and captions but also assign tags to subplots, enhancing the overall impact of the visualization.\n\n\nfancy_plot +\n  plot_annotation(\n    title    = \"Salary and education\",\n    subtitle = \"should you invest in yourself?\",\n    caption  = \"Source: ISLR package\",\n    tag_levels = \"A\",  # 'a', '1', ‚Å†'i‚Å†, or 'I'\n  )\n\n\n\nYou can label your plots by assigning tags such as ‚Äò1‚Äô for numerals, large ‚ÄòA‚Äô for uppercase Latin letters, small ‚Äòa‚Äô for lowercase Latin letters, ‚ÄòI‚Äô for uppercase Roman numerals, and ‚Äòi‚Äô for lowercase Roman numerals. While the tags can be more intricate, the {patchwork} package documentation offers comprehensive details. However, I personally never needed to use anything beyond capital letters.\n12. use {ggplot} options\nThe final point I‚Äôd like to mention before demonstrating how to save this plot is that you can incorporate any {ggplot} feature into your plot. The only distinction is that you use the ‚Äú&‚Äù sign instead of the ‚Äúplus‚Äù sign to introduce new features to all subplots within the patchwork.\n\n\nbla <- fancy_plot +\n  plot_annotation(\n    title    = \"Salary and education\",\n    subtitle = \"should you invest in yourself?\",\n    caption  = \"Source: ISLR package\",\n    tag_levels = \"A\",  \n  ) & \n  theme_538() &\n  theme(legend.position = \"none\",\n        text=element_text(family=\"Times New Roman\"),\n        plot.title = element_text(color = \"#0099f8\", size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(face = \"bold.italic\", hjust = 0.5),\n        plot.caption = element_text(face = \"italic\"))\n\nbla\n\n\n\nSave your fancy picture\nOnce you‚Äôre satisfied with your visualization, you can save it using the ‚Äúggsave‚Äù command in your preferred format, quality, and size.\n\n\nggsave(\n  plot     = bla, \n  filename = \"fancy_plot.png\", \n  # \"pdf\", \"jpeg\", \"tiff\", \"png\", \"bmp\", \n  # \"svg\", \"eps\", \"ps\", \"tex\" or \"wmf\"\n  device = png,\n  dpi    = 1000,\n  width  = 10,\n  height = 7\n)\n\n\nWhat‚Äôs nest\nNow, combining plots is impressive, but to elevate your Data Scince skills, you should also learn how to combine tables, which will take you less than 5 minutes if you watch this video\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2023-12-16-patchwork/thumbnail_patchwork.jpg",
    "last_modified": "2023-12-22T19:37:33+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2023-11-12-boxplots/",
    "title": "Master Box-Violin Plots in {ggplot2} and Discover 10 Reasons Why They Are Useful",
    "description": "Boxplots display a wealth of useful information about the dataset. Let‚Äôs start with the most basic boxplot, build every part of this notched box-violin plot in {ggplot2} step by step, and understand why every detail matters üòâ",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-11-24",
    "categories": [
      "videos",
      "statistics",
      "visualization"
    ],
    "contents": "\n\nContents\nThis post as ca. 17 minutes video\n1. Basic box plot provides a five-number summary of the data\n2. Box plots show how a variable changes across groups or categories\n3. Boxplots emphasize outliers\nMake your plot interactive for reading it\n\n4. Be more confident by adding notches - 95% CIs for medians\n5. Add sample size\nImplicit\nExplicit\nSimplify coding\n\n6. Display distribution via the violin plot\nMake violin transparent\n\n7. Add fancy points without overlapping\nMake points transparent & colored\n\n8. Add average point with 95% CIs for the average\nPimp your plot\nChange plot design with themes\nControl titles, captions, axis\nControl legend, fonts, sizes\n\nSave your fancy picture\n9. Add more variables to produce subplots quick\nCorrect the mistakes\n\n10. What‚Äôs next? Display significance tests!\nLess important things\nDisplay groups side by side\n\n\nThis post as ca. 17 minutes video\n\n\n\n\n\n\n\n‚Ä¶ for instance, the ‚Ä¶\n1. Basic box plot provides a five-number summary of the data\nTo create a basic box plot, we‚Äôll load necessary packages and obtain the ‚ÄúWage‚Äù dataset from the ISLR package, which contains information about American salaries. We‚Äôll then use the ggplot() function with two arguments: the data and the aesthetic mappings, where we put a numeric column we want to study. Finally, we‚Äôll use the geom_boxplot() function to create a box plot, which IS the five-number summary for wages.\n\n\nlibrary(tidyverse)      # for {ggplot2} & Co. packages\nlibrary(ISLR)           # for Wage dataset\ntheme_set(theme_test()) # for beautiful plot design\n\nset.seed(777)           # for reproducibility \nd <- Wage %>% \n  sample_n(777) %>% \n  mutate(wage = round(wage))\n\nggplot(d, aes(wage))+\n  geom_boxplot()\n\n\n\nThe five-number summary (from bottom to top) is represented by the:\nminimum wage, which is the end of the lowest whisker\nfirst quartile (Q1), or the lowest horizontal line (hinge) of the box. It‚Äôs also called 25th percentile, because 25% of all the data is located between the minimum and the lowest line of the box\nmedian is the thick horizontal line in the middle of the box, which splits all the data in half and also the data in the box in half, so that both sub-boxes have 25% of the data even if they don‚Äôt look completely uneven. The median is sometimes called second quartile (Q2), or 50th percentile\nthird quartile (Q3) is the highest horizontal line (hinge) of the box. It‚Äôs also called a 75th percentile, because 75% of all the data is located between the minimum and the highest line of the box, and finally the\nmaximum wage, as the end of the highest whisker\n\nBy the way, there is the whole Nature paper dedicated to visualizing box plots, so, if you are interested what really smart people have to say about it, have a look at their work: ‚ÄúKrzywinski, M., Altman, N. Visualizing samples with box plots. Nat Methods 11, 119‚Äì120 (2014). https://doi.org/10.1038/nmeth.2813‚Äù. Here I‚Äôll just use some of their fantastic plots:\n\nThe box itself covers the middle 50% of the salaries. It goes from the 1st to the 3rd quartile, that‚Äôs why we call it the interquartile range (IQR). The whiskers stretch the box to the lowest and highest values, but exclude outliers, giving us a really good idea of what we could generally earn. However, salaries of folks with different education levels, professions, etc. would differ, right? That leads us to the second benefit of box plots‚Ä¶\n2. Box plots show how a variable changes across groups or categories\nFor example, to create side-by-side box plots that compare the distribution of wages across different education levels, we‚Äôll put education levels on the x-axis and salaries on the y-axis.\n\n\nggplot(d, aes(x = education, y = wage))+\n  geom_boxplot()\n\n\n\nThis box plot shows us that higher education levels lead to higher salaries. Half of the people with the highest degree earn more than almost anyone with the lowest degree. So, education matters! Also, big boxes mean high variation of salaries (like in ‚Äú4. College grad‚Äù), which offers more opportunities to earn money. Small boxes mean low variation (like in ‚Äú1. < HS Grad‚Äù), implying less room for career growth.\nThe people with the lowest degree are stuck with low pay. And the median of the lowest education level is very close to the lower hinge of the box. This means that, the distribution is skewed towards lower salaries. So that, the average salary in this group is misleading. It‚Äôs too high. And there are only two exceptions. Two people who earn as much as college graduates. They might be entrepreneurs. This leads us to the next benefit of box plots.\n3. Boxplots emphasize outliers\nBut what is an outlier and why does it matter? In brief, outliers are data points significantly divergent from the rest, lying beyond 1.5 times the interquartile range (IQR) from the nearest box hinge (Q1 or Q3).\n\\[Lower \\ whisker = Q1 - 1.5*IQR\\]\n\\[Upper \\ whisker = Q3 + 1.5*IQR\\]\n\nWhile outliers are often errors, they can also result from unusual events or provide new and interesting insights. Therefore, outliers do not always need to be removed. However, if a point falls outside 3 times the interquartile range (IQR), it can be deemed ‚Äòextreme‚Äô and should most likely be excluded from the dataset. In any case, both extreme points and outliers can skew our results, and should therefore be emphasized. Which can be easily done by specifying the color, size, and shape of outliers inside the geom_boxplot() function:\n\n\nggplot(d, aes(x = education, y = wage))+\n  geom_boxplot(outlier.color = \"red\",\n               outlier.size  = 2,\n               outlier.shape = 8)\n\n\n\nMake your plot interactive for reading it\nThe only problem with this box plot is that sometimes we want to know the exact values of ourliers or the 5-point summary. However, we have to guess it by staring at the y-axis. But there is a solution to it, namely, to make our plot interactive by:\nloading the {plotly} library\nwrapping our R-code into a ggplotly() command and\npointing to the extreme earners in the high school grade, the Steve Jobses or Mark Zuckerbergs of the world, or finding out the median and quartiles, in order to confidently understand what 50% of people earn.\n\n\nlibrary(plotly)\nggplotly(\n  ggplot(d, aes(x = education, y = wage))+\n  geom_boxplot(outlier.color = \"red\",\n               outlier.size  = 2,\n               outlier.shape = 8)\n  )\n\n\n\n4. Be more confident by adding notches - 95% CIs for medians\nSpeaking of confidence, while the median represents a more robust measure of central tendency compared to the mean, to enhance our confidence in the median, we can effortlessly add 95% confidence intervals to it. This is displayed as a notch around the median on the boxplot. To achieve this, we simply include the ‚Äúnotch = TRUE‚Äù argument inside the ‚Äúgeom_boxplot()‚Äù command:\n\n\nggplot(d, aes(x = education, y = wage))+\n  geom_boxplot(outlier.color = \"red\",\n               outlier.size  = 2,\n               outlier.shape = 8,\n               notch = TRUE)\n\n\n\nThe Notch is important for two reasons. The first is - overlapping. Namely, if notches of neighboring boxplots overlap, there is a strong evidence, that samples don‚Äôt differ. On the other hand, if the notches don‚Äôt overlap, it indicates that the samples might differ significantly. However, only a real statistical test can confirm this, and we‚Äôll get to that at the end of the video.\n\nThe second reason the notch is important is that it gives us a hint about whether our sample size is too small or sufficiently big. Here‚Äôs how we know: the notch is calculated as the median plus or minus 1.57 times the interquartile range (IQR) divided by the square root of our sample size:\n\\[Median \\pm \\frac{1.57*IQR}{\\sqrt n}\\]\n\nSo, if the notch stays inside the box, our sample size is sufficiently big, and we can trust our data, being confident in our median. But if the notch goes outside the box, we can not trust our data because our sample size is too small.\n\nBy the way, a small sample size is the reason the notches in box plots are rarely shown in scientific studies. I am also guilty of that. So, if YOU display notches, your box plots will be much better than those of most scientific studies. And since sample size is so crucial, displaying it will make your box plot even more informative. And there are two ways to add sample size to every plot: implicitly and explicitly.\n5. Add sample size\nImplicit\nThe implicit way is similar to notches, where we do not show the real numbers. Using the ‚Äúvarwidth = TRUE‚Äù argument inside the ‚Äúgeom_boxplot()‚Äù function will adjust the width of every box, scaled by \\(\\sqrt n\\). The wider the box, the more data it contains.\n\n\nggplot(d, aes(x = education, y = wage))+\n  geom_boxplot(outlier.color = \"red\",\n               outlier.size = 2,\n               outlier.shape = 8,\n               notch = TRUE,\n               varwidth = TRUE)\n\n\n\nExplicit\nHowever, it‚Äôs always more useful to see the actual numbers. To achieve this, we‚Äôll first count the number of observations per education group and save it in a separate small data frame. Then, we‚Äôll use this data frame to display discrete labels on the x-axis below every boxplot, including:\nthe name of the education group,\na division fraction represented by ‚Äúbackslash n‚Äù,\nthe text ‚Äún =‚Äù, and finally,\nthe actual numbers themselves;\n‚Ä¶ and if we look at the result, we‚Äôll confirm that the widest box corresponds to the highest number, while the narrowest box represents the lowest number.\n\n\n# count education groups first\ncounts <- d %>% \n  group_by(education) %>% \n  count()\n\ncounts\n\n# A tibble: 5 √ó 2\n# Groups:   education [5]\n  education              n\n  <fct>              <int>\n1 1. < HS Grad          67\n2 2. HS Grad           254\n3 3. Some College      171\n4 4. College Grad      183\n5 5. Advanced Degree   102\n\nggplot(d, aes(x = education, y = wage))+\n  geom_boxplot(outlier.color = \"red\",\n               outlier.size = 2,\n               outlier.shape = 8,\n               notch = TRUE,\n               varwidth = T)+ # T = TRUE\n  scale_x_discrete(\n    labels = paste(counts$education, \"\\n n = \", counts$n) )\n\n\n\nHere is an IMPORTANT detail: it is highly recommended to have at least 5 observations per group for the boxplot to be useful!\nSimplify coding\nNow, as the code becomes more bulky, adding new features to the plot may become overwhelming. To streamline our programming journey, we can save the code we‚Äôre satisfied with into a separate object at any point and use it for further modifications.\n\n\np <- ggplot(d, aes(x = education, y = wage))+\n  geom_boxplot(outlier.color = \"red\",\n               outlier.size = 2,\n               outlier.shape = 8,\n               notch = TRUE,\n               varwidth = T)+\n  scale_x_discrete(\n    labels = paste(counts$education, \"\\n n = \", counts$n) )\n\np\n\n\n\n6. Display distribution via the violin plot\nFor example, we could use ‚Äúp‚Äù to add the violin plot. Why violins? Well, violin plots might be even more informative than box plots because they show the distribution of data along both the horizontal and vertical axes, revealing its shape, symmetry, and spread.\n\nThis can be used to identify the most common wage values, the number of peaks, and to see whether the data is skewed or symmetric (or normally distributed. A violin plot is created by using a mathematical technique called kernel density estimation, which basically smooths out the data and makes it look like a violin.)\n\n\np + \n  geom_violin()\n\n\n\nIf we overlay the violin on top of the box plot, they would overlap. However, we can make the violin transparent using the ‚Äúalpha‚Äù argument, which ranges from 0 to 1. The closer alpha is to zero, the more transparent the violin becomes.\nMake violin transparent\n\n\np1 <- p + \n  geom_violin(alpha = 0.01)\n\np1\n\n\n\n7. Add fancy points without overlapping\n\n\na <- p1 + \n  geom_point() \n\nb <- p1 + \n  geom_jitter() \n\nlibrary(patchwork)\na + b\n\n\n\nThe usefulness of the violin plot lies in implicitly showing the data points. However, even though we don‚Äôt necessarily need to see the data points, there are times when we still want to explicitly visualize them. ‚ÄúNow, listen to me very carefully: (says Terminator 800): avoid using‚Äùgeom_point‚Äù or ‚Äúgeom_jitter‚Äù to display points. Those commands are found in many tutorials and I have used them too. Until I found a better choice: ‚Äúgeom_sina()‚Äù from the {ggforce} package, where ‚Äúsina‚Äù stands for Strength, Victory and Beauty! üòâ\n\n\np1 + \n  ggforce::geom_sina()\n\n\n\nMake points transparent & colored\nHere we can see that the points now completely fill out the violin, which makes more sense compared to ‚Äúgeom_point‚Äù and ‚Äúgeom_jitter.‚Äù However, they completely overshadow the box plot. Thus, similar to the violin, we can first make the points more transparent by applying the ‚Äúalpha‚Äù argument, and then go one step further and paint them in different colors, to literally make our plot a bit more aesthetic üòÅ.\n\n\nlibrary(ggforce)\np2 <- p1 + \n  geom_sina(alpha = 0.3, aes(color = education))\n\np2\n\n\n\nHaving all those beautiful points is amazing, however, one of the most important points is still missing - the average. But why do we need the average if we have the median in the center of the box? First of all, people understand the mean better than the median, and secondly, having both mean and median on the same plot will emphasize the skewness of data.\n8. Add average point with 95% CIs for the average\nNamely, when the mean and the median are close to each other, the data is most likely normally distributed, and we can use t-tests or ANOVA to compare groups. While when the mean lies far from the median, the data is skewed, and we would need to apply non-parametric methods, such as Mann-Whitney U or Kruskal-Wallis tests, to compare groups.\nTo display the mean,\nwe‚Äôll use the ‚Äústat_summary()‚Äù function,\nset the ‚Äúfun.data‚Äù argument to ‚Äúmean_cl_boot,‚Äù and\nthe ‚Äúgeom‚Äù argument to ‚Äúpoint‚Äù, and finally\nwe‚Äôll color our average blue and increase its size to make it stand out on the plot.\n\n\np2 +\n  stat_summary(fun.data = mean_cl_boot, geom = \"point\",\n               color = \"blue\", size = 3)\n\n\n\nHowever, an average without confidence intervals is like a political statement‚Äîsounds very confident, but nobody is confident it will be done. So displaying confidence intervals (CIs) around the mean is as important as displaying the notches around medians. For that, we‚Äôll use the same command ‚Äústat_summary‚Äù with a new argument: geom = ‚Äúerrorbar‚Äù.\nAdding the average with 95% confidence intervals makes any box plot even more informative and elegant. And it‚Äôs not only my opinion. Namely, a scientific paper about ‚ÄúVisualizing samples with box plots‚Äù published in Nature says:\n‚ÄúBox plots may be combined with sample mean and 95% CI error bars to communicate more information about samples in roughly the same amount of space.‚Äù 1\n\n\n\np3 <- p2 +\n  stat_summary(fun.data = mean_cl_boot, geom = \"point\",\n               color = \"blue\", size = 3)+\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\",\n               color = \"blue\", size = 1, width = 0.4)\n\np3\n\n\n\nWhat information is it, you might wonder? Well, let‚Äôs check this out by making this plot interactive with ‚Äúggplotly‚Äù. According to the highest education degree, we should earn $143,000 on average. But since:\nthe median with $133,000 is far below the average and not even inside the confidence bands of the average, and\nsince the widest part of the violin, which shows what the majority of people earn, is also far below the mean at $119,000,\n‚Ä¶ reporting the average salary of $143,000 on skewed data will overpromise and disappoint. That‚Äôs why I personally get a bit skeptical when any source reports averages instead of medians. But I‚Äôd be interested to know what you think, so feel free to let me know in the comments section below.\n\n\nggplotly(p3)\n\n\n\nPimp your plot\nChange plot design with themes\nNow, if this plot is not looking nice enough to you yet, you can quickly improve it‚Äôs appearance, by choosing a new plot-design with a single command. Here are some useful themes for your convenience:\ntheme_bw()\ntheme_classic()\ntheme_minimal()\ntheme_test() - which we already used before the whole time.\n\n\np4 <- p3 +\n  theme_bw()\n\np4\n\n\n\nControl titles, captions, axis\nThen, you can specify titles, captions, axis names within the ‚Äúlabs‚Äù command:\n\n\np5 <- p4 +\n  labs(\n    title    = \"Salary and education\",\n    subtitle = \"should you invest in yourself?\",\n    caption  = \"Source: ISLR package\",\n    x = \"Education levels\",\n    y = \"Salary in US$\"\n  )\n\np5\n\n\n\nControl legend, fonts, sizes\nAnd finally, you can move or remove the legend, adjust color, size, font, or even the location of text on the plot. For instance, we can choose the ‚ÄúTimes New Roman‚Äù font on our plot to match the text font in our publication. It‚Äôs just effective! But we are just scratching the surface because the next thing you‚Äôll learn will absolutely blow your mind. But before we loose this beautiful plot, I want to show you how to save it.\n\n\np6 <- p5 +\n  theme(legend.position = \"none\",\n        plot.title = element_text(\n          color = \"#0099f8\", size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(face = \"bold.italic\", hjust = 0.5),\n        plot.caption = element_text(face = \"italic\"),\n        axis.title.x = element_text(color = \"blue\", size = 14, \n                                face = \"bold\"),\n    axis.title.y = element_text(size  = 14, face=\"italic\"),\n    text = element_text(family = \"Times New Roman\"))\n\np6\n\n\n\nSave your fancy picture\nOnce you‚Äôre satisfied with your visualization, you can save it using the ‚Äúggsave‚Äù command in your preferred format, quality, and size. And if you are satisfied with this video (or article) so far, consider hitting the like üëç button.\n\n\nggsave(\n  \"magic_boxplot.jpeg\", \n  # \"pdf\", \"jpeg\", \"tiff\", \"png\", \"bmp\", \n  # \"svg\", \"eps\", \"ps\", \"tex\" or \"wmf\"\n  device= jpeg,\n  plot  = p6,\n  width = 10,\n  height= 7, \n  dpi   = 1000)\n\n\n9. Add more variables to produce subplots quick\nNow, while visualizing education levels is already very insightful, we can go one step further and add two more variables to our plot. ‚ÄòYes, we can,‚Äô as Obama would say. By employing the ‚Äòfacet_grid‚Äô or ‚Äòfacet_wrap‚Äô functions with two additional categorical variables, we can generate distinct subplots, each representing different combinations of these categorical variables.\n\n\np6 +\n  facet_grid(jobclass ~ health_ins)\n\n\n\nCorrect the mistakes\nThese additions provide even more useful information but also introduce some problems.\nFirst, R warns us that ‚ÄúNotches went outside hinges‚Äù, primarily due to a small sample size causing larger confidence intervals than the box itself. It‚Äôs advisable not to rely heavily on these 95% CIs for the median in such situations and avoid notches unless they fall entirely within the IQR. Although we initially followed this advice, splitting our plot into four subplots resulted in incorrect sample sizes on the x-axis. Thus, we need to either replace or remove them. For instance, let‚Äôs eliminate the entire ‚Äòscale_x_discrete‚Äô command along with all the labels.\nNext, we‚Äôll incorporate more data by using the entire ‚ÄúWage‚Äù dataset instead of the small portion we selected initially.\nSince too many points could be too dark, we‚Äôll increase the transparency of points by setting the ‚Äúalpha‚Äù parameter closer to zero.\nAnd just for fun, we can add a horizontal green line to indicate the total average salary across the whole dataset, allowing us to observe how people‚Äôs earnings compare to the average.\n\n\nggplot(Wage, aes(x = education, y = wage))+\n  geom_boxplot(outlier.color = \"red\",\n               outlier.size  = 2,\n               outlier.shape = 8,\n               notch         = TRUE,\n               varwidth      = T)+\n  geom_violin(alpha = 0.1)+\n  geom_sina(alpha   = 0.15,  aes(color = education))+\n  stat_summary(fun.data = mean_cl_boot, geom = \"point\",\n               color = \"blue\", size = 3)+\n  stat_summary(fun.data = mean_cl_boot, geom = \"errorbar\",\n               color = \"blue\", size = 1, width = 0.4)+\n  labs(\n    title    = \"Salary and education\",\n    subtitle = \"should you invest in yourself?\",\n    caption  = \"Source: ISLR package\",\n    x = \"Education levels\",\n    y = \"Salary in US$\"\n  )+\n  theme_bw()+\n  theme(legend.position = \"none\",\n        text=element_text(family=\"Times New Roman\"),\n        plot.title = element_text(color = \"#0099f8\", size = 16, face = \"bold\", hjust = 0.5),\n        plot.subtitle = element_text(face = \"bold.italic\", hjust = 0.5),\n        plot.caption = element_text(face = \"italic\"))+\n  facet_grid(jobclass ~ health_ins) +\n  # set some threshold\n  geom_hline(yintercept = mean(Wage$wage, na.rm = T) , linetype = \"dashed\", color = \"green\", size = 1)\n\n\n\n10. What‚Äôs next? Display significance tests!\nSo, notched box-violin plots are great for visualizing and comparing the distributions of different groups, but how can we test whether the differences are statistically significant? What if I told you that you can do that with just four words of code? Yes, you heard me right. With only four words, you can perform both frequentist and Bayesian ANOVAs on your data, and even show pairwise post-hoc tests on the same plot. And if you want to learn how to do it and how to interpret every number on this plot, check out my video on ANOVA (with ggstatplot by Indrajeet Patil 2).\n\n\n\nLess important things\nDisplay groups side by side\n\n\nggplot(d, aes(x = education, y = wage, fill = jobclass))+\n  geom_boxplot(outlier.color = \"red\",\n               outlier.size  = 2,\n               outlier.shape = 8,\n               notch = TRUE,\n               position = position_dodge(width = 0.9))+\n  geom_violin(position = position_dodge(width = 0.9), alpha = 0.1)\n\n\n\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\nKrzywinski, M., Altman, N. Visualizing samples with box plots. Nat Methods 11, 119‚Äì120 (2014). https://doi.org/10.1038/nmeth.2813 https://www.nature.com/articles/nmeth.2813.pdf?proof=true‚Ü©Ô∏é\nPatil I (2018). ggstatsplot: ‚Äòggplot2‚Äô Based Plots with Statistical\nDetails. doi: 10.5281/zenodo.2074621 (URL:\nhttps://doi.org/10.5281/zenodo.2074621), <URL:\nhttps://CRAN.R-project.org/package=ggstatsplot>.‚Ü©Ô∏é\n",
    "preview": "posts/2023-11-12-boxplots/thumbnail_boxplot.png",
    "last_modified": "2023-11-24T16:28:39+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2023-10-23-scatterplots/",
    "title": "7 Reasons to Master Scatter Plots in {ggplot2} with World Happiness Data",
    "description": "Today, we'll explore happiness data and uncover seven compelling reasons why scatter plots are indispensable for data analysis. You‚Äôll learn about (1) whether money can actually make you happy, (2) how wealth has changed in the USA, Germany, India, and Venezuela over the past 20 years, (3) whether happy people live longer, and much more. The results might surprise you üòâ",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-11-12",
    "categories": [
      "videos",
      "statistics",
      "visualization"
    ],
    "contents": "\n\nContents\nThis post as 14 minutes video\nClick here to get World Happiness Dataset\n1. Identifying relationships between variables\n2. Identifying Outliers Interactively\n3. Identifying patterns and trends over time (aka time series)\n4. Assessing Linearity\n5. Recognize clusters of data\n6. Multivariate Exploratory Analysis (MEA üêê) to tell a compelling story\nPimp your plot\n\n7. Correlation Analysis, or what‚Äôs next?\nConclusion\n\nThis post as 14 minutes video\n\n\n\n\n\n\n\nClick here to get World Happiness Dataset\n\n\n1. Identifying relationships between variables\nThe first reason scatter plots are useful is that scatter plots show the relationship between two continuous variables. For instance, let‚Äôs take the freely available World Happiness Dataset and generate a scatter plot illustrating the relationship between happiness and wealth:\n\n\nlibrary(readxl)         # to read the excel file\nlibrary(janitor)        # to clean names\nlibrary(tidyverse)      # for {ggplot2} and Co. packages\ntheme_set(theme_test()) # for beautiful plot design\n\nhappy_data <- read_excel(\"DataForTable2.1WHR2023.xls\") %>% \n  clean_names() %>% \n  rename(happiness = life_ladder,\n         wealth    = log_gdp_per_capita,\n         country   = country_name,\n         choise    = freedom_to_make_life_choices,\n         life_expectancy = healthy_life_expectancy_at_birth) \n\n\nAfter loading packages and data and renaming some variables for our convenience, we can create the scatter plot by employing the ‚Äòggplot()‚Äô function. This function requires two arguments: the ‚Äòdata‚Äô and the ‚Äòaesthetic‚Äô mappings. These aesthetic mappings determine the variables to be mapped onto the x- and y-axes of the plot. Lastly, we‚Äôll utilize the ‚Äògeom_point()‚Äô function to display points that connect wealth on the x- with happiness on the y-axes.\n\n\nggplot(data = happy_data, aes(x = wealth, y = happiness)) +\n  geom_point()\n\n\n\nThis scatter plot reveals that wealth has a positive relationship with happiness: the wealthier people get, the happier they seem to be. So, all those memes about ‚Äúmoney doesn‚Äôt make you happy‚Äù are only true if you don‚Äôt have any money. The only problem with this plot is that some counties clearly stand out from the trend. This brings us to the second obvious advantage of using scatter plots: they help to identify outliers immediately.\n2. Identifying Outliers Interactively\nOutliers are data points that significantly differ from the rest of the data. These outliers may represent errors or anomalies in the data, unusual events, or even interesting findings. In any case, they can skew our results, so it‚Äôs important to clearly identify them. And there‚Äôs a simple way to achieve that - by making this plot interactive. For that, we‚Äôll:\nload the {plotly} library,\ngroup our data by country inside of aesthetics so that the country will be displayed on the plot,\nand wrap our plot inside of ‚Äúggplotly‚Äù command.\n\n\nlibrary(plotly)\n\npic <- ggplot(happy_data, aes(x = wealth, y = happiness, group = country)) +\n  geom_point()\n\nggplotly(pic)\n\n\n\nHere we can identify countries with both low wealth and low happiness scores, such as the Central African Republic or Burundi, as well as the happiest and wealthiest countries, like Luxembourg or Switzerland. But the most interesting countries are the outliers, such as happy countries with not much money, like Venezuela.\nHowever, this plot has two issues: firstly, it appears quite crowded, and secondly, it fails to indicate whether this relationship changes over time. This brings us to the third reason why scatter plots are useful: their capability to identify patterns and trends over time, commonly referred to as time series analysis.\n3. Identifying patterns and trends over time (aka time series)\nTo declutter our plot, let‚Äôs focus on just a few countries: the US, Germany, India, and, for fun, Venezuela, as it stands out as an interesting outlier. We‚Äôll explore whether the wealth of these countries has changed over time. To do this, we only need to add one more argument to the aesthetic mappings: ‚Äòcolor‚Äô.\n\n\nd <- happy_data %>%\n  filter(country %in% c(\"United States\", \"Germany\", \"India\", \"Venezuela\"))\n\npic <- ggplot(d, aes(x = year, y = wealth, color = country)) +\n  geom_point()\n\nggplotly(pic)\n\n\n\nAnd voila, the plot reveals that India has experienced the highest GDP per capita growth in the last 20 years, while the wealth of the US and Germany has seen minimal increases. These three trends are quite neat and linear, allowing us to make short-term predictions about the future of these countries. However, the wealth dynamics of Venezuela are much more dramatic and far from linear. This leads us to the fourth reason why scatter plots are useful: they help us assess linearity in our data.\n4. Assessing Linearity\nAssessing linearity is crucial for selecting appropriate statistical models. For example, if we were to apply a classic linear model in some statistical software without examining the scatter plot, we might mistakenly conclude that Venezuela‚Äôs wealth rapidly decreased over the last 20 years. However, when we use a non-linear model, which is the default inside the ‚Äògeom_smooth‚Äô function (that‚Äôs why we don‚Äôt need to specify anything inside), we realize that there are actually two linear trends: one depicts linear growth from 2005 to 2012, and the other illustrates a linear decline after 2012.\n\n\na <- pic + geom_smooth(method = lm) \nb <- pic + geom_smooth() \n\nlibrary(patchwork)\na + b\n\n\n\nThese two trends separate data into two clear distinct groups, which, if overlooked, may lead to opposite results. This brings us to the next advantage that scatter plots offer: recognizing clusters within data.\n5. Recognize clusters of data\nThe significance of paying attention to grouping patterns is best explained by Simpson‚Äôs Paradox (imagine Homer Simpson). Simpson‚Äôs Paradox is a statistical phenomenon where a trend may initially appear within individual data groups but disappears or even reverses when these groups are combined. To enhance the distinction between these groups visually, you can make use of the ‚Äòstat_ellipse‚Äô function.\n\n\na <- ggplot(d, aes(x = life_expectancy, y = happiness)) +\n  geom_point()+\n  geom_smooth(method = lm)+ \n  stat_ellipse()\n\nb <- ggplot(d, aes(x = life_expectancy, y = happiness, color = country)) +\n  geom_point()+\n  geom_smooth(method = lm)+ \n  stat_ellipse() \n\nlibrary(patchwork)\na + b\n\n\n\nIn our example, the overall trend shows that happiness increases with life expectancy. However, as we delve into these trends within groups, we see that happiness increases only for Americans and Venezuelans. For Germans, the trend disappears as they remain similarly happy regardless of life expectancy. In the case of Indians, the longer they live, the less happy they become. Could this really be true? If you have any insights into why this might be the case, please share them in the comments below. This genuinely puzzles me.\n6. Multivariate Exploratory Analysis (MEA üêê) to tell a compelling story\nNow, since ‚Äúcolor‚Äù provided completely new information to our two-dimensional plot, we could see ‚Äúcolor‚Äù argument as a third dimension. Similarly, we can introduce five additional dimensions, which will transform a simple scatter plot into a captivating narrative. Think of it as a form of Multivariate Exploratory Analysis (MEA üêê), but without the complexity of hardcore statistics. And here is how.\nTo incorporate first three additional dimensions, we will start by adding the ‚Äúshape‚Äù and ‚Äúsize‚Äù arguments to the aesthetic settings along with already existing ‚Äúcolor‚Äù. Then, we‚Äôll add ‚Äúyears‚Äù as labels for each plot using ‚Äúgeom_label_repel‚Äù function from {ggrepel} package. Here, we can use ‚Äúmax.overlaps‚Äù argument to reduce the overlapping of labels.\n\n\nset.seed(1) \npic <- ggplot(d, aes(x = life_expectancy, y = happiness, \n                shape = country, \n                size  = wealth,\n                color = perceptions_of_corruption))+\n  geom_point()+\n  # install.packages(\"ggrepel\") if needed\n  ggrepel::geom_label_repel(aes(label = year), size = 1, max.overlaps = 100)\n\npic\n\n\n\nThe third dimension, ‚Äúshape‚Äù, now represents different countries.\nThe dimension of wealth, which is represented by ‚Äúsize‚Äù, underscores that Venezuela undergoes the most significant fluctuations in wealth between years, while other countries maintain relatively consistent wealth levels over time.\nIn terms of the perception of corruption, indicated by the blue color (where darker shades represent less corruption), Germans tend to believe that corruption in government and business is not a significant issue, especially in recent years. Interestingly, in years before 2012, when Germans perceived more corruption, both the happiness score and life expectancy were lower compared to later years when they became more relaxed about corruption and became happier.\nIn contrast, both Indians and Venezuelans became less happy after 2015. Interestingly, both of these groups believe that politicians and businesses in their countries are corrupt. However, it‚Äôs intriguing to note that while Venezuelans react similarly to Germans ‚Äì associating more corruption with lower happiness ‚Äì the opposite seems true in India. In India, darker colors (indicating lower perceived corruption) correspond to lower happiness scores and higher life expectancy.\nOnce again, India presents a unique pattern in this regard, and I‚Äôm intrigued by the reasons behind this.\nTo introduce two more dimensions, we‚Äôll start by categorizing the ‚Äúsocial_support‚Äù and ‚Äúgenerosity‚Äù variables. We‚Äôll divide them based on whether they fall below or above the median. For ‚Äúsocial_support,‚Äù which signifies having someone to rely on in times of need, such as relatives or friends, we‚Äôll categorize individuals as either ‚Äúlonely‚Äù or those who ‚Äúhave loved ones.‚Äù As for ‚Äúgenerosity,‚Äù which pertains to donating money to a charity in the past month, we‚Äôll classify individuals as either ‚Äúgreedy‚Äù or ‚Äúgenerous.‚Äù\nTo visualize these two categorical variables as two additional dimensions of information, we‚Äôll use the ‚Äúfacet_grid‚Äù or ‚Äúfacet_wrap‚Äù functions. This will generate four distinct subplots, each representing different combinations of these categorical variables. Additionally, the ‚Äúposition_jitter‚Äù argument inside the ‚Äúgeom_point‚Äù function will help reduce the overlapping of points.\n\n\nmea <- d %>% \n  mutate(social_support = case_when(\n    social_support > median(social_support, na.rm = T) ~ \"have loved ones\", \n    TRUE ~ \"lonely\")) %>% \n  mutate(generosity = case_when(\n    generosity > median(generosity, na.rm = T) ~ \"generous\", \n    TRUE ~ \"greedy\"))\n\nset.seed(1)\npic2 <- ggplot(mea, aes(x = life_expectancy, y = happiness, \n                shape = country, \n                size  = wealth,\n                color = perceptions_of_corruption)) + \n  facet_grid(generosity ~ social_support)+\n  geom_point(position = position_jitter(width = .5, height = .5))+\n  ggrepel::geom_label_repel(aes(label = year), size = 1, max.overlaps = 100)\n\npic2\n\n\n\nThis adds an even more intriguing layer to our story. Specifically, only the Germans appear in every subplot. What‚Äôs the common thread? Well, it turns out that most Germans have someone they can rely on in times of trouble. However, when it comes to generosity, they‚Äôre split right down the middle ‚Äì with half of them displaying a generous spirit and the other half leaning toward thriftiness. Having spent half of my life in Germany, I can certainly relate to this.\nAs for Americans, there are no individuals classified as both greedy and lonely at the same time. In fact, most U.S. citizens are happy and generous.\nIn contrast, the majority of Venezuelans are categorized as greedy, and there are no generous Venezuelans who have someone they can rely on, indicating a lack of trust and economical troubles in the country. Interestingly, two data points where Venezuelans are generous coincide with low wealth. This suggests a positive correlation between high generosity and low wealth :).\nAnd then there‚Äôs India. The data suggests that many Indians fall into the ‚Äúlonely‚Äù category, lacking someone they can truly rely on in times of need. This is surprising, especially considering that India has recently become the most populated country.\nThese patterns paint a fascinating picture, raising many questions about culture, societal values, and the complex relationship between wealth, generosity, corruption and social support. With the inclusion of eight variables instead of just two, what started as a simple scatter plot has transformed into a Multidimensional Exploratory Machine, offering deeper insights from complex datasets. I personally find it fascinating, but I‚Äôd like to hear what you guys think, so definitely let me know in the comments section below.\nPimp your plot\nBefore we delve into the seventh and final advantage of scatter plots, let me demonstrate a quick way to enhance the appearance of your plot:\nTo begin with, you can control the appearance of the regression line. You can make it dashed, change its color, or eliminate confidence intervals.\nTitles, captions, and axis labels are easily customizable via ‚Äúlabs‚Äù command.\nYou can manually adjust shapes, colors, sizes, and more.\nThe ‚Äútheme‚Äù argument provides extensive customization options. For me, the most valuable ones include the ability to position the legend and specify colors, sizes, and fonts for titles, captions, and axes.\nWhat‚Äôs even more exciting is that you can easily transform your plot‚Äôs overall design by applying pre-designed themes. For example, you can utilize the ‚ÄúCleveland‚Äù theme from the {ggpubr} package.\nAdditionally, incorporating horizontal or vertical lines can effectively highlight significant thresholds.\n\n\npic3 <- pic2 + \n  geom_smooth(method = lm, linetype=\"dashed\", color = \"red\", se = T)+ \n  # control titles, captions and axis labels\n  labs(\n    title    = \"World Happiness: 2005 - 2022\",\n    subtitle = \"Example of 4 countries\",\n    caption  = \"Source: https://worldhappiness.report/data/\",\n    x        = \"Healthy Life Expectancy (years)\",\n    y        = \"Happiness score or subjective well-being\")+\n  # control shape, color and size\n  scale_shape_manual(values=c(15, 16, 17, 18))+ \n  # format the text\n  theme(\n    legend.position = \"left\",\n    plot.title   = element_text(color = \"blue\", size = 15),\n    plot.subtitle= element_text(face  = \"bold\"),\n    plot.caption = element_text(face  = \"italic\"),\n    axis.title.x = element_text(color = \"blue\", size = 14, \n                                face = \"bold\"),\n    axis.title.y = element_text(size  = 14, face=\"italic\"))+\n  # control scales\n  scale_x_continuous(breaks = c(55, 60, 65, 70)) +\n  scale_y_continuous(breaks = seq(2, 8, by = 1)) +\n  # change plot design\n  ggpubr::theme_cleveland()+\n  # set some threshold\n  geom_hline(yintercept = median(happy_data$happiness, na.rm = T) , linetype = \"dashed\", color = \"green\")\n\npic3\n\n\n\nI understand that the plot may appear crowded and overwhelming, but the good news is - you won‚Äôt often need all these options at the same time, but you‚Äôll find them valuable for different projects. Here, I‚Äôve showcased what‚Äôs possible to include in a scatter plot, but it‚Äôs your choice to decide which elements to use.\nFinally, once you‚Äôre satisfied with your visualization, you can save it using the ‚Äúggsave‚Äù command in your preferred format, quality, and size. And if you are satisfied with this article so far, consider to subscribe to my YouTube channel, where you‚Äôll find more useful topics.\n\n\nggsave(\n  \"fancy_plot.png\", \n  plot = pic3,\n  # \"pdf\", \"jpeg\", \"tiff\", \"png\", \"bmp\", \n  # \"svg\", \"eps\", \"ps\", \"tex\" or \"wmf\"\n  device = png, \n  dpi = 1000,\n  width = 9, \n  height = 7)\n\n\n7. Correlation Analysis, or what‚Äôs next?\n\n\n\nFinally, any scatter plot with correlation lines should answer the question of whether this correlation is statistically significant! And if you want to learn how to create such plot with all these statistical details in just a single line of code and how to interpret all these numbers, be sure to check out my video on correlation.\nConclusion\nOverall, scatter plots are a versatile and powerful tool that can be used by data scientists to explore the data, identify relationships, identify outliers, identify patterns and trends, visualize data in a multidimensional space, validate models, and communicate findings effectively.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2023-10-23-scatterplots/thumbnail_scatter.png",
    "last_modified": "2023-11-12T13:46:05+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2023-08-23-histogramsdensityplots/",
    "title": "Epic Histograms & Density plots with {ggplot2}",
    "description": "Histograms display the shape of the distribution of continuous numeric data. The distribution can be symmetrical, right-skewed, left-skewed, unimodal, or multimodal. Knowing the shape of the distribution helps us decide which statistical test is appropriate. For example, if the distribution is symmetrical, we could use a t-test or linear regression. However, if the distribution is skewed, we'd need to use the Mann-Whitney test or median regression. Moreover, when the data has several peaks, we might need to transform the data before analyzing it. Otherwise, when we calculate central tendencies like the average, we will heavily misrepresent reality. Histograms also help to identify outliers, which is very useful for cleaning the data. So, visualizing the distribution with histograms and density plots helps us avoid these pitfalls.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-10-01",
    "categories": [
      "videos",
      "statistics",
      "visualization"
    ],
    "contents": "\n\nContents\nThis post as 9 minutes video\nBasic histogram\nChange the number of bins\nChange the width of bins\nAdd central tendency, SD, IQR, CIs\nAnnotations\nCompare distribution of several groups\nAdd density curves\nPimp your plot\nWhat‚Äôs next?\n\nThis post as 9 minutes video\n\n\n\n\n\n\n\nBasic histogram\n\nTo create a histogram, we‚Äôll first load the {tidyverse} meta-package, which includes {ggplot2}. Additionally, we‚Äôll load the {ISLR} package to get the ‚ÄúWage‚Äù dataset containing information about American salaries. Next, we‚Äôll use the ‚Äòggplot()‚Äô function with just two arguments: the ‚Äúdata‚Äù and ‚Äúaesthetics‚Äù. The variable we want to visualize will be assigned to the x-axis. And finally, we‚Äôll add the ‚Äògeom_histogram()‚Äô function to get the histogram itself.\nA histogram divides a numeric variable into multiple bars. The width of every bar covers a range of numeric values called a ‚Äúbin‚Äù, while a bar‚Äôs height indicates the number of data points within the corresponding bin.\n\n\nlibrary(tidyverse) # for ggplot2 & co. packages\nlibrary(ISLR)      # for Wage dataset\ntheme_set(theme_test()) # for nicer looking plots\n\nggplot(data = Wage, aes(x = wage)) +\n  geom_histogram()\n\n\n\nA warning message only informs us that 30 equal bins are used by default. But we can easily change the number of bins with a ‚Äòbin‚Äô argument. The number of bins is important; because having many bins allows us to see the precise distribution of the data. However, if there are too many bins, it can become challenging to distinguish the signal from the noise. On the other hand, with only a few bins, the histogram may lack the level of detail needed to discern any useful patterns or trends in the data.\nChange the number of bins\n\n\na <- ggplot(Wage, aes(x = wage)) +\n  geom_histogram(bins = 4)\n\nb <- ggplot(Wage, aes(x = wage)) +\n  geom_histogram(bins = 100)\n\nlibrary(patchwork)\na + b\n\n\n\nThe only issue with the bin is that we don‚Äôt precisely know the range of values it represents. That‚Äôs why the ‚Äúbinwidth‚Äù argument is much better; it allows us to specify the range of values within each bin. For example, if we set the ‚Äúbinwidth‚Äù to 50, we‚Äôll get 1702 values between 75 and 125.\nChange the width of bins\n\n\na <- ggplot(Wage, aes(x = wage)) +\n  geom_histogram(binwidth = 50)\n\nlibrary(plotly)\nggplotly(a)\n\n\n\ntable(between(Wage$wage, 75, 125))\n\n\nFALSE  TRUE \n 1298  1702 \n\na + stat_bin(binwidth = 50, geom='text', color = \"red\",\n             aes(label=..count..), \n             position=position_stack(vjust=0.5))\n\n\n\nAdd central tendency, SD, IQR, CIs\n\nSince the purpose of the histogram is to help us understand whether the distribution is symmetric or skewed, adding central tendency lines such as the mean or median can be even more helpful. We can easily include a mean line by using the ‚Äògeom_vline()‚Äô function with the ‚Äòxintercept‚Äô argument within the aesthetics.\n\n\nggplot(Wage, aes(x = wage)) +\n  geom_histogram() + \n  geom_vline(aes(xintercept = mean(wage)))\n\n\n\nWe can enhance the visual appeal of this plot by filling the bars with a blue color, outlining the bars in white, and making the line more prominent by using a red color, increasing its size, and using a dashed linetype. Now, we can clearly see that the distribution is skewed, and the Shapiro-Wilk normality test confirms that our wages are not normally distributed. Moreover, this histogram identifies a few outliers, which represent wealthy individuals which fall outside the main body of the salary distribution. So, histograms serve as data traffic lights, signaling when it‚Äôs time to stop and take a closer look.\n\n\nggplot(Wage, aes(x = wage)) +\n  geom_histogram(fill = \"blue\", color = \"white\") + \n  geom_vline(aes(xintercept = mean(wage)), color = \"red\",\n             size = 1, linetype=\"dashed\")\n\n\nshapiro.test(Wage$wage)\n\n\n    Shapiro-Wilk normality test\n\ndata:  Wage$wage\nW = 0.87957, p-value < 2.2e-16\n\nAnd while any measure of central tendency, whether it‚Äôs the mean or median, is indeed useful, what enhances our understanding even further is the addition of more vertical lines to display standard deviations or quantiles. These lines help us visualize where the majority of the data falls within the distribution. To achieve this, we begin by calculating the standard deviation, as approximately 68.26% of the data falls within one standard deviation from the mean, and about 95.44% falls within two standard deviations.\nAdditionally, we can calculate the 25th percentile (also known as the 1st Quartile) and the 75th percentile (also known as the 3rd Quartile) to depict the Interquartile Range (IQR), which encompasses the middle 50% of the salary values. Moreover, we can consider more extreme percentiles to identify where 95% of the salaries are located.\n\n\nwage_stats <- Wage %>% \n  summarise(\n    # 68.26% of data = mean ¬± 1 SD\n    # 95.44% of data = mean ¬± 2 SD = 95% CIs\n    mean_wage = mean(wage),\n    SD_wage   = sd(wage),\n    # 50% of data = IQR\n    med_est   = median(wage),\n    conf.25   = quantile(wage, 0.25 ),\n    conf.75   = quantile(wage, 0.75 ),\n    # 95% of data\n    conf.low  = quantile(wage, 0.025),\n    conf.high = quantile(wage, 0.975))\n\nwage_stats\n\n  mean_wage SD_wage  med_est  conf.25  conf.75 conf.low conf.high\n1  111.7036 41.7286 104.9215 85.38394 128.6805 52.40079  267.9011\n\nAfterward, we can utilize these values to draw four vertical lines, enhancing the descriptive power of our distribution.\n\n\na <- ggplot(Wage, aes(x = wage)) +\n  geom_histogram(fill = \"blue\", color = \"white\") + \n  # mean + SD + 95% CI\n  geom_vline(data = wage_stats, aes(xintercept=mean_wage), \n             size=1, color = \"red\")+\n  geom_vline(data = wage_stats, aes(xintercept=mean_wage + SD_wage),\n             linetype=\"dashed\", color = \"red\")+\n  geom_vline(data = wage_stats, aes(xintercept=mean_wage - SD_wage),\n             linetype=\"dashed\", color = \"red\")+\n  geom_vline(data = wage_stats, aes(xintercept=mean_wage + SD_wage*2),\n             linetype=\"dotted\", color = \"red\")+\n  geom_vline(data = wage_stats, aes(xintercept=mean_wage - SD_wage*2),\n             linetype=\"dotted\", color = \"red\")\n\nb <- ggplot(Wage, aes(x = wage)) +\n  geom_histogram(fill = \"blue\", color = \"white\") + \n  # median + IQR + 95% quantiles\n  geom_rect(aes(x = med_est, xmin = conf.low, \n                xmax = conf.high, ymin = 0, ymax = Inf), \n            data = wage_stats, alpha = 0.2, fill = \"green\") +\n  geom_rect(aes(x = med_est, xmin = conf.25, \n                xmax = conf.75, ymin = 0, ymax = Inf), \n            data = wage_stats, alpha = 0.4, fill = \"green\") +\n  geom_vline(data = wage_stats,         \n             aes(xintercept=med_est),\n             size=1, color = \"red\")\n\na + b\n\n\n\nAnnotations\nAnd of course, it‚Äôs sometimes beneficial to annotate these lines. To do this, we‚Äôll employ the ‚Äògeom_label‚Äô command with specified x and y coordinates to precisely position the annotation box. Additionally, we‚Äôll utilize the ‚Äòlabel‚Äô argument, which allows us to include both plain text and any of the calculated values:\n\n\nc <- a + geom_label(aes(x = 110, y = 450, size = 3, fontface = \"bold\", \n                label = paste(\"Mean:\", round(wage_stats$mean_wage) )))\n\nd <- b + geom_label(aes(x = 110, y = 450, size = 3, fontface = \"bold\", \n                label = paste(\"Median:\", round(wage_stats$med_est) )))\n\nc + d\n\n\n\nCompare distribution of several groups\nBut that was just the beginning. Histograms also enable us to compare the distributions of several groups. We can display them either on the same plot using the ‚Äòfill‚Äô argument, which fills the bars with different colors, or on different subplots, using ‚Äúfacet_wrap‚Äù or ‚Äúfacet_grid‚Äù functions, or even both.\n\n\nggplot(Wage, aes(x = wage, fill = jobclass)) +\n  geom_histogram(alpha=0.6)+\n  facet_wrap(~ education)\n\n\n\nAdd density curves\nBut we can take it one step further by adding a density curve to the plot. Density plots are smooth curves that represent the distribution of a data set. They provide a more accurate way to visualize the distribution and are less sensitive to outliers compared to histograms.\n\nTo create a density plot in {ggplot2}, we first need to plot the histogram in terms of relative frequency rather than absolute frequency. Absolute frequency counts the natural occurrences in each bin, while relative frequency represents the proportion of occurrences in each bin. We can achieve this by using the ‚Äòafter_stat‚Äô function to calculate proportions. By dividing the counts in each bin by the sum of all counts, we obtain the height of each bar, which represents the proportion or percentage of values falling within that bin.\nWe can use the ‚Äògeom_density()‚Äô function to fit a density curve on top of the histogram because the density curve is already expressed in proportions or percentages by default. If necessary, we can also display the percentages in the middle of our bars using the ‚Äòstat_bin‚Äô function and the ‚Äòposition‚Äô argument. Finally, the ‚Äòscale_y_continuous‚Äô function allows us to label the y-axis with percentages, for example, ‚Äò40%‚Äô instead of ‚Äò0.4‚Äô.\n\n\nggplot(iris, aes(x=Sepal.Length)) + \n  geom_histogram(aes(y = after_stat(count / sum(count))), \n                 binwidth = 1)+\n  geom_density() +\n  stat_bin(aes(y = after_stat(count / sum(count)), \n        label = scales::percent(after_stat(count / sum(count)))),\n    position = position_stack(vjust = 0.5),\n    binwidth = 1, geom = \"text\", color = \"white\"\n  )+\n  scale_y_continuous(labels = scales::percent)\n\n\n\nPimp your plot\nFinally, we can enhance the appearance of our plot by:\ndividing our data into multiple groups,\nutilizing the ‚Äúlabs()‚Äù function to specify titles, captions, and axis labels, and\nemploying the ‚Äútheme()‚Äù function to adjust the legend placement and format the text on the plot\n\n\nwage_stats <- Wage %>% \n  group_by(jobclass, education, health_ins) %>% \n  summarize(mean_wage  = mean(wage))\n\nggplot(Wage, aes(x=wage, color = jobclass)) + \n  geom_histogram(aes(y=..density..), fill = \"white\")+\n  geom_density(aes(color = jobclass), size = 1)+ \n  geom_vline(data = wage_stats,\n             aes(xintercept=mean_wage, color = jobclass),\n             linetype=\"dashed\", size=1)+\n  facet_grid(health_ins ~ education, scales = \"free\")+\n  labs(\n    title    = \"American Salaries: March 2011\",\n    subtitle = \"Money Business\",\n    caption  = \"Source: Secret Data Base Noone Knows About\",\n    x        = \"Salary (in U.S. dollars)\",\n    y        = \"Density\"\n  )+\n  theme(\n    legend.position = \"top\",\n    plot.title    = element_text(color = \"red\", size = 15),\n    plot.subtitle = element_text(face = \"bold\"),\n    plot.caption  = element_text(face = \"italic\"),\n    axis.title.x  = element_text(color = \"red\", size = 14, face = \"bold\"),\n    axis.title.y  = element_text(size = 14, face = \"italic\")\n  )\n\n\n\nWhat‚Äôs next?\nI hope you now see that displaying data in the form of histograms, density curves, and central tendencies is a useful tool for making decisions. For example, we could test our mean (in blue) against the mean of other people (in red) using a one-sample t-test to determine if our result significantly differs. However, one-sample t-tests open up a completely new chapter, and you can explore all the intricacies right here in this video or this blog-article.\n\n\n\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2023-08-23-histogramsdensityplots/thumbnail_hist_dens.png",
    "last_modified": "2023-10-01T22:32:49+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2023-07-10-barplots/",
    "title": "Epic Bar Plots with {ggplot2}",
    "description": "Bar charts are useful for visualizing categorical data, group comparisons, and effective data communication through bar labels. In this video we'll learn the secrets of producing visually stunning bar charts using the {ggplot2} package.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-08-23",
    "categories": [
      "videos",
      "statistics",
      "visualization"
    ],
    "contents": "\n\nContents\nThis post as 14 minutes video\nBasic barplots 1: count\nBasic barplots 2: identity\nBarplots with multiple groups\nStacked barplot\nDodged barplot\n\nBarplot With Error Bars: and why you shouldn‚Äôt use them!\nPercent stacked barplots\n\nWhat‚Äôs next? Barplots with Significance Tests\n\nThis post as 14 minutes video\n\n\n\n\n\n\n\nBasic barplots 1: count\nTo start, we need to load the {tidyverse} meta-package since it includes {ggplot2} and other valuable packages. We‚Äôll also need some data, such as the Wage dataset from the {ISLR} package. Typical raw data often involves cases that haven‚Äôt been counted yet. Fortunately, manual counting isn‚Äôt necessary, as counting is inherently integrated into the default functionality of the geom_bar function. To create our first bar chart using the ggplot() function, we‚Äôll only need to specify three elements: the (1) data frame, (2) aesthetics including the categorical variable education on the x-axis, and (3) the geom_bar function with the stat = ‚Äúcount‚Äù argument. Explicitly specifying stat = ‚Äúcount‚Äù is unnecessary though since it‚Äôs the default behavior. In these illustrations, the bar height signifies the case count for each education level.\n\n\nlibrary(tidyverse)\n\nWage <- ISLR::Wage\nglimpse(Wage)\n\nRows: 3,000\nColumns: 11\n$ year       <int> 2006, 2004, 2003, 2003, 2005, 2008, 2009, 2008, 2‚Ä¶\n$ age        <int> 18, 24, 45, 43, 50, 54, 44, 30, 41, 52, 45, 34, 3‚Ä¶\n$ maritl     <fct> 1. Never Married, 1. Never Married, 2. Married, 2‚Ä¶\n$ race       <fct> 1. White, 1. White, 1. White, 3. Asian, 1. White,‚Ä¶\n$ education  <fct> 1. < HS Grad, 4. College Grad, 3. Some College, 4‚Ä¶\n$ region     <fct> 2. Middle Atlantic, 2. Middle Atlantic, 2. Middle‚Ä¶\n$ jobclass   <fct> 1. Industrial, 2. Information, 1. Industrial, 2. ‚Ä¶\n$ health     <fct> 1. <=Good, 2. >=Very Good, 1. <=Good, 2. >=Very G‚Ä¶\n$ health_ins <fct> 2. No, 2. No, 1. Yes, 1. Yes, 1. Yes, 1. Yes, 1. ‚Ä¶\n$ logwage    <dbl> 4.318063, 4.255273, 4.875061, 5.041393, 4.318063,‚Ä¶\n$ wage       <dbl> 75.04315, 70.47602, 130.98218, 154.68529, 75.0431‚Ä¶\n\nggplot(data = Wage, aes(x = education)) +\n  geom_bar(stat = \"count\")\n\n\nggplot(Wage, aes(education)) +\n  geom_bar()\n\n\n\nBasic barplots 2: identity\nHowever, when dealing with preprocessed and summarized data where counts are already categorized, we can utilize stat = ‚Äúidentity‚Äù within the geom_bar function. To demonstrate this, let‚Äôs create the simplest possible data frame containing only three categories and three corresponding counts.\n\n\ndf <- tibble(\n  category = c(\"A\", \"B\", \"C\"),\n  counts = c(10, 20, 30)\n)\ndf\n\n# A tibble: 3 √ó 2\n  category counts\n  <chr>     <dbl>\n1 A            10\n2 B            20\n3 C            30\n\nThe aesthetics will now encompass not only the x-axis with a categorical variable but also the y-axis with readily available counts. The stat = ‚Äúidentity‚Äù (think of the Bourne Identity movie) argument helps to identify which counts correspond to which categories. With this straightforward example in hand, let‚Äôs discover how we can significantly enhance the visual appeal of our plot.\n\n\nggplot(data = df, aes(x = category, y = counts)) +\n  geom_bar(stat = \"identity\")\n\n\n\nFirst of all, we can effortlessly adjust the width of our grey bars by employing the width argument, setting it, for example, to 0.4:\n\n\nggplot(data = df, aes(x = category, y = counts)) +\n  geom_bar(stat = \"identity\", width = 0.4)\n\n\n\nBut gray bars appear rather dull. Thus, to reduce the monotony, we can inject some colors into the chart. For example, by using the fill argument inside of geom_bar, we can paint the bars white, while utilizing the color argument to give the edges a chocolate shade. It‚Äôs like a tiramisu ‚Äì with a chocolate coating on the outside and a creamy white filling on the inside.\n\n\nggplot(data = df, aes(x = category, y = counts)) +\n  geom_bar(stat = \"identity\", width = 0.4, \n           fill = \"white\", color = \"chocolate4\")\n\n\n\nBut if, for whatever reason, we want different colors for different bars, we can put the fill and color arguments inside of aesthetics to make it aesthetically more appealing:\n\n\nggplot(data = df, aes(x = category, y = counts, \n                    fill = category, color = -counts)) +\n  geom_bar(stat = \"identity\")\n\n\n\nBy the way, we can save our plot as an object, which not only reduces the need for excessive typing but also enables us to employ this object for subsequent manipulations:\n\n\np <- ggplot(data = df, aes(x = category, y = counts, fill = category)) +\n  geom_bar(stat = \"identity\", width = 0.777)\np\n\n\n\nFor example, we can manually fill the bars with a color of our choice, or apply custom color palettes to our object, but, to be honest, I never use them:\n\n\np + scale_fill_manual(values = c(\"#999999\", \"orange\", \"violet\"))\n\n\np + scale_fill_brewer(palette = \"Dark2\") \n\n\n\nInstead, I absolutely love and use the minimalistic grey scaled bars in varying appealing shades of gray (50 shades of gray pic ;)\n\n\np <- p + scale_fill_grey() \np\n\n\n\nHowever, what is totally not appealing is this bland, gray background. Fortunately, there‚Äôs a straightforward solution for it ‚Äì the use of themes! Themes allow us to change the plot‚Äôs design with a single command. Here are examples of minimalistic or classic themes. However, by typing ‚Äútheme_‚Äù and pressing the ‚Äútab‚Äù button, you‚Äôll be presented with multiple themes, enabling you to quickly find your personal favorite.\n\n\np + theme_minimal()\n\n\np <- p + theme_classic()\np\n\n\n\nCool, isn‚Äôt it? But do you know what‚Äôs even cooler? The fact that we can easily add labels to our bars. Everyone appreciates labels, right? :) We achieve this using the ‚Äúgeom_text‚Äù function and telling our plot-aesthetics from which column we want to extract our labels:\n\n\np + geom_text(aes(label=counts))\n\n\n\nWhile labels are indeed useful, the placement of our text is rather peculiar ‚Äì neither on the bar, nor under the bar. Additionally, they are somewhat small and challenging to discern due to their black color. Therefore, it‚Äôs necessary to enlarge their size, vertically adjust them and make their color more visible.\n\n\np + geom_text(aes(label=counts), \n              size=10, vjust=2, color=\"orange\")\n\n\n\nBut wait, there‚Äôs more ‚Äì we can actually determine the labels themselves by simply writing them down! And you won‚Äôt believe this ‚Äì we can make those labels bold!\n\n\np + \n  geom_text(\n    aes(label = c(\"BLACK\", \"IS THE NEW\", \"ORANGE\")), \n    size = 7, vjust = 2, color = \"orange\", fontface = \"bold\")\n\n\n\nAnd when vertical positioning just won‚Äôt fit all that amazing text, we can use a few tricks to bring our plot to another level:\nfirst, we can effortlessly flip the entire plot using the coord_flip function;\nthen, we‚Äôll horizontally adjust our text, ensuring it fits right near the bars;\nwe‚Äôll then extend the x-axis to 45 to make sure every bit of text is displayed;\nand finally, we can even change the order of our discrete categories for a smoother, more reader-friendly flow with a scale_x_discrete function\n\n\np <- p + \n  geom_text(\n    aes(label = c(\"BLACK\", \"IS THE NEW\", \"ORANGE\")), \n    size = 7, hjust = -0.1, color = \"orange\", fontface = \"bold\")+\n  coord_flip()+\n  ylim(0, 45)+\n  scale_x_discrete(limits = c(\"A\", \"B\", \"C\"))\np\n\n\n\nNow that our main plot is in good shape, let‚Äôs consider the legend. Well, handling the legend can be pain in the ass. But, no worries, you‚Äôre in control here. With the theme function and the legend.position argument, you can adjust the legend‚Äôs placement ‚Äì top, bottom, or no legend at all. Plus, if you need to, you can change the legend‚Äôs name and labels too to whatever you want.\n\n\na <- p + theme(legend.position = \"top\")\nb <- p + theme(legend.position = \"bottom\")\nc <- p + theme(legend.position = \"none\")\n \np <- p + theme(legend.position = \"bottom\")+ \n  scale_fill_grey(name = \"Quarter\", \n                  labels = c(\"1\", \"2\", \"3\"))\n\nlibrary(patchwork)\n(a + b) / (c + p)\n\n\n\n\n\ntheme(\n\n    # Legend title and text labels\n    #:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n    \n    # Title font color size and face\n    legend.title = element_text(color, size, face),\n    # Title alignment. Number from 0 (left) to 1 (right)\n    legend.title.align = NULL,             \n    # Text label font color size and face\n    legend.text = element_text(color, size, face), \n    # Text label alignment. Number from 0 (left) to 1 (right)\n    legend.text.align = NULL,\n    \n    # Legend position, margin and background\n    #:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n    # Legend position: right, left, bottom, top, none\n    legend.position = \"right\", \n    # Margin around each legend\n    legend.margin = margin(0.2, 0.2, 0.2, 0.2, \"cm\"),\n    # Legend background\n    legend.background = element_rect(fill, color, size, linetype),\n    \n    # Legend direction and justification\n    #:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n    # Layout of items in legends (\"horizontal\" or \"vertical\")\n    legend.direction = NULL, \n    # Positioning legend inside or outside plot \n    # (\"center\" or two-element numeric vector) \n    legend.justification = \"center\", \n    \n    # Background underneath legend keys\n    #:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n    legend.key = element_rect(fill, color),  # Key background\n    legend.key.size = unit(1.2, \"lines\"),    # key size (unit)\n    legend.key.height = NULL,                # key height (unit)\n    legend.key.width = NULL,                 # key width (unit)\n    \n    # Spacing between legends. \n    #:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n    legend.spacing = unit(0.4, \"cm\"), \n    legend.spacing.x = NULL,                 # Horizontal spacing\n    legend.spacing.y = NULL,                 # Vertical spacing\n    \n    # Legend box\n    #:::::::::::::::::::::::::::::::::::::::::::::::::::::::::::\n    # Arrangement of multiple legends (\"horizontal\" or \"vertical\")\n    legend.box = NULL, \n    # Margins around the full legend area\n    legend.box.margin = margin(0, 0, 0, 0, \"cm\"), \n    # Background of legend area: element_rect()\n    legend.box.background = element_blank(), \n    # The spacing between the plotting area and the legend box\n    legend.box.spacing = unit(0.4, \"cm\")\n)\n\n\nAnd while we can actually control everything in the legend (‚ÄòYes, we can,‚Äô Obama), most of the time we don‚Äôt need to. But what we consistently need is the ability to control Titles, Subtitles, Captions, and Axes. Let me introduce you to just two commands that let you achieve exactly that:\nthe ‚Äòlabs‚Äô command controls what we see and\nthe ‚Äòtheme‚Äô command controls how different text elements look like\n\n\np + \n  labs(\n    title    = \"Quarterly TV-show Profit (in million U.S. dollars)\",\n    subtitle = \"A simple bar chart with gray scaling, on colored issue\",\n    caption  = \"Source: Secret Data Base Noone Knows About\",\n    x        = \"Quarter of 2020\",\n    y        = \"Profit in 2020\"\n  )+\n  theme(\n    plot.title    = element_text(color = \"#0099f9\", size = 15),\n    plot.subtitle = element_text(face = \"bold\"),\n    plot.caption  = element_text(face = \"italic\"),\n    axis.title.x  = element_text(color = \"#0099f9\", size = 14, face = \"bold\"),\n    axis.title.y  = element_text(size = 14, face = \"italic\"),\n    axis.text.y   = element_blank(),\n    axis.ticks.y  = element_blank()\n  )\n\n\n\nOk, let‚Äôs summarize what we‚Äôve learned so far. The first final plot might appear as a huge chunk of code, but it‚Äôs surprisingly intuitive when broken down step by step. For instance:\nWith counts for three categories in our data frame,\nWe can fill the bars with different colors and vary the width of the bars,\nApply shades of gray for a consistent appearance,\nAlter the legend‚Äôs name, labels, and order,\nEnhance the plot‚Äôs overall visual appeal by using different themes,\nSort the bars and attach custom labels to them,\nRotate the plot 90 degrees and adjust the x-axis length,\nRevise the legend‚Äôs position or even remove the legend,\nIntroduce titles, captions and new axes names, and finally,\nModify the size and color of titles, caption, and axes ‚Äì even going bold or italic if needed.\n\n\ndf\n\n# A tibble: 3 √ó 2\n  category counts\n  <chr>     <dbl>\n1 A            10\n2 B            20\n3 C            30\n\nggplot(data = df, aes(x = category, y = counts, fill = category)) +\n  geom_bar(stat = \"identity\", width = 0.7)+ \n  scale_fill_grey(name = \"NEW TV SHOW\", \n                  labels = c(\"ORANGE\", \"IS THE NEW\", \"GRAY???\"))+\n  theme_classic()+\n  scale_x_discrete(limits = c(\"C\", \"B\", \"A\"))+\n  geom_text(\n    aes(label = c(\"DREAM BIG\",\"START SMALL\",\"ACT NOW\")),\n    color = \"black\", size = 5, hjust = -0.1)+ # vjust\n  coord_flip()+\n  ylim(0, 45)+ \n  theme(legend.position = \"bottom\") + \n  labs(\n    title    = \"Quarterly TV-show Profit (in million U.S. dollars)\",\n    subtitle = \"A simple bar chart with ca. 50 shades of gray\",\n    caption  = \"Source: Secret Data Base Noone Knows About\",\n    x        = \"CREATIVITY\",\n    y        = \"INVEST IN YOURSELF\"\n  )+\n  theme(\n    plot.title    = element_text(color = \"orange\", size = 15),\n    plot.subtitle = element_text(face = \"bold\"),\n    plot.caption  = element_text(face = \"italic\"),\n    axis.title.x  = element_text(color = \"orange\", size = 14, face = \"bold\"),\n    axis.title.y  = element_text(size = 14, face = \"italic\"),\n    axis.text.y   = element_blank(),\n    axis.ticks.y  = element_blank()\n  )\n\n\n\nAnd before we loose all these changes, we can save this plot in the format of our choice using ggsave command:\n\n\nggsave(\n  filename = \"basic_plot.jpg\",\n  plot     = last_plot(), \n  device   = jpeg, \n  width    = 5, \n  height   = 3)\n\n\nBarplots with multiple groups\nWhile this stunning bar plot featuring just one categorical variable looks gorgeous, let‚Äôs get real ‚Äì most of the time, we‚Äôre dealing with many categorical variables that need plotting. We might need to stack them, align them side by side, or even fit them into numerous subplots. So, let‚Äôs get a more realistic dataset with four categorical variables, each having multiple levels, and plot the hell out of them, starting with stacked barplots.\n\n\nlibrary(ggstats) # for stat_prop\nd <- as.data.frame(Titanic) %>%\n  dplyr::mutate(percentage = Freq/sum(Freq))\nstr(d)\n\n'data.frame':   32 obs. of  6 variables:\n $ Class     : Factor w/ 4 levels \"1st\",\"2nd\",\"3rd\",..: 1 2 3 4 1 2 3 4 1 2 ...\n $ Sex       : Factor w/ 2 levels \"Male\",\"Female\": 1 1 1 1 2 2 2 2 1 1 ...\n $ Age       : Factor w/ 2 levels \"Child\",\"Adult\": 1 1 1 1 1 1 1 1 2 2 ...\n $ Survived  : Factor w/ 2 levels \"No\",\"Yes\": 1 1 1 1 1 1 1 1 1 1 ...\n $ Freq      : num  0 0 35 0 0 0 17 0 118 154 ...\n $ percentage: num  0 0 0.0159 0 0 ...\n\nStacked barplot\nIn general, the code remains quite similar, yet it carries two fresh nuances. Firstly, instead of filling the bars along the x-axis as we did before, a method that merely results in distinct colors for x-axis bars, we‚Äôre now filling a different categorical variable with color. Secondly, we can command the geom_bar function to neatly stack the bars we‚Äôve just filled, via the position = ‚Äústack‚Äù argument.\n\n\nggplot(data = d, aes(x = Class, y = Freq, fill = Survived)) +\n  geom_bar(stat = \"identity\", position = \"stack\")\n\n\n\nDodged barplot\nHowever, that arrangement seems a bit dodgy, doesn‚Äôt it? The problem is, it hampers our ability to make effective comparisons of survived versus not-survived people on titanic. What we truly want is to have the bars grouped near each other for a clearer contrast. So, we need to change this dodgy positioning for a position_dodge.\n\n\nggplot(data = d, aes(x = Class, y = Freq, fill = Survived)) +\n  geom_bar(stat = \"identity\", position = \"dodge\")\n\n\n\nBut wait, there‚Äôs more: we can take it a step further and\nemploy the facet_grid function to incorporate two additional categorical variables, such as Age and Sex,\nattach labels to a dodged barplot, just as we‚Äôve recently did, and\nelevate the plot‚Äôs aesthetics by implementing a fresh colors and new theme.\n\n\nggplot(data = d, aes(x = Class, y = Freq, fill = Survived)) +\n  geom_bar(stat = \"identity\", position = position_dodge())+\n  facet_grid(Age ~ Sex, scales = \"free\")+\n  geom_text(aes(label=Freq), position = position_dodge(0.9),\n            vjust=-.1, color=\"black\", size=3.5)+\n  scale_fill_brewer(palette=\"Paired\")+\n  theme_bw()\n\n\n\nNot bad for just a few lines of code, right? However, we‚Äôre merely scratching the surface. The next thing I‚Äôm about to reveal will blow your mind. But before we dive into that, there‚Äôs something you might consider doing ‚Äì yet you really shouldn‚Äôt.\nBarplot With Error Bars: and why you shouldn‚Äôt use them!\nWhile I generally appreciate barplots, there‚Äôs one type I genuinely dislike: barplots with error bars, commonly seen in scientific papers. My reservations arise because these bars usually depict average values along with their 95% confidence intervals. Two reasons fuel my disapproval: first, bars are designed to represent counts rather than averages, and secondly, if we‚Äôre presenting averages as points with confidence intervals anyway, including bars seems superfluous, contributing little beyond the information conveyed by the average points already. With that being said, if you‚Äôre still inclined to create such plots, the simplest approach in my opinion involves using the stat_summary function with three distinct ‚Äúgeoms‚Äù: columns, points, and error bars. Through this method, you can calculate 95% confidence intervals either in a normal or bootstrapped manner.\n\n\ncar <- mtcars %>% \n  rownames_to_column(var = \"car_name\") %>% \n  mutate(cylinders = factor(cyl))\n\nlibrary(ggstatsplot)\nggplot(car, aes(x = cylinders, y = mpg)) +\n  stat_summary(fun = mean, geom = \"col\", fill = \"orange\") +\n  stat_summary(fun = mean, geom = \"point\", size = 5) +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\", width = 0.5) + # mean_cl_boot\n  theme_ggstatsplot()\n\n\n\nPercent stacked barplots\nNow, let‚Äôs get back to real barplots, and make them great again (pic of Trump with his stupid base cap ;). Another intuitive approach to enhance the somewhat lacking information in stacked bars is to ensure each bar group reaches 100%, don‚Äôt you think? Well, accomplishing that is very easy with the scale_y_continuous function and the percent option from the {scales} package.\n\n\nlibrary(scales)\nggplot(data = d, aes(x = Class, y = Freq, fill = Survived)) +\n  geom_bar(stat = \"identity\", position = \"fill\")+\n  scale_y_continuous(labels = percent)\n\n\n\nHowever, while the percentages on the y-axis appear impressive, having the actual percentage of each category displayed as text within each bar would be far more effective, right? To achieve this, we‚Äôll weight our Frequency and, instead of relying on the ‚Äúidentify‚Äù statistics within geom_bar, we‚Äôll explicitly opt for the ‚Äúproportion‚Äù statistics within geom_text. And by employing the position_fill function with 0.5 specified within, our text would be perfectly positioned at the exact center of each category.\n\n\nggplot(data = d, aes(x = Class, weight = Freq, fill = Survived)) +\n  geom_bar(position = \"fill\")+  # stat=\"identity\" is removed\n  scale_y_continuous(labels = percent)+\n  geom_text(stat = \"prop\", position = position_fill(0.5))+ \n  theme_test()\n\n\n\nThis plot illustrates that 5.5% of all passengers belonged to the 1st class and did not survive. While this representation might be what we want, I more often want each class to be 100% for a clear comparison of survival within each class. This can be achieved using the ‚Äúby‚Äù argument inside of aesthetics, where ‚Äúby‚Äù argument need to have identical variable to one on the x-axes.\n\n\nggplot(data=d, aes(x=Class, weight=Freq, fill=Survived, by = Class)) +\n  geom_bar(position=\"fill\")+\n  scale_y_continuous(labels=percent)+\n  geom_text(stat = \"prop\", position = position_fill(.5))+\n  scale_fill_manual(values = c(\"orange\", \"grey\"))+\n  theme_minimal()\n\n\n\nNow we observe that the survival rate of the rich is the highest at 62.5%, whereas the crew exhibited the highest sacrifice rate of 76% among all classes.\nBy the way, after weighting our frequencies, we gain the flexibility to create as many groups as needed. For example, let‚Äôs consider the ‚ÄúSex‚Äù factor. Interestingly, a majority of men did not survive, whereas the majority of women did. Moreover, it‚Äôs evident that during that period, a person‚Äôs chances of survival were positively linked to their social status: the wealthier one was, the significantly greater their chances of survival were. Not everyone will make it, but if you‚Äôve made it so far in this video, consider liking it.\n\n\nggplot(data=d, aes(x=Class, weight=Freq, fill=Survived, by = Class)) +\n  geom_bar(position=\"fill\")+\n  scale_y_continuous(labels=percent)+\n  geom_text(stat = \"prop\", position = position_fill(.5))+\n  facet_grid(~Sex)+\n  scale_fill_brewer(palette=\"Paired\")+\n  theme_test()\n\n\n\nWhat‚Äôs next? Barplots with Significance Tests\nAnd if you‚Äôre genuinely interested in determining the real statistical significance and wish to add some informative statistics to your visualization, you can conduct a quick Chi-Squared Test for both males and females by utilizing the {ggstatsplot} package.\n\n\nlibrary(ggstatsplot)\ngrouped_ggbarstats(\n  data = d, \n  x = Survived,\n  y = Class, \n  count = Freq, \n  label = \"both\",\n  grouping.var = Sex\n)\n\n\n\nHowever, as I‚Äôve already produced an extra video covering the interpretation of similar Chi-Square tests, I won‚Äôt elaborate on these numbers here. Instead, I‚Äôll display a link to this video on the screen for your convenience.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2023-07-10-barplots/thumbnail_barplots.png",
    "last_modified": "2023-08-23T11:56:41+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-08-01-sjplot/",
    "title": "R package reviews {sjPlot} How to Easily Visualize Data And Model Results",
    "description": "One picture is worth a thousand words. That's why visualizing data and model results is a crutial skill for any data scientist. {sjPlot} package became my favorite tool for visualization. That's why I want to share with you some simple but very effective commands which will make you more productive today. So, let's visualize Wage dataset, visualize bunch of models and see what people earn and what factors determine the salary.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-08-15",
    "categories": [
      "videos",
      "statistics",
      "R package reviews",
      "visualization",
      "models"
    ],
    "contents": "\n\nContents\nThis post as a video\nAll the functions you‚Äôll learn from this article\nLoad all packages at once to avoid interruptions\nPlot data\nView dataframe with view_df\nPlot frequencies with plot_frq, plot_grpfrq and plot_grid\nPlot or display cross (pivot) tables\n(not in the video) Plot grouped proportional tables (I am not sure it‚Äôs very intuitive)\nPlot histograms of salaries and display averages + SD\nPlot likert scales as centered stacked bars\n\nPlot model results\nPlot predictions\nPlot coefficients\nTable with coeffitients, 95% CIs, p-values & more\nPlot fancy models üòâüí™ü§ì\nPlot multiple models\nMore than one model\n\nWhat‚Äôs next\nFurther readings and references\n\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs less then 9 minutes long.\n\n\n\n\n\n\n\nAll the functions you‚Äôll learn from this article\nview_df(), plot_frq(), save_plot(), plot_grpfrq(), plot_grid(), plot_xtab(), tab_xtab(), plot_gpt(), plot_likert(), plot_model(), tab_model(), plot_models()\nLoad all packages at once to avoid interruptions\n\n\nlibrary(tidyverse)   # for everything useful in R ;) \nlibrary(ISLR)        # for \"Wage\" dataset about salaries\nlibrary(sjPlot)      # for easy visualization\nlibrary(likert)      # for \"pisaitems\" dataset with likert data\nlibrary(lme4)        # for mixed-effects models\nlibrary(lmerTest)    # for p-values in mixed-effects models\n\n\nPlot data\nView dataframe with view_df\nView data-frame (view_df) function with only 4 arguments, (1) your data, (2) show frequencies, (3) show percentages and (4) show missing values, displays a range of numeric variables and the counts + percentages of missing values and categorical variables, giving you a nice big picture of your data.\n\n\nview_df(Wage, show.frq = T, show.prc = T, show.na = T)\n\n\nData frame: Wage\n\n\nID\n\n\nName\n\n\nLabel\n\n\nmissings\n\n\nValues\n\n\nValue Labels\n\n\nFreq.\n\n\n%\n\n\n1\n\n\nyear\n\n\n\n\n0 (0.00%)\n\n\nrange: 2003-2009\n\n\n\n\n\n\n2\n\n\nage\n\n\n\n\n0 (0.00%)\n\n\nrange: 18-80\n\n\n\n\n\n\n3\n\n\nmaritl\n\n\n\n\n0 (0.00%)\n\n\n\n\nNever Married2. Married3. Widowed4. Divorced5. Separated\n\n\n64820741920455\n\n\n21.6069.130.636.801.83\n\n\n4\n\n\nrace\n\n\n\n\n0 (0.00%)\n\n\n\n\nWhite2. Black3. Asian4. Other\n\n\n248029319037\n\n\n82.679.776.331.23\n\n\n5\n\n\neducation\n\n\n\n\n0 (0.00%)\n\n\n\n\n< HS Grad2. HS Grad3. Some College4. College Grad5. Advanced Degree\n\n\n268971650685426\n\n\n8.9332.3721.6722.8314.20\n\n\n6\n\n\nregion\n\n\n\n\n0 (0.00%)\n\n\n\n\nNew England2. Middle Atlantic3. East North Central4. West North Central5. South Atlantic6. East South Central7. West South Central8. Mountain9. Pacific\n\n\n030000000000\n\n\n0.00100.000.000.000.000.000.000.000.00\n\n\n7\n\n\njobclass\n\n\n\n\n0 (0.00%)\n\n\n\n\nIndustrial2. Information\n\n\n15441456\n\n\n51.4748.53\n\n\n8\n\n\nhealth\n\n\n\n\n0 (0.00%)\n\n\n\n\n<=Good2. >=Very Good\n\n\n8582142\n\n\n28.6071.40\n\n\n9\n\n\nhealth_ins\n\n\n\n\n0 (0.00%)\n\n\n\n\nYes2. No\n\n\n2083917\n\n\n69.4330.57\n\n\n10\n\n\nlogwage\n\n\n\n\n0 (0.00%)\n\n\nrange: 3.0-5.8\n\n\n\n\n\n\n11\n\n\nwage\n\n\n\n\n0 (0.00%)\n\n\nrange: 20.1-318.3\n\n\n\n\n\n\nPlot frequencies with plot_frq, plot_grpfrq and plot_grid\nHowever, we often want to see an actual picture, for example, display frequencies and percentages of categorical variables on a bar plot. For that {sjPlot} package provides a convenient plot-frequencies (plot_frq) function, which does just that. For instance, plotting education shows that around 9% of people in our data did not finish a high school, while around 14% have a PhD.\n\n\nWage %>% \n  plot_frq(education)\n\n\n\nSince {sjPlot} package works with tidyverse ü•≥, we can easily group the data by any other categorical variable, let‚Äôs take race, and get frequencies and percentages for every group. plot_grid() function puts several subplots in a single plot and even names the subplots. For instance, a subplot C shows that most of Afro-Americans in our dataset ARE highly educated. And of coarse you can save this publication-ready plot with ‚Ä¶ surprise surpriiise ‚Ä¶ save_plot command.\n\n\np <- Wage %>% \n  group_by(race) %>% \n  plot_frq(education) %>%\n    plot_grid()\n\n\n\n\n\nsave_plot(filename = \"race_vs_education.jpg\", fig = p, width = 30, height = 19)\n\nquartz_off_screen \n                2 \n\nWhile seeing counts and percentages of separate groups is cool, we sometimes want to put groups directly near each other. And that‚Äôs exactly what plot-grouped-frequencies (plot_grpfrq) function does. For instance, it clearly shows that most of the people with lower education degrees work in factories, while folks with higher education degrees work with information.\n\n\nplot_grpfrq(\n  var.cnt = Wage$education, \n  var.grp = Wage$jobclass)\n\n\n\nPlot or display cross (pivot) tables\nThis IS already useful, however, plot_xtab function goes one step further and displays percentages of jobclasses inside of every educational degree as stacked-bars, where counts are identical to the previous plot, but every educational category as one 100 percent. Such display only reinforces our hypothesis that highly educated folks usually work in the IT and shows a clear association between jobclass and education. As if that were not enough, plot_xtab even tests this hypothesis with the Chi-Squared test of association and displays a significant p-value and a large effect size.\n\n\n# as stacked proportional bars\nplot_xtab(\n  x   = Wage$education, \n  grp = Wage$jobclass, \n  margin  = \"row\", \n  bar.pos = \"stack\",\n  show.summary = TRUE,\n  coord.flip   = TRUE)\n\n\n\nSo, plot_xtab essentially visualizes cross tables, also known as pivot tables. And if for some reason you want an actual table with the results of a statistical test, you can use tab_xtab function instead.\n\n\ntab_xtab(\n  var.row = Wage$education, \n  var.col = Wage$jobclass, \n  show.row.prc = T)\n\n\neducation\n\n\njobclass\n\n\nTotal\n\n\nIndustrial\n\n\nInformation\n\n\n< HS Grad\n\n\n19070.9¬†%\n\n\n7829.1¬†%\n\n\n268100¬†%\n\n\nHS Grad\n\n\n63665.5¬†%\n\n\n33534.5¬†%\n\n\n971100¬†%\n\n\nSome College\n\n\n34252.6¬†%\n\n\n30847.4¬†%\n\n\n650100¬†%\n\n\nCollege Grad\n\n\n27440¬†%\n\n\n41160¬†%\n\n\n685100¬†%\n\n\nAdvanced Degree\n\n\n10223.9¬†%\n\n\n32476.1¬†%\n\n\n426100¬†%\n\n\nTotal\n\n\n154451.5¬†%\n\n\n145648.5¬†%\n\n\n3000100¬†%\n\n\nœá2=282.643 ¬∑ df=4 ¬∑ Cramer‚Äôs V=0.307 ¬∑ p=0.000\n\n\n\n(not part of the video) By the way, we can decide what kind of percentages are calculated, rows or columns or even single cells, whether we want to stack the bars and many more. It will automatically conduct Fisher‚Äôs test of association if samples are small (<5).\n(not in the video) Plot grouped proportional tables (I am not sure it‚Äôs very intuitive)\nThe p-values are based on chisq.test of x and y for each grp.\n\n\nWage %>% \n  plot_gpt(x = health_ins, y = jobclass, grp = education) \n\n\n\nPlot histograms of salaries and display averages + SD\nBut enough about counting, since our dataset is about salaries, let‚Äôs figure our who earns more, industrial or information workers? Plot frequencies function, which we used for counting, can also easily answer this question if we give it a (1) wage variable, (2) tell it to plot a histogram, (3) to show an average with standard deviation and (4) to display a normal curve to see whether our salaries are normally distributed. This visualization reveals that industrial workers get 103 thousand dollars on average, while IT crowd gets 17 thousand more.\n\n\nWage %>% \n  group_by(jobclass) %>% \n  plot_frq(wage, type = \"histogram\", show.mean = TRUE, normal.curve = TRUE) %>% \n  plot_grid()\n\n\n\nPlot likert scales as centered stacked bars\nThe last thing I‚Äôd like to share with you before we visualize models, is a visualization of likert scale data. If you have same scales or categories across different variables, plot_likert function nicely compares percentages of scales or categories across those variables.\n\n\ndata(pisaitems)\n\nd <- pisaitems %>% \n  dplyr::select(starts_with(\"ST25Q\"))\n\nview_df(d, show.frq = T, show.prc = T)\n\n\nData frame: d\n\n\nID\n\n\nName\n\n\nLabel\n\n\nValues\n\n\nValue Labels\n\n\nFreq.\n\n\n%\n\n\n1\n\n\nST25Q01\n\n\n\n\n\n\nNever or almost neverA few times a yearAbout once a monthSeveral times a monthSeveral times a week\n\n\n668213143139952035311436\n\n\n10.1820.0321.3331.0217.43\n\n\n2\n\n\nST25Q02\n\n\n\n\n\n\nNever or almost neverA few times a yearAbout once a monthSeveral times a monthSeveral times a week\n\n\n24019167891031794894751\n\n\n36.7525.6815.7814.527.27\n\n\n3\n\n\nST25Q03\n\n\n\n\n\n\nNever or almost neverA few times a yearAbout once a monthSeveral times a monthSeveral times a week\n\n\n1113116164128181456910658\n\n\n17.0424.7419.6222.3016.31\n\n\n4\n\n\nST25Q04\n\n\n\n\n\n\nNever or almost neverA few times a yearAbout once a monthSeveral times a monthSeveral times a week\n\n\n20145199611276887973622\n\n\n30.8530.5719.5513.475.55\n\n\n5\n\n\nST25Q05\n\n\n\n\n\n\nNever or almost neverA few times a yearAbout once a monthSeveral times a monthSeveral times a week\n\n\n1231712141103141564515165\n\n\n18.7818.5115.7323.8623.12\n\n\nplot_likert(d) \n\n\n\nPlot model results\nVisualizing data is quite, but let‚Äôs get to the really cool visualization stuff!\nPlot predictions\nplot_model function is the actual reason I love {sjPlot} package. I literally use it everyday!\nFor example if I want to know how education influences salary, I‚Äôll plot predictions from a simple linear model. Plotting prediction immediately tells me the story. Namely, people who did not even finish a high school, have the lowest salary compared to any other education level. Moreover, we can see that increasing education level means increasing salaries. So, education matters!\n\n\nm <- lm(wage ~ education, data = Wage)\nplot_model(m, type = \"pred\")\n\n$education\n\n\nPlot coefficients\nThe only thing we can‚Äôt see from this plot is whether this increase is significant or not. We could use a well known summary table for that, but the output, although useful, is not really pleasing to the human eye and is not suitable for publication.\n\n\nsummary(m)\n\n\nCall:\nlm(formula = wage ~ education, data = Wage)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-112.31  -19.94   -3.09   15.33  222.56 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                   84.104      2.231  37.695  < 2e-16 ***\neducation2. HS Grad           11.679      2.520   4.634 3.74e-06 ***\neducation3. Some College      23.651      2.652   8.920  < 2e-16 ***\neducation4. College Grad      40.323      2.632  15.322  < 2e-16 ***\neducation5. Advanced Degree   66.813      2.848  23.462  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 36.53 on 2995 degrees of freedom\nMultiple R-squared:  0.2348,    Adjusted R-squared:  0.2338 \nF-statistic: 229.8 on 4 and 2995 DF,  p-value: < 2.2e-16\n\nLuckily for us, plot_model() with the argument show.values = TRUE transforms a boring summary table into this informative picture, which shows the increase in salary in thousands of dollars as compared to no education (Intercept, not shown) with 95% confidence intervals and significance stars which indicate that those increases in salary are significant.\n(not in the video) Where vertical 0 indicates no effect (x-axis position 1 for most glm‚Äôs and position 0 for most linear models)\n\n\nplot_model(m, show.values = TRUE, width = 0.1)+\n  ylab(\"Increase in salary as compared to no education\")\n\n\nggsave(\"plot_model1.jpg\", device = jpeg, plot = last_plot(), width = 5, height = 4)\n\n\nTable with coeffitients, 95% CIs, p-values & more\nHowever, sometimes we still need to report the summary table, but we need to make it look better. And that‚Äôs where tab_model command comes into play. Within tab_model we can show the reference level, hide the intercept and change the style of p-values.\n\n\ntab_model(m, \n          show.reflvl = T, \n          show.intercept = F, \n          p.style = \"numeric_stars\")\n\n\n¬†\n\n\nwage\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n< HS Grad\n\n\nReference\n\n\n\n\n\n\nHS Grad\n\n\n11.68 ***\n\n6.74¬†‚Äì¬†16.62\n\n\n<0.001\n\n\nSome College\n\n\n23.65 ***\n\n18.45¬†‚Äì¬†28.85\n\n\n<0.001\n\n\nCollege Grad\n\n\n40.32 ***\n\n35.16¬†‚Äì¬†45.48\n\n\n<0.001\n\n\nAdvanced Degree\n\n\n66.81 ***\n\n61.23¬†‚Äì¬†72.40\n\n\n<0.001\n\n\nObservations\n\n\n3000\n\n\nR2 / R2 adjusted\n\n\n0.235 / 0.234\n\n\np<0.05¬†¬†¬†** p<0.01¬†¬†¬†*** p<0.001\n\n\nPlot fancy models üòâüí™ü§ì\nBut the most amazing thing about plot_model and tab_model functions is that they work with almost any type of model you can imagine! I successfully used it for mixed-effects models, Bayesian models or negative-binomial models, to name a few. And the authors of the package constantly improve the functionality, so that, at the moment you read this blog-post, {sjPlot} package is most likely improved.\n\nHere is an example of how ease we can visualize a very fancy model, namely a generalized linear mixed-effects regression for negative-binomial distribution of age with 3 way interaction term and a random effect of education.\n\n\nm.nb <- glmer.nb(age ~ wage * jobclass * health + (1|education), data = Wage)\n\nplot_model(m.nb, type = \"int\")[[4]]\n\n\nggsave(\"plot_model2.jpg\", device = jpeg, plot = last_plot(), width = 5, height = 3)\n\n\nThis interactions show that industrial workers with a very good health earn 50 thousand dollars already at the age of 31, while IT crowd gets the same salary ca. 8 years later, however at the age of 45 the IT crowd catches on and even starts to slowly overtake the factory workers, and finally, while IT folks get to the salary of 300 thousand dollars already at the age of 50, factory workers might reach this kind of wealth only at the end of their carrier, at around 63 years old. And the non-overlapping confidence intervals indicate that such difference in salaries is significant.\nBesides, you can easily change the appearance of your results by changing the default order of predictors and even choose particular values from the numeric predictor. For instance, let‚Äôs take three salary values 50, 150 & 300 as we just talked about them and display our results in a different way.\n\n\nplot_model(m.nb, type = \"pred\", terms = c(\"health\", \"jobclass\", \"wage [50, 150, 300]\"))\n\n\n\nMoreover, type argument allows to create various plot types. For example, we can easily visualize random effects if we want to.\n\n\nplot_model(m.nb, \n           type  = \"re\", \n           width = .5, \n           show.values = T) + ylim(0.9,1.1)\n\n\n\nPlot multiple models\nIt only gets better from now. If we want to explore several dependent variables with the same predictors, we can use plot_models function to plot several models at once. In the first code example we‚Äôll use already familiar argument - show.values - and a new one - grid - which plots models in separate fields to avoid congestion and overload of information on the picture.\n\n\n# fit two models\nfit1 <- lm(age ~ education + jobclass + health_ins, data = Wage)\nfit2 <- lm(wage ~ education + jobclass + health_ins, data = Wage)\n\n# plot multiple models\nplot_models(fit1, fit2, show.values = T, grid = TRUE)\n\n\n\nIn the second example we avoid clutter by simply not using show.values, since we can kind of read them from the x-axes, and we‚Äôll use p.shape = TRUE argument instead, in order to display p-values as shapes instead of significance stars.\n\n\nplot_models(fit1, fit2, p.shape = TRUE)\n\n\n\nMore than one model\ntab_model can also easily display multiple models. Here, collapse.ci = TRUE argument conveniently puts confidence intervals below the estimates, so that we can report several models near each other.\n\n\ntab_model(fit1, fit2, \n          collapse.ci = TRUE, \n          p.style     = \"numeric_stars\")\n\n\n¬†\n\n\nage\n\n\nwage\n\n\nPredictors\n\n\nEstimates\n\n\np\n\n\nEstimates\n\n\np\n\n\n(Intercept)\n\n\n43.15 ***(41.68¬†‚Äì¬†44.63)\n\n\n<0.001\n\n\n93.72 ***(89.14¬†‚Äì¬†98.31)\n\n\n<0.001\n\n\neducation [2. HS Grad]\n\n\n-0.21 (-1.75¬†‚Äì¬†1.34)\n\n\n0.794\n\n\n8.15 ***(3.34¬†‚Äì¬†12.95)\n\n\n0.001\n\n\neducation [3. SomeCollege]\n\n\n-2.01 *(-3.65¬†‚Äì¬†-0.37)\n\n\n0.016\n\n\n17.89 ***(12.79¬†‚Äì¬†22.99)\n\n\n<0.001\n\n\neducation [4. CollegeGrad]\n\n\n-0.48 (-2.13¬†‚Äì¬†1.17)\n\n\n0.568\n\n\n33.03 ***(27.91¬†‚Äì¬†38.15)\n\n\n<0.001\n\n\neducation [5. AdvancedDegree]\n\n\n1.35 (-0.45¬†‚Äì¬†3.16)\n\n\n0.142\n\n\n57.90 ***(52.28¬†‚Äì¬†63.52)\n\n\n<0.001\n\n\njobclass [2. Information]\n\n\n1.42 **(0.56¬†‚Äì¬†2.28)\n\n\n0.001\n\n\n3.67 **(1.00¬†‚Äì¬†6.35)\n\n\n0.007\n\n\nhealth ins [2. No]\n\n\n-3.29 ***(-4.20¬†‚Äì¬†-2.39)\n\n\n<0.001\n\n\n-19.89 ***(-22.72¬†‚Äì¬†-17.07)\n\n\n<0.001\n\n\nObservations\n\n\n3000\n\n\n3000\n\n\nR2 / R2 adjusted\n\n\n0.033 / 0.031\n\n\n0.284 / 0.283\n\n\np<0.05¬†¬†¬†** p<0.01¬†¬†¬†*** p<0.001\n\n\nWhat‚Äôs next\nBy the way, if you want to visualize and test ALL the assumptions of ANY model with a SINGLE function, check out this video about another amazing package created by the same author - Daniel L√ºdecke.\nFurther readings and references\nhttps://strengejacke.github.io/sjPlot/\nhttps://cran.r-project.org/web/packages/sjPlot/index.html\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-08-01-sjplot/thumbnail_sjPlot.png",
    "last_modified": "2023-08-15T18:43:54+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2023-05-09-datawrangling4/",
    "title": "{dplyr} on Steroids: Handling Data Bases",
    "description": "If you know how to tidy up data within one table, you're already a skilled data scientist! However, as data continues to grow exponentially, taking your skills to the next level involves mastering the art of working with multiple tables within a database, typically done using SQL. In this post, we'll learn three essential techniques using {dplyr} that will allow you to handle databases with ease: merging multiple tables, reducing redundancy through table joins, and effortlessly modifying values within the resulting table.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-07-14",
    "categories": [
      "videos",
      "statistics",
      "data wrangling",
      "R package reviews"
    ],
    "contents": "\n\nContents\nPrevious topics\nChapter 1: Combine tables\nChapter 1 as 4 minutes video\nBind columns and bind rows\nUnion, setdiff and intersect\n\nChapter 2: Join tables\nChapter 2 as 10 minutes video\nMutating joins\n1. inner_join\n2. left_join\n3. right_join\n4. full_join\n\nFiltering joins combine rows\n1. semi_join\n2. anti_join\n\n\nChapter 3: Conditioning\nChapter 3 as 2 minutes video\nIf ‚Ä¶ else ‚Ä¶\nCase when\n\nConclusion and what‚Äôs next?\nFurther readings and references\n\nPrevious topics\nTo maximize the effect of this post you can have a look at {dplyr} for beginners, advanced {dplyr} and the review about {tidyr} package before.\nConsider following tables:\n\n\nlibrary(tidyverse) # it's the only package you need\nx <- tibble(A = c(\"1\", \"1\", \"2\"), B = c(\"a\", \"b\", \"a\"))\ny <- tibble(A = c(\"1\", \"2\"), B = c(\"a\", \"b\"))\nz <- tibble(A = c(\"3\", \"2\", \"1\"), C = c(\"a\", \"b\", \"c\"), D = c(\"here\", \"you\", \"go\"))\n\n\n\n\n\nChapter 1: Combine tables\nChapter 1 as 4 minutes video\n\n\n\n\n\n\n\nBind columns and bind rows\nTo combine several tables into one, we can combine their columns or rows. The bind_cols function is the most intuitive way to combine columns.\n\n\nbind_cols(x, z) \n\n# A tibble: 3 √ó 5\n  A...1 B     A...3 C     D    \n  <chr> <chr> <chr> <chr> <chr>\n1 1     a     3     a     here \n2 1     b     2     b     you  \n3 2     a     1     c     go   \n\nbind_cols even renames identical column names, making sure each column is unique. But here‚Äôs the catch: the tables can have a different number of columns, but they must have the same number of rows. Otherwise it refuses to work and throws a following error message:\n\n\nbind_cols(x, y)\n\n\nError in `bind_cols()`:\n! Can't recycle `..1` (size 3) to match `..2` (size 2).\nLooking to stack two tables vertically? Look no further than bind_rows! Just remember, these tables should have the same column names. Otherwise, bind_rows will generously populate your new dataset with NAs wherever the column names don‚Äôt align.\n\n\nbind_rows(x, z)\n\n# A tibble: 6 √ó 4\n  A     B     C     D    \n  <chr> <chr> <chr> <chr>\n1 1     a     <NA>  <NA> \n2 1     b     <NA>  <NA> \n3 2     a     <NA>  <NA> \n4 3     <NA>  a     here \n5 2     <NA>  b     you  \n6 1     <NA>  c     go   \n\nAnd that can be a problem. Because when dealing with multiple large tables, it‚Äôs easy to lose track of which data belongs to which table. But no worries! The ‚Äò.id‚Äô argument comes to the rescue. It allows you to easily track and identify which table certain values belong to.\n\n\nbind_rows(y, z,  .id = \"table\") \n\n# A tibble: 5 √ó 5\n  table A     B     C     D    \n  <chr> <chr> <chr> <chr> <chr>\n1 1     1     a     <NA>  <NA> \n2 1     2     b     <NA>  <NA> \n3 2     3     <NA>  a     here \n4 2     2     <NA>  b     you  \n5 2     1     <NA>  c     go   \n\nUnion, setdiff and intersect\nHowever, even though bind_rows always delivers, when dealing with sizable tables containing numerous columns, we may end up with an excess of irrelevant columns without even realizing it. That‚Äôs why I personally lean towards using the union_all command. It accomplishes the same task as bind_rows, but if my data isn‚Äôt clean, it throws an informative error message, which often saves me from chaos.\n\n\n\n\n\nunion_all(y, z)\n\n\nError in `union_all()`:\n! `x` and `y` are not compatible.\n‚úñ Different number of columns: 2 vs 3.\n\n\nbind_rows(x, y)\n\n# A tibble: 5 √ó 2\n  A     B    \n  <chr> <chr>\n1 1     a    \n2 1     b    \n3 2     a    \n4 1     a    \n5 2     b    \n\nunion_all(x, y)\n\n# A tibble: 5 √ó 2\n  A     B    \n  <chr> <chr>\n1 1     a    \n2 1     b    \n3 2     a    \n4 1     a    \n5 2     b    \n\nBut while combining tables via bind_rows or union_all is useful, it might produce duplicates üëØ‚Äç‚ôÄ, if identical rows appear in both tables. We usually don‚Äôt want that! In order to exclude duplicates and keep only one of identical rows while combining tables, we can use union command instead of union_all.\n\n\n\n\n\nunion(x, y)\n\n# A tibble: 4 √ó 2\n  A     B    \n  <chr> <chr>\n1 1     a    \n2 1     b    \n3 2     a    \n4 2     b    \n\nWhile the union command excludes duplicates, there are situations where we need to identify the distinct rows in table ‚Äúx‚Äù that do not exist in table ‚Äúy‚Äù, or vice versa. This is where the setdiff command comes into play. By specifying two tables, we can easily find all the unique rows that belong to the first specified table.\n\n\n\n\n\n# find rows that appear in x but not y\nsetdiff(x,y)\n\n# A tibble: 2 √ó 2\n  A     B    \n  <chr> <chr>\n1 1     b    \n2 2     a    \n\nTo obtain unique rows from the ‚Äúy‚Äù table, simply specify ‚Äúy‚Äù as the first table, followed by the ‚Äúx‚Äù table.\n\n\n\n\n\n# find rows that appear in y but not x\nsetdiff(y,x)     \n\n# A tibble: 1 √ó 2\n  A     B    \n  <chr> <chr>\n1 2     b    \n\nBut while setdiff completely excludes duplicate rows from a dataset, there are times when we actually want to focus on those duplicates to better understand the repetitive patterns in our data. In order to isolate only the duplicate rows, we can use the intersect command.\n\n\n\n\n\nintersect(x,y)   # find duplicates\n\n# A tibble: 1 √ó 2\n  A     B    \n  <chr> <chr>\n1 1     a    \n\nChapter 2: Join tables\nCombining and merging tables through joins is incredibly useful for data science, as it enables us to unlock valuable insights within complex datasets. While joining tables is usually performed using SQL, this approach comes with numerous limitations. In this chapter, we‚Äôll explore how the intuitive syntax of {dplyr} can revolutionize our ability to join tables, taking our skills to a whole new level of efficiency and flexibility.\nChapter 2 as 10 minutes video\n\n\n\n\n\n\n\nBeing able to handle duplicate rows, as we did in previous chapter, is fantastic! But you might wonder, what if we have duplicate columns? That can occur when we use the bind_cols command. For instance, if we mistakenly combine two identical tables, each with 2 columns, the result will have 4 columns. The bind_cols command conveniently renames them to avoid confusion, yet it retains all columns. However, as they do not provide any unique information, they are totally redundant!\n\n\nbind_cols(x, x)\n\n# A tibble: 3 √ó 4\n  A...1 B...2 A...3 B...4\n  <chr> <chr> <chr> <chr>\n1 1     a     1     a    \n2 1     b     1     b    \n3 2     a     2     a    \n\nThus, we would love R to recognize duplicate columns, to eliminate redundancy by retaining only one column, but also to merge columns that differ in order to make our final model more informative. For example, we might want to combine data about customer purchases with data about customer demographics in order to understand what factors influence customer behavior. And this is precisely the purpose of join commands - to allow you to combine data from multiple tables. There are two types of joins: mutating and filtering. Let‚Äôs begin with the four mutating joins.\nMutating joins\n1. inner_join\nThe inner_join is the most intuitive. It finds identical columns, keeps just one of them, adds all not matching columns behind it and ignores the rest.\n\n\n\nOne useful example of applying inner_join is when you have two datasets, let‚Äôs say a ‚ÄúCustomers‚Äù dataset and an ‚ÄúOrders‚Äù dataset. Both datasets have a common column, such as ‚ÄúCustomer ID,‚Äù which serves as a unique identifier for each customer. Such unique identifier is often called key column.\nBy performing an inner_join on the ‚ÄúCustomer ID‚Äù column, you can merge the two datasets based on matching customer IDs. This operation allows you to create a consolidated dataset that contains information from both datasets but only for customers who have placed orders. In other words, it filters out any customers who have not made any purchases.\n\n\n# Load the dplyr package\nlibrary(dplyr)\n\n# Create the Customer Information table\ncustomer_info <- data.frame(\n  CustomerID = c(\"001\", \"002\", \"003\"),\n  Name = c(\"John\", \"Sarah\", \"Michael\"),\n  Age = c(25, 30, 28),\n  Email = c(\"john@example.com\", \"sarah@example.com\", \"michael@example.com\")\n)\n\n# Create the Purchase History table\npurchase_history <- data.frame(\n  CustomerID = c(\"001\", \"002\", \"004\"),\n  PurchaseDate = as.Date(c(\"2022-07-15\", \"2022-06-10\", \"2022-08-20\")),\n  Product = c(\"Laptop\", \"Phone\", \"Headphones\")\n)\n\n# Perform the inner join\ninner_join(customer_info, purchase_history, by = \"CustomerID\")\n\n  CustomerID  Name Age             Email PurchaseDate Product\n1        001  John  25  john@example.com   2022-07-15  Laptop\n2        002 Sarah  30 sarah@example.com   2022-06-10   Phone\n\nThis merged dataset can be valuable for analyzing customer behavior, understanding purchasing patterns, or conducting targeted marketing campaigns to specific customer segments.\nBut we have to be very careful here!!! (Arni) The inner_join only keeps observations from one table that have perfectly matching values in the other table. If we take our x and y tables, we‚Äôll see that only the first row ‚Äú1 a‚Äù is kept, while ‚Äú1 b‚Äù and ‚Äú2 a‚Äù from table x have no match in table y, and the ‚Äú2 b‚Äù from table y has no match in table x.\n\n\n# install.packages(\"gridExtra\")\nlibrary(gridExtra)\ngrid.arrange(\n  tableGrob(x),\n  tableGrob(y),\n  tableGrob(inner_join(x, y)),\n  ncol = 3)\n\n\n\nAnd that‚Äôs the most important property of an inner_join - unmatched rows in either table are not included in the result. This means that generally inner joins are not appropriate in most analyses, because it is too easy to lose observations.\nIf you want to keep all the observations from ‚Äúx‚Äù, or from ‚Äúy‚Äù, or from both tables, we need to apply outer_joins. And there are three of them:\n\nleft_join keeps all observations from the left table\nright_join keeps all observations from the right table\nfull_join keeps all observations from both tables\n\n2. left_join\n\n\n\nWhile the inner_join may be the most intuitive, it is the left_join that proves to be most commonly employed join in practice and, actually, the most useful for several reasons:\nfirst, if one table is more important then other, left joins allow us to preserve all the observations from the left table, even if there are no matches in the right table. This ensures that no important data is lost during merging;\nsecondly, the new data from the right table that corresponds to the matching observations in the left table is added\nand finally, left joins highlight any discrepancies or mismatches between the tables being joined. When a left join introduces empty values (such as NA) for unmatched observations, it serves as an indicator of data inconsistencies or missing values. Detecting these mismatches early on is crucial for ensuring data integrity and avoiding erroneous conclusions.\nLet‚Äôs consider the same example where you have two tables: one containing customer information and another containing purchase history. Here, we want to merge the two tables using a left join to retain all customer information while incorporating relevant purchase data. Particularly, customers 001 and 002 had matching records in both tables, so their purchase information was included in to final table. Customer 003, who had no corresponding record in the purchase history table, resulted in NULL values for the purchase-related columns.\n\n\n# Perform the left join\nleft_join(customer_info, purchase_history, by = \"CustomerID\")\n\n  CustomerID    Name Age               Email PurchaseDate Product\n1        001    John  25    john@example.com   2022-07-15  Laptop\n2        002   Sarah  30   sarah@example.com   2022-06-10   Phone\n3        003 Michael  28 michael@example.com         <NA>    <NA>\n\nThe ‚Äúby =‚Äù argument in the ‚Äúleft_join‚Äù command is crucial!, because it specifies columns that serve as the matching criterion between the two tables being joined. It determines how the rows from each table will be matched and merged.\nIf the ‚Äúby =‚Äù argument is not provided, the left_join command finds identical columns in both tables and uses them all by default. However, that could result in incorrect matches or cause errors during the merging process if columns having the same information named differently. Therefore, it is crucial to specify the correct column(s) to ensure accurate and meaningful results in the merged table.\n\n\n# Create the first table\nt1 <- data.frame(\n  ID = c(1, 2, 3),\n  Name = c(\"John\", \"Sarah\", \"Michael\")\n)\n\n# Create the second table\nt2 <- data.frame(\n  Identifier = c(1, 2, 3),\n  Value = c(10, 20, 30)\n)\n\n\nFor example, let‚Äôs left_join t1 and t2 based on the common ID/Identifier column.\nIn the first case, when the ‚Äúby‚Äù argument is not specified, the left_join command attempts to automatically match the columns. However, since the columns have different names (ID vs.¬†Identifier), the join does not work, because our tables have no common variables.\n\n\n# Perform the left join without specifying the \"by\" argument\nleft_join(t1, t2)\n\n\nError in `left_join()`:\n! `by` must be supplied when `x` and `y` have no common\n  variables.\nBut if the correct ‚Äúby‚Äù argument is used, specifying that the ID column from t1 corresponds to the Identifier column from t2, left_join works properly.\n\n\nleft_join(t1, t2, by = c(\"ID\" = \"Identifier\"))\n\n  ID    Name Value\n1  1    John    10\n2  2   Sarah    20\n3  3 Michael    30\n\n3. right_join\n\n\n\nRight_join works similarly to left_join by retaining all observations from the right table, ‚Äúpurchase_history‚Äù in our case. In fact, in many cases, a right_join can be replaced by a left_join by reversing the table order. In this way you can achieve the same outcome and retain the same information. So, to be honest, we can ignore right_join, but we can‚Äôt ignore the full_join, and here is why.\n\n\nright_join(customer_info, purchase_history)\n\n  CustomerID  Name Age             Email PurchaseDate    Product\n1        001  John  25  john@example.com   2022-07-15     Laptop\n2        002 Sarah  30 sarah@example.com   2022-06-10      Phone\n3        004  <NA>  NA              <NA>   2022-08-20 Headphones\n\nleft_join(purchase_history, customer_info)\n\n  CustomerID PurchaseDate    Product  Name Age             Email\n1        001   2022-07-15     Laptop  John  25  john@example.com\n2        002   2022-06-10      Phone Sarah  30 sarah@example.com\n3        004   2022-08-20 Headphones  <NA>  NA              <NA>\n\n4. full_join\n\n\n\nFull_joins are extremely useful due to their ability to retain all records from both tables, even if there are mismatches or missing values. In this way full_joins ensure that no data is lost during the merging process, which is crucial for thorough data analysis and decision-making. Besides, full_joins help to identify and investigate inconsistencies and gaps between the datasets, leading to better data quality and more complete data for analysis.\n\n\nfull_join(customer_info, purchase_history)\n\n  CustomerID    Name Age               Email PurchaseDate    Product\n1        001    John  25    john@example.com   2022-07-15     Laptop\n2        002   Sarah  30   sarah@example.com   2022-06-10      Phone\n3        003 Michael  28 michael@example.com         <NA>       <NA>\n4        004    <NA>  NA                <NA>   2022-08-20 Headphones\n\nCompared to inner_join that retains only two matching rows, and compared to left_join and right_join that retain only three rows, the full_join combines all four records from both tables, incorporating NA values in the absence of matches. When not all columns act as ‚Äúkeys‚Äù, a full join seamlessly combines the functionalities of both left and right joins, offering a comprehensive merging approach.\nHowever, not everything is rosy here! While returning every mismatch can help identify inconsistencies between tables and potentially fill gaps using advanced machine learning techniques, it can also pose a significant challenge. The sheer volume of ‚Äúnew‚Äù observations generated can quickly become overwhelming. In large and complex datasets, which are often the norm, and when both tables contain NAs from the beginning, distinguishing between original NAs and the ‚Äúnew‚Äù NAs becomes nearly impossible. This creates a dangerous situation where you might unknowingly proceed with analysis and produce unrealistic results.\nTherefore, despite the benefits of joins, be very careful and always double check the output.\nFiltering joins combine rows\nFiltering joins affect only the rows (or observations), not the columns (or variables). In contrast to mutating joins, filtering joins never duplicate rows. There are two filtering joins, semi_joins and anti_joins.\n1. semi_join\n\n\n\nSemi_joins help identify and keep matching records between two tables while retaining only the columns from the first (left) table.\n\nThe inner_join might look similar, but it would keep columns from both tables. By keeping only the desired columns from the left table, semi_joins help to reduce data complexity and focus on the essential information needed for analysis. In other words they filter and extract relevant records, resulting in a more concise and targeted dataset.\n\n\nsemi_join(customer_info, purchase_history)\n\n  CustomerID  Name Age             Email\n1        001  John  25  john@example.com\n2        002 Sarah  30 sarah@example.com\n\ninner_join(customer_info, purchase_history)\n\n  CustomerID  Name Age             Email PurchaseDate Product\n1        001  John  25  john@example.com   2022-07-15  Laptop\n2        002 Sarah  30 sarah@example.com   2022-06-10   Phone\n\n2. anti_join\n\n\n\nAnti_joins are used to exclude records that match between two tables, essentially filtering out the common observations.\n\nAn anti_join retains only unmatched rows, making it useful for identifying discrepancies between tables, and if the anti-join returns no results, it‚Äôs actually a good thing ;).\n\n\nanti_join(customer_info, purchase_history)\n\n  CustomerID    Name Age               Email\n1        003 Michael  28 michael@example.com\n\nNow that you have your final table in the desired structure, you probably want to explore and visualize your data, aren‚Äôt you? If you‚Äôre looking for a comprehensive video that will teach you how to efficiently explore all the aspects of the data you see on your screen below right now, then you definitely don‚Äôt want to miss out on this one.\n\nChapter 3: Conditioning\nChapter 3 as 2 minutes video\n\n\n\n\n\n\n\nHaving columns and rows in the desired structure is fantastic, and if your table is prepared to embark on the ‚Äúmachine learning wonderland,‚Äù it‚Äôs even better. However, sometimes we need to change some values within our tables. Here, I would like to introduce two highly valuable techniques that I use everyday.\nIf ‚Ä¶ else ‚Ä¶\nThe first technique is the ifelse command, which enables the creation of a new column based on an existing one (as shown in the first line of code below), or the modification of values within an existing variable (as demonstrated in the second line). When working with NAs in your data, it‚Äôs crucial to specify the ‚Äúna.rm = TRUE‚Äù argument to prevent all new observations from becoming NAs (as illustrated in the third line of code).\n\n\nfull_join(customer_info, purchase_history) %>%\n  select(CustomerID, Age) %>% \n  mutate(Age_category = ifelse(Age < mean(Age, na.rm = T), \"low\", \"high\")) %>% \n  mutate(Age = ifelse(Age < median(Age, na.rm = T), \"low\", \"high\")) %>% \n  mutate(Age_NAs = ifelse(Age < mean(Age), \"low\", \"high\"))\n\n  CustomerID  Age Age_category Age_NAs\n1        001  low          low      NA\n2        002 high         high      NA\n3        003 high         high      NA\n4        004 <NA>         <NA>      NA\n\nThe only drawback of using ifelse is that it becomes cumbersome to write multiple ifelse statements when dealing with numerous cases, which is where the case_when function comes in handy.\nCase when\n\nLet‚Äôs consider the ‚Äúwho‚Äù dataset ü¶â, which is freely available from the World Health Organization Global Tuberculosis (cough üò∑) Report. Initially, it may appear messy, but if you‚Äôre familiar with my previous tutorials on {dplyr} and {tidyr}, you can easily tidy it up and then transform the age categories from the peculiar ‚Äú014‚Äù format to more comprehensible ‚Äú0 ‚Äì 14 years old‚Äù. By using the TRUE ~ \"bla\" clause at the end of the case_when command, any categories that do not match the provided ones will be changed to ‚Äúbla‚Äù. If there are occurrences of ‚Äúbla‚Äù, it indicates that further cleaning is required. However, if no ‚Äúbla‚Äù cases are present, as in our example, it signifies that we have successfully accomplished our task. Let‚Äôs congratulate ourselves on a job well done! Cheers!\n\n\nd <- who %>%\n  \n  # I have reviewed {dplyr} and {tidyr} for this piece of code.\n  pivot_longer(cols = new_sp_m014:newrel_f65, values_drop_na = T) %>%\n  select(-iso2, -iso3) %>%\n  mutate(name = stringr::str_replace(name, \"newrel\", \"new_rel\")) %>%\n  separate(name, into = c(\"new\", \"type\", \"sexage\")) %>%\n  separate(\"sexage\", into = c(\"sex\", \"age\"), sep = 1) %>%\n  \n  # Now use case_when to change values\n  mutate(age_2 = case_when(\n    age == \"014\"  ~ \"0 ‚Äì 14 years old\",\n    age == \"1524\" ~ \"15 - 24 years old\",\n    age == \"2534\" ~ \"25 - 34 years old\",\n    age == \"3544\" ~ \"35 - 44 years old\",\n    age == \"4554\" ~ \"45 - 54 years old\",\n    age == \"5564\" ~ \"55 - 64 years old\",\n    age == \"65\"   ~ \"65 or older\",\n    TRUE          ~ \"bla\"\n  ))\n\nd\n\n# A tibble: 76,046 √ó 8\n   country      year new   type  sex   age   value age_2            \n   <chr>       <dbl> <chr> <chr> <chr> <chr> <dbl> <chr>            \n 1 Afghanistan  1997 new   sp    m     014       0 0 ‚Äì 14 years old \n 2 Afghanistan  1997 new   sp    m     1524     10 15 - 24 years old\n 3 Afghanistan  1997 new   sp    m     2534      6 25 - 34 years old\n 4 Afghanistan  1997 new   sp    m     3544      3 35 - 44 years old\n 5 Afghanistan  1997 new   sp    m     4554      5 45 - 54 years old\n 6 Afghanistan  1997 new   sp    m     5564      2 55 - 64 years old\n 7 Afghanistan  1997 new   sp    m     65        0 65 or older      \n 8 Afghanistan  1997 new   sp    f     014       5 0 ‚Äì 14 years old \n 9 Afghanistan  1997 new   sp    f     1524     38 15 - 24 years old\n10 Afghanistan  1997 new   sp    f     2534     36 25 - 34 years old\n# ‚Ñπ 76,036 more rows\n\n# janitor is an amazing package I also have reviewed ;)\njanitor::tabyl(d$age_2)\n\n           d$age_2     n   percent\n  0 ‚Äì 14 years old 10882 0.1430976\n 15 - 24 years old 10868 0.1429135\n 25 - 34 years old 10850 0.1426768\n 35 - 44 years old 10875 0.1430055\n 45 - 54 years old 10876 0.1430187\n 55 - 64 years old 10851 0.1426900\n       65 or older 10844 0.1425979\n\nConclusion and what‚Äôs next?\nNow, with your final, stunning table in front of you, the natural inclination is to visualize and explore your data, right? To achieve this in the most effective way possible, check out the {DataExplorer} package next.\nThank you for reading!\nFurther readings and references\nMost of this article originate from ‚ÄúR for Data Science‚Äù book by Garrett Grolemund and Hadley Wickham.\n\n\n\n",
    "preview": "posts/2023-05-09-datawrangling4/dplyr_4_thumbnail.png",
    "last_modified": "2023-07-14T11:26:41+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-12-01-quantileregression/",
    "title": "Quantile Regression as an useful Alternative for Ordinary Linear Regression",
    "description": "Ordinary linear regression often fails to correctly describe skewed or heteroscedastic data, totally srews up if data has outliers, and describes only the mean of the response variable. Quantile Regression promises to solve all these problems and delivers more results.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-07-07",
    "categories": [
      "videos",
      "statistics",
      "models"
    ],
    "contents": "\n\nContents\nThis post as a video\nWhy do we need Quantile Regression (QR)?\n1. Solve outliers problem: Median Regression (only 5th quaNtile, or 2nd quaRtile)\n2. Solve heteroscedasticity\n3. Solve not-normal (skewed) distribution & not-homogen variances across groups + categorical predictor\n4. Model more then just mean or just median - model several quantiles\n5. Model the entire conditional distribution of salaries via all possible quantiles\n6. Multivariable regression\n1) American salaries\nNonparametric non-linear quantile regression\n\n2) Efficiency of cars\n\n7. Some further useful things\nConfidence intervals\nEquality of slopes\nSpeed up the model\nContrasts in median regression\nMedian regression with interactions\nBayesian median regression\n{lqmm} package: Fitting Linear Quantile Mixed Models\nRandom intercept model\nRandom slope model\n\nFinal thoughs\n\nReferences and further readings\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs only 14 minutes long.\n\n\n\n\n\n\n\nWhy do we need Quantile Regression (QR)?\nParticularly, QR:\nis robust to outliers and influential points\ndoes not assume a constant variance (known as homoskedasticity) for the response variable or the residuals\ndoes not assume normality\nbut the main advantage of QR over linear regression (LR) is that QR explores different values of the response variable, instead of only the average, and delivers therefore a more complete picture of the relationships between variables.\nSo, let‚Äôs:\ntake problematic data,\nbuild both, linear and quantile models, and see\nwhether QR can solve problems and be a truly Useful Alternative for Ordinary Linear Regression.\n1. Solve outliers problem: Median Regression (only 5th quaNtile, or 2nd quaRtile)\nWe‚Äôll first see how both models deal with outliers. For that we‚Äôll create a small data set with ONE obvious outlier and use geom_smooth() function to create a linear model and geom_quantile() function for a quick quantile regression, with only 5th quantile, which makes it a median-based regression.\n\n\n# create data\nlibrary(tidyverse)\nd <- tibble(\n  predictor = c(  1,   2,   3,  4,   5,   6,   7),\n  outcome   = c(1.5, 2.3, 2.8,  4.1, 5.3, 0, 6.8)\n)\n\n# plot ordinary and median regressions\nggplot(d, aes(predictor, outcome))+\n  geom_point()+\n  geom_smooth(method = lm, se = F,color = \"red\", )+\n  geom_quantile(quantiles = 0.5)\n\n\n\nThis plot shows, that linear model tries to please all points and misses most of them, which results in a bad fit. In contrast, the Median Regression ignores the outlier and visually fits the rest of the data much better. But how do we know that Median Regression is indeed better?\nWell, if we create an ordinary and quantile regressions, we can compare the amount of information they loose. The Akaike‚Äôs Information Criterion (AIC) measures such loss of information. Namely, the lower the AIC, the better the model. Thus, a lower AIC of QR indicates a smaller loss of information from the data, as compared to LR, making QR a better model. Moreover, since the slope of LR is not significant, while the slope of QR is, using a wrong model could cost you an important discovery. So, no Nobel Price for you!\n\n\n# model median (2nd quantile) regression\nlr <- lm(outcome ~ predictor, data = d)\nlibrary(quantreg)\nmr <- rq(outcome ~ predictor, data = d, tau = .5)\n\n# compare models\nAIC(lr, mr) # => the lower AIC the better\n\n   df      AIC\nlr  3 34.90235\nmr  2 27.09082\n\nlibrary(sjPlot) # I made a video on this üì¶\ntheme_set(theme_bw())\nplot_models(lr, mr, show.values = TRUE, \n            m.labels = c(\"Linear model\", \"Median model\"), \n            legend.title = \"Model type\")\n\n\n\nBy the way, we can use ols_plot_resid_lev() function from {olsrr} package and see that we indeed have an outlier.\n\n\nlibrary(olsrr)\nols_plot_resid_lev(lr)\n\n\n\n2. Solve heteroscedasticity\nNow let‚Äôs take a real world heteroscedastic data and see whether median regression handles it better. Engel dataset from {quantreg} package explores the relationship between household food expenditure and household income. Similarly to previous example, the median and mean fits are quite different, which can be explained by the strong effect of the two unusual points with high income and low food expenditure. Probably just greedy people.\n\n\n# get heteroscedastic data\ndata(engel)\nggplot(engel, aes(income, foodexp))+\n  geom_point()+\n  geom_smooth(method = lm, se = F, color = \"red\")+\n  geom_quantile(color = \"blue\", quantiles = 0.5)+\n  geom_quantile(color = \"gray\", alpha = 0.3, \n                  quantiles = seq(.05, .95, by = 0.05))\n\n\n\nIn order to better justify the use of QR, we can check heteroskedasticity via Breusch-Pagan test. Our test detects heteroscedasticity, so that we again need an alternative to linear regression. And, a lower AIC of median-based regression again shows a better fit, as compared to the mean-based regression.\n\n\n# compare models\nlr   <- lm(foodexp ~ income, data = engel)\n\nlibrary(performance) # I made a video on this üì¶\ncheck_heteroscedasticity(lr)\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p < .001).\n\nqm50 <- rq(foodexp ~ income, data = engel, tau = 0.5)\n\nAIC(lr, qm50)\n\n     df      AIC\nlr    3 2897.351\nqm50  2 2827.260\n\n3. Solve not-normal (skewed) distribution & not-homogen variances across groups + categorical predictor\nNow let‚Äôs see how both models handle not-normally distributed or skewed data, and, at the same time, see how they handle categorical predictors.\n\n\n\nFor that we‚Äôll use a Wage dataset from {ISLR} package and model the salary of 30 industrial and IT workers. And when we check the assumptions of linear model, we‚Äôll see, that our data has no outliers, but is not-normality distributed and variances between groups differ, so our data is again - heteroscedastic. And that‚Äôs a big problem, because if SEVERAL assumption of a model fail, we CAN NOT trust the results of such model.\n\n\n# get not-normal data\nlibrary(ISLR)\nset.seed(1) # for reproducibility\nsalary <- Wage %>% \n  group_by(jobclass) %>% \n  sample_n(30)\n\nlr <- lm(wage ~ jobclass, data = salary)\n\ncheck_outliers(lr)\n\nOK: No outliers detected.\n- Based on the following method and threshold: cook (0.5).\n- For variable: (Whole model)\n\ncheck_normality(lr) \n\nWarning: Non-normality of residuals detected (p < .001).\n\ncheck_homogeneity(lr)\n\nWarning: Variances differ between groups (Bartlett Test, p = 0.005).\n\n(By the way, if we don‚Äôt specify any quantiles in quanlile regression, the default 5th quantile or - median regression (tau = 0.5) will be modeled.)\nAnd what are those results? Well, linear model reveals, that average annual salary of IT workers is almost 37.000$ higher as compared to industrial workers, and such big difference in means is significant. While median regression shows, that IT crowd earns only 19.6 thousand dollars more and this difference in medians is not significant.\nThe lower AIC of the median regression again shows that QR performs better then LR. So that, while in the case with outliers LR missed an important discovery, here LR discovered nonsense.\n\n\n# tau = .5 - or median regression is a default\nmr <- rq(wage ~ jobclass, data = salary, tau = 0.5) \n\nplot_models(lr, mr, show.values = T, \n            m.labels = c(\"Linear model\", \"Median model\"), \n            legend.title = \"Model type\")\n\n\nAIC(lr, mr)\n\n   df      AIC\nlr  3 630.4943\nmr  2 614.7647\n\nSuch nonsense is often caused by small samples, and indeed, if we take all 3000 workers from Wage dataset, we‚Äôll see that both models show significantly higher salary of IT crowd as compared with factory workers. However, the median regression still shows a smaller difference and a smaller AIC tells us that QR is still a better model, which makes sense for not-normally distributed and heteroscedastic data. Now, let‚Äôs finally get to the main advantage of QR. (halliluja)\n\n\nlr <- lm(wage ~ jobclass, data = Wage)\nmr <- rq(wage ~ jobclass, data = Wage)\n\nplot_models(lr, mr, show.values = T, \n            m.labels = c(\"Linear model\", \"Median model\"), \n            legend.title = \"Model type\")\n\n\nAIC(lr, mr)\n\n   df      AIC\nlr  3 30774.50\nmr  2 30248.76\n\n4. Model more then just mean or just median - model several quantiles\n\n\n# model several quantiles\nlibrary(ggridges)\nggplot(Wage, aes(x = wage, y = jobclass, fill = factor(stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\", calc_ecdf = TRUE,\n    quantile_lines = TRUE, quantiles = c(.1, .5, .9)\n  ) +\n  scale_fill_viridis_d(name = \"Quantiles\")+\n  xlab(\"salary\")\n\n\n\nWhile median regression delivers better results, the median is still a single central location, similar to the mean. But since median regression is a special case of QR, which uses only a 5th quantile, and since QR can easily model other quantiles too, a QR allows you to easily model low and high salaries! In other words, QR can be extended to noncentral locations. Namely, if we take a low quantile, for example 0.1 instead of 0.5, we‚Äôll model the difference between low income factory and low income IT workers. Similarly, if we take a high quantile, for example 0.9 instead of 0.5, we‚Äôll be able to check the difference between top salaries of industrial vs.¬†top salaries of IT workers.\n\n\nlr   <- lm(wage ~ jobclass, data = Wage)\nqm10 <- rq(wage ~ jobclass, data = Wage, tau = 0.10)\nqm50 <- rq(wage ~ jobclass, data = Wage, tau = 0.50)\nqm90 <- rq(wage ~ jobclass, data = Wage, tau = 0.90)\n\nplot_models(lr, qm10, qm50, qm90,\n            show.values = TRUE,\n            m.labels = c(\"LR\", \"QR 10%\", \"QR 50%\", \"QR 90%\"), \n            legend.title = \"Model type\")+\n  ylab(\"Increase in wage after switch to IT\")\n\n\n\nThe results show, that for low salaries the difference between industrial and IT jobs is smaller, then for median or high salaries. The reason for that could be education, so that when your education level is low, switching jobs from factory to IT would only increase your salary by ca. 8.000 bucks, while when you have a college degree, changing to IT will increase your salary by over 25.000 bucks. However, the reason itself is not important. What is important here, is that, while ordinary linear regression describes only an average change in salaries when we switch from industrial to IT job, quantile regression uncovers what happen after you switch jobs having low, median or high salary. In other words, a new salary after switching jobs depends on the salary before switching, which makes sense. But what doesn‚Äôt make any sense is that, an ordinary linear regression over-promises increase in salary for low earners and under-promises increase in salary for high earners. Thus, QR reveals a more complete picture of reality, and allows you to make a more informed decision.\n\n\nlibrary(ggridges)\nggplot(Wage, aes(x = wage, y = jobclass, fill = factor(stat(quantile)))) +\n  stat_density_ridges(\n    geom = \"density_ridges_gradient\", calc_ecdf = TRUE,\n    quantile_lines = TRUE, quantiles = seq(.1, .9, by = 0.1)\n  ) +\n  scale_fill_viridis_d(name = \"Quantiles\")\n\n\n\n\n\nqm20 <- rq(wage ~ jobclass, data = Wage, tau = 0.20)\nqm30 <- rq(wage ~ jobclass, data = Wage, tau = 0.30)\nqm70 <- rq(wage ~ jobclass, data = Wage, tau = 0.70)\nqm80 <- rq(wage ~ jobclass, data = Wage, tau = 0.80)\n\nplot_models(lr, qm10, qm20, qm30, qm50, qm70, qm80, qm90, show.values = TRUE)+\n  theme(legend.position = \"none\")+\n  ylab(\"Increase in wage after switch to IT\")\n\n\n\nBut that is just a beginning! Because, similarly to low (tau = 0.1) or high (tau = 0.9) quantiles, we can model more quantile to get more useful inference. And we can even ‚Ä¶\n5. Model the entire conditional distribution of salaries via all possible quantiles\n‚Ä¶ by defining the sequence of quantiles, from let‚Äôs say 0.1 to 0.9, and defining the step, in order to control how many quantiles we model. For example using ‚Äúby = 0.1‚Äù will model 9 quantiles from 0.1 to 0.9.\nPlotting the summary of our model (a quantile process\nplot) uncovers how switching to IT affects the entire conditional distribution of salaries. The red lines show the mean effect with confidence intervals estimated by linear regression. While shaded gray area shows confidence intervals for the quantile regression estimates. The non-overlapping confidence intervals between quantile and linear regression can be seen as significant difference between models. So that, linear regression significantly over-promises the increase in salaries when you switch to IT for low and medium earners (if we ignore the very small overlap from 0.3 to 0.6 quantiles), significantly underestimates the increase in salary for top 10% earners, while correctly describes the increase in salary for only a small part of workers with already relatively high salaries.\n\n\nseq(0.1, 0.9, by = 0.1)\n\n[1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\n\nq <- rq(wage ~ jobclass, data = Wage, \n        tau = seq(0.1, 0.9, by = 0.1))\n\nsummary(q) %>% \n  plot(parm = \"jobclass2. Information\")\n\n\n\n6. Multivariable regression\nSo, I think a univariable QR is already much more useful then LR. But that‚Äôs not all, multivariable QR is even more useful, because it can uncover which variables are important for low or for high values of the response variable.\nLet‚Äôs have a look at two multivariable examples.\n1) American salaries\nIn the first example we‚Äôll continue to model salaries, but instead of only a ‚Äújobclass‚Äù predictor, we‚Äôll add ‚Äúage‚Äù and ‚Äúrace‚Äù predictors.\nLet‚Äôs interpret the influence of ‚Äúage‚Äù on salary first. The young low earners would significantly increase their salaries as they age, because y-axis, which shows the slope of this increase, is positive and does not include zero. However, this realistic increase over lifetime is significantly smaller then average, promised by the linear regression, because red and gray confidence intervals don‚Äôt overlap. The young high earners have much higher slope, meaning much stronger increase in salary over lifetime, which was significantly underestimated by the linear regression. Here again, high educational degree could cause young people to earn a lot of money already in the beginning of their lives, and opens better chances to increase the salary over lifetime.\nThe interpretation of the categorical predictor ‚Äúrace‚Äù is even more interesting. Since ‚ÄúWhite‚Äù people are the intercept, ‚ÄúBlack‚Äù- and ‚ÄúAsian-Americans‚Äù can be compared to ‚ÄúWhite‚Äù Americans. Here, linear regression shows that on average for low income folks, Black people earn significantly less then White people, because the coefficient is negative and does not cross the zero, which is wrong. Because, in reality, since gray confidence intervals cross the zero, there is no significant difference between White and Black folks with low income. In contrast, when salaries are high, Black workers earn significantly less then White workers, even when they earn millions.\nThe wages of Asian Americans show the opposite. Namely, while linear regression mistakenly predicts that Asian folks get significantly more then White folks, independently of their salary, QR shows that low income Asian people earn significantly less or similar to White people.\n\n\n# multivariable regression\nq <- rq(wage ~ jobclass + age + race, data = Wage, \n        tau = seq(.05, .95, by = 0.05))\n\nsummary(q) %>% \n  plot(c(\"jobclass2. Information\", \"age\", \"race2. Black\", \"race3. Asian\"))\n\n\n\nSince, in all of the panels of the plot, the quantile regression estimates lie at some point outside the confidence intervals for the ordinary least squares regression, we can conclude that the effects of ‚Äújobclass‚Äù, ‚Äúage‚Äù and ‚Äúrace‚Äù are not constant across salaries, but depends on a height of the salary.\nAnd if that‚Äôs not enough, you can go one step further and conduct a ‚Ä¶\nNonparametric non-linear quantile regression\n‚Ä¶ for numeric predictors using {quantregGrowth} package. But before you do that, have a look at the last example where we check the influence of 5 predictors on the efficiency of cars.\n\n\n# non-linear quantile regression\nlibrary(quantregGrowth)\nset.seed(1)\no <-gcrq(wage ~ ps(age), \n         data = Wage %>% sample_n(100), tau=seq(.10,.90,l=3))\n\n# par(mfrow=c(1,2)) # for several plots\nplot(o, legend=TRUE, conf.level = .95, shade=TRUE, lty = 1, lwd = 3, col = -1, res=TRUE) \n\n\n\n2) Efficiency of cars\nHere, a linear regression will answer the question - which variables affect the average car mileage? A low quantile of 0.1 will tell us which predictors are important for not efficient cars, which drive only a few miles per gallon of gas. A high quantile of 0.9 will tell us which predictors are important for highly efficient cars, which drive a lot of miles per gallon of gas. We‚Äôll also conduct a median regression in order to compare it to LR and for a more complete presentation of the results.\nLet‚Äôs start with that. The negative coefficient of ‚Äúhorsepower‚Äù indicates significant decrease in efficiency of cars with increasing horsepower. Both, mean-based and median-based models agree on that. However, while linear regression reports ‚ÄúEngine displacement‚Äù to be not-important for efficiency, median regression shows that it is important. Moreover, quantile regression reports that increasing acceleration significantly reduces mileage of not-efficient cars and has no effect on highly efficient cars, while linear regression can‚Äôs say anything about low or highly efficient cars.\n\n\ncars <- Auto %>% \n  select(mpg, cylinders, displacement, horsepower, acceleration, origin)\n\nl   <- lm(mpg ~ ., data = cars)\nq10 <- rq(mpg ~ ., data = cars, tau = .1)\nq50 <- rq(mpg ~ ., data = cars, tau = .5)\nq90 <- rq(mpg ~ ., data = cars, tau = .9)\n\nlibrary(gtsummary) # I made a video on this üì¶\ntbl_merge(\n    tbls = list(\n      tbl_regression(l) %>% bold_p(),\n      tbl_regression(q10, se = \"nid\") %>% bold_p(), \n      tbl_regression(q50, se = \"nid\") %>% bold_p(),\n      tbl_regression(q90, se = \"nid\") %>% bold_p()\n),\n    tab_spanner = c(\"OLS\", \"QR 10%\", \"QR 50%\", \"QR 90%\")\n  )\n\n\nCharacteristic\n      \n        OLS\n      \n      \n        QR 10%\n      \n      \n        QR 50%\n      \n      \n        QR 90%\n      \n    Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n      Beta\n      95% CI1\n      p-value\n    cylinders\n-0.88\n-1.7, -0.05\n0.037\n-0.29\n-1.1, 0.51\n0.5\n-1.2\n-1.8, -0.53\n\n-2.3\n-3.2, -1.3\ndisplacement\n-0.01\n-0.03, 0.01\n0.3\n-0.02\n-0.03, -0.01\n\n-0.01\n-0.03, 0.00\n0.037\n-0.01\n-0.03, 0.01\n0.6horsepower\n-0.11\n-0.14, -0.08\n\n-0.07\n-0.12, -0.03\n0.003\n-0.07\n-0.10, -0.05\n\n-0.07\n-0.10, -0.05\nacceleration\n-0.39\n-0.61, -0.17\n\n-0.44\n-0.69, -0.18\n\n-0.54\n-0.73, -0.34\n\n-0.11\n-0.52, 0.31\n0.6origin\n1.7\n0.96, 2.4\n\n-0.08\n-1.1, 0.89\n0.9\n2.1\n1.1, 3.0\n\n1.9\n0.72, 3.1\n0.0021 CI = Confidence Interval\n    \n\nThe se = \"nid\" argument produces 95% confidence intervals and p-values, which allows to build this useful table. And if you want to learn how to produce similar publication ready tables for data summaries, results of statistical tests or models, check out my video on {gtsummary} package.\n7. Some further useful things\nConfidence intervals\nThere are several ways to compute confidence intervals for quantile regression. This can be specified using the \"se =\" option in the summary() or tbl_regression() functions. The default value is se=\"rank\", however, it does not deliver p-values, while other options ‚Äúnid‚Äù, ‚Äúiid‚Äù (not good), ‚Äúker‚Äù and ‚Äúboot‚Äù do (type ?summary.rq for details). However, using ‚Äúboot‚Äù is recommended only with large data-sets.\nEquality of slopes\nKhmaladze [1981] introduced the tests of equality of slopes across quantiles. Or anova() can compare two (better) or more slopes.\n\n\nKhmaladzeTest(wage ~ jobclass, data = Wage, \n              tau = seq(.05, .95, by = 0.05))\n\n\nTest of H_0: location \n\nJoint Test Statistic: 0.06793593 \n\nComponent Test Statistics: 0.06793593 \n\nanova(qm10, qm50)\n\nQuantile Regression Analysis of Deviance Table\n\nModel: wage ~ jobclass\nJoint Test of Equality of Slopes: tau in {  0.1 0.5  }\n\n  Df Resid Df F value   Pr(>F)   \n1  1     5999  6.6577 0.009896 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nanova(qm20, qm30, qm50, qm70)\n\nQuantile Regression Analysis of Deviance Table\n\nModel: wage ~ jobclass\nJoint Test of Equality of Slopes: tau in {  0.2 0.3 0.5 0.7  }\n\n  Df Resid Df F value Pr(>F)\n1  3    11997  0.8401 0.4717\n\nSpeed up the model\nThe default calculation method is method = \"br\". For more than a few thousand observations it is worthwhile considering method = \"fn\". For extremely large data sets use method = \"pfn\".\nContrasts in median regression\n\n\nmr <- rq(wage ~ education, data = Wage, tau = 0.5)\n\nemmeans::emmeans(mr, pairwise ~ education, weights = \"prop\", adjust = \"bonferroni\")\n\n$emmeans\n education          emmean   SE   df lower.CL upper.CL\n 1. < HS Grad         81.3 1.79 2995     77.8     84.8\n 2. HS Grad           94.1 1.13 2995     91.9     96.3\n 3. Some College     104.9 1.34 2995    102.3    107.5\n 4. College Grad     118.9 1.79 2995    115.4    122.4\n 5. Advanced Degree  141.8 2.46 2995    136.9    146.6\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                             estimate   SE   df t.ratio\n 1. < HS Grad - 2. HS Grad               -12.8 2.12 2995  -6.043\n 1. < HS Grad - 3. Some College          -23.6 2.23 2995 -10.589\n 1. < HS Grad - 4. College Grad          -37.6 2.53 2995 -14.853\n 1. < HS Grad - 5. Advanced Degree       -60.5 3.04 2995 -19.867\n 2. HS Grad - 3. Some College            -10.8 1.75 2995  -6.191\n 2. HS Grad - 4. College Grad            -24.8 2.12 2995 -11.702\n 2. HS Grad - 5. Advanced Degree         -47.7 2.71 2995 -17.585\n 3. Some College - 4. College Grad       -14.0 2.24 2995  -6.244\n 3. Some College - 5. Advanced Degree    -36.9 2.80 2995 -13.143\n 4. College Grad - 5. Advanced Degree    -22.9 3.05 2995  -7.511\n p.value\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n  <.0001\n\nP value adjustment: bonferroni method for 10 tests \n\nplot_model(mr, type = \"pred\")\n\n$education\n\n\nMedian regression with interactions\n\n\nmr <- rq(wage ~ education*jobclass, data = Wage, tau = 0.5)\n\nemmeans::emmeans(mr, pairwise ~ jobclass|education, weights = \"prop\", adjust = \"fdr\")\n\n$emmeans\neducation = 1. < HS Grad:\n jobclass       emmean   SE   df lower.CL upper.CL\n 1. Industrial    81.3 1.60 2990     78.2     84.4\n 2. Information   86.7 3.31 2990     80.2     93.2\n\neducation = 2. HS Grad:\n jobclass       emmean   SE   df lower.CL upper.CL\n 1. Industrial    93.5 1.07 2990     91.4     95.6\n 2. Information   95.2 2.07 2990     91.2     99.3\n\neducation = 3. Some College:\n jobclass       emmean   SE   df lower.CL upper.CL\n 1. Industrial   102.9 1.65 2990     99.6    106.1\n 2. Information  105.9 1.69 2990    102.6    109.2\n\neducation = 4. College Grad:\n jobclass       emmean   SE   df lower.CL upper.CL\n 1. Industrial   118.9 2.83 2990    113.3    124.4\n 2. Information  118.9 2.31 2990    114.3    123.4\n\neducation = 5. Advanced Degree:\n jobclass       emmean   SE   df lower.CL upper.CL\n 1. Industrial   134.7 6.82 2990    121.3    148.1\n 2. Information  141.8 2.83 2990    136.2    147.3\n\nConfidence level used: 0.95 \n\n$contrasts\neducation = 1. < HS Grad:\n contrast                       estimate   SE   df t.ratio p.value\n 1. Industrial - 2. Information    -5.41 3.68 2990  -1.472  0.1412\n\neducation = 2. HS Grad:\n contrast                       estimate   SE   df t.ratio p.value\n 1. Industrial - 2. Information    -1.74 2.33 2990  -0.750  0.4536\n\neducation = 3. Some College:\n contrast                       estimate   SE   df t.ratio p.value\n 1. Industrial - 2. Information    -3.06 2.36 2990  -1.294  0.1959\n\neducation = 4. College Grad:\n contrast                       estimate   SE   df t.ratio p.value\n 1. Industrial - 2. Information     0.00 3.66 2990   0.000  1.0000\n\neducation = 5. Advanced Degree:\n contrast                       estimate   SE   df t.ratio p.value\n 1. Industrial - 2. Information    -7.07 7.38 2990  -0.958  0.3383\n\nplot_model(mr, type = \"int\")\n\n\n\nBayesian median regression\n\n\nlibrary(brms)\n\nd <- tibble(\n  predictor = c(  1,   2,   3,  4,   5,   6,   7),\n  outcome   = c(1.5, 2.3, 2.8,  4.1, 5.3, 0, 6.8)\n)\n\nmr <- brm(\n  bf(outcome ~ predictor,\n     quantile = 0.5),\n  data = d, iter = 2000, warmup = 1000, chains = 4, refresh = 0,\n  family = asym_laplace(link_quantile = \"identity\")\n)\n\nmr2 <- quantreg::rq(outcome ~ predictor, data = d, tau = .5)\n\nfitted_brm <- fitted(mr, dpar = \"mu\")\n\nggplot(d, aes(predictor, outcome)) + \n  geom_point() + \n  geom_ribbon(aes(ymin = fitted_brm[,3], ymax = fitted_brm[,4], fill = 'brm'), alpha = 0.2) +\n  geom_line(aes(y = fitted(mr2), color = \"rq\")) + \n  geom_line(aes(y = fitted_brm[,1], color = \"brm\"))\n\n\n{lqmm} package: Fitting Linear Quantile Mixed Models\nRandom intercept model\n\n\nlibrary(lqmm)\ndata(Orthodont)\nrim <- lqmm(distance ~ age, random = ~ 1, group = Subject,\ntau = c(0.1,0.5,0.9), data = Orthodont)\nsummary(rim)\n\nCall: lqmm(fixed = distance ~ age, random = ~1, group = Subject, tau = c(0.1, \n    0.5, 0.9), data = Orthodont)\n\ntau = 0.1\n\nFixed effects:\n                Value Std. Error lower bound upper bound  Pr(>|t|)\n(Intercept) 16.733609   0.795732   15.134525     18.3327 < 2.2e-16\nage          0.522199   0.076843    0.367778      0.6766 1.375e-08\n               \n(Intercept) ***\nage         ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntau = 0.5\n\nFixed effects:\n                Value Std. Error lower bound upper bound  Pr(>|t|)\n(Intercept) 16.811968   0.794204   15.215955     18.4080 < 2.2e-16\nage          0.618802   0.086257    0.445462      0.7921 3.569e-09\n               \n(Intercept) ***\nage         ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntau = 0.9\n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(>|t|)    \n(Intercept) 16.82679    0.78848    15.24227     18.4113 < 2.2e-16 ***\nage          0.79619    0.10620     0.58277      1.0096 1.131e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAIC:\n[1] 476.0 (df = 4) 432.2 (df = 4) 485.4 (df = 4)\n\nRandom slope model\n\n\nrsm <- lqmm(distance ~ age, random = ~ age, group = Subject,\ntau = c(0.1,0.5,0.9), cov = \"pdDiag\", data = Orthodont)\nsummary(rsm)\n\nCall: lqmm(fixed = distance ~ age, random = ~age, group = Subject, \n    covariance = \"pdDiag\", tau = c(0.1, 0.5, 0.9), data = Orthodont)\n\ntau = 0.1\n\nFixed effects:\n                Value Std. Error lower bound upper bound Pr(>|t|)    \n(Intercept) 16.742845   0.633675   15.469428     18.0163  < 2e-16 ***\nage          0.570874   0.234984    0.098657      1.0431  0.01883 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntau = 0.5\n\nFixed effects:\n               Value Std. Error lower bound upper bound  Pr(>|t|)    \n(Intercept) 16.76179    0.63284    15.49006     18.0335 < 2.2e-16 ***\nage          0.65987    0.22714     0.20342      1.1163  0.005495 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ntau = 0.9\n\nFixed effects:\n                Value Std. Error lower bound upper bound  Pr(>|t|)\n(Intercept) 16.797380   0.641928   15.507377      18.087 < 2.2e-16\nage          0.620556   0.091293    0.437096       0.804 1.367e-08\n               \n(Intercept) ***\nage         ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAIC:\n[1] 529.0 (df = 5) 510.9 (df = 5) 475.0 (df = 5)\n\nFinal thoughs\nthe QR can be applied in any case where relationships for different levels of response variable are needed to be addressed differently\nthe more data you have, the more details QR can capture from the conditional distribution of response\nsplitting a sample into several small dataset (low values of outcome, high values of the outcome) and using LR on them reduces statistical power. Besides, the results could differ depending on where the cut point (e.g.¬†for low values) is set.\nthe interquantile range can be easily modeled and plotted with QR (i.e., .25, .50, .75), like a fancy box-plot for continuous variables :)\n\n\nset.seed(1)\no <-gcrq(wage ~ ps(age), \n         data = Wage %>% sample_n(1000), tau=seq(.25,.75,l=3))\n\n# par(mfrow=c(1,2)) # for several plots\nplot(o, legend=TRUE, conf.level = .95, shade=TRUE, lty = 1, lwd = 3, col = -1, res=TRUE) \n\n\n\nReferences and further readings\nThe best introduction to QR!!! https://books.google.de/books?id=Oc91AwAAQBAJ&printsec=frontcover&hl=de&source=gbs_ge_summary_r&cad=0#v=onepage&q&f=false\nQuantile Regression\nLingxin Hao - Johns Hopkins University, USA\nDaniel Q. Naiman - The Johns Hopkins University\nI loved this paper too! But, be careful about their interpretation using ‚Äúgap‚Äù, it is confusing and might be incorrect, as shown in the next reference: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4166511/pdf/nihms529550.pdf\nCommentary to the reference above with some corrections, among which the most important one - is that we can interpret the coefficients of QR as we do with OLS (page 9):\nhttps://srcd.onlinelibrary.wiley.com/doi/10.1111/cdev.13141\nhttp://www.econ.uiuc.edu/~roger/research/rq/QRJEP.pdf\nhttp://www.econ.uiuc.edu/~roger/research/rq/QReco.pdf\n\n\n\n",
    "preview": "posts/2022-12-01-quantileregression/thumbnail_quantile_regression.png",
    "last_modified": "2023-07-07T16:09:35+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2023-04-18-datawrangling3/",
    "title": "Transform Your Data Like a Pro with {tidyr} and Say Goodbye to Messy Data!",
    "description": "Every data scientist dreams of creating beautiful visualizations, conducting complex modeling, and diving into machine learning methods. However, most of the time, messy data hinders our ability to do really cool stuff. Thus, tidying up the data is the key to unlocking your full potential. Unfortunately, reshaping data in Excel can be a tedious and error-prone task. Do you remember the time when you needed to quickly transform columns to rows or rows to columns, split or combine columns, or handle missing values? With the {tidyr} package, you'll be able to transform your data quickly, accurately, and efficiently, preparing yourself for the stuff that really matters ;)",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-05-09",
    "categories": [
      "videos",
      "statistics",
      "data wrangling",
      "R package reviews"
    ],
    "contents": "\n\nContents\nThis post as a video\nReshape data: make then wider or longer\npivot_wider & spread\npivot_longer & gather\n\nSplit or combine cells\nunite\nseparate\n\nNested data\nCreate nested data\nReshape nested data\n\nExpand and complete tables\nexpand\ncomplete\n\nFurther readings and references\n\n‚ÄúHappy families are all alike; every unhappy family is unhappy in its own way.‚Äù \n‚Äì Leo Tolstoy\n\n‚ÄúTidy datasets are all alike, but every messy dataset is messy in its own way.‚Äù \n‚Äì Hadley Wickham\n\nThis picture shows three simple rules which make a dataset tidy1:\nEach column is a variable\nEach row is an observation\nEach cell is a single value\nBut it‚Äôs surprising, how rare these three rules are followed.\nThis post as a video\nTo maximize the effect of this post you should definitely work through Data Wrangling Vol. 1 and Data Wrangling Vol. 2 before. And I also recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 13 minutes long.\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\n\n\nReshape data: make then wider or longer\n\n\n\npivot_wider & spread\nImagine the situation where various types of information are combined into one column, but you need to ‚Äúspread‚Äù them out into separate columns, because the information they convey cannot coexist, like in columns type and count. The ‚Äúcases‚Äù and ‚Äúpopulation‚Äù values have nothing in common and therefore, don‚Äôt belong in the same column. So, we have to broaden our table by taking the column ‚Äúnames from‚Äù the ‚Äútype‚Äù and the ‚Äúvalues from‚Äù the ‚Äúcount‚Äù column. Simply put, we need to make our table wider. And while it‚Äôs easy to manually copy and paste the data for two categories, you wouldn‚Äôt want to do that with 100 categories.\n\n\n\ntable2 %>% \n  pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 √ó 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nA ‚Äúpivot_wider‚Äù is amazing, but if you work with real-world data (which is often dirty), you‚Äôll certainly encounter two common problems. The first one is that ‚Äúpivot_wider‚Äù will eventually produce missing values. For this case, ‚Äúpivot_wider‚Äù has an useful argument that fills in the missing values.\n\n\nx <- tibble(A = c(\"a\", \"b\", \"c\"), B = c(\"one\", \"two\", \"three\"), C = c(1, 2, 3) )\n\nx %>% pivot_wider(names_from = B, values_from = C)\n\n# A tibble: 3 √ó 4\n  A       one   two three\n  <chr> <dbl> <dbl> <dbl>\n1 a         1    NA    NA\n2 b        NA     2    NA\n3 c        NA    NA     3\n\nx %>% pivot_wider(names_from = B, values_from = C, values_fill = list(C = 0))\n\n# A tibble: 3 √ó 4\n  A       one   two three\n  <chr> <dbl> <dbl> <dbl>\n1 a         1     0     0\n2 b         0     2     0\n3 c         0     0     3\n\nThe second problem is that widening a table can lead to several values being put into one cell, resulting in a list of values that violates the third principle of tidy data - each cell is a single measurement.\n\nTo address this problem, you can utilize the ‚Äúvalues_fn‚Äù argument, which enables you to fill this cell with an aggregation function such as ‚Äúmean‚Äù. If there are still missing values after aggregation (because there was nothing to aggregate), you can fill them with a value of your choice using the ‚Äúvalues_fill‚Äù argument.\n\n\nmtcars %>% \n  pivot_wider(\n    id_cols = c(vs, am), \n    names_from = cyl, \n    values_from = mpg)\n\n# A tibble: 4 √ó 5\n     vs    am `6`       `4`       `8`       \n  <dbl> <dbl> <list>    <list>    <list>    \n1     0     1 <dbl [3]> <dbl [1]> <dbl [2]> \n2     1     1 <NULL>    <dbl [7]> <NULL>    \n3     1     0 <dbl [4]> <dbl [3]> <NULL>    \n4     0     0 <NULL>    <NULL>    <dbl [12]>\n\nmtcars %>% \n  pivot_wider(\n    id_cols = c(am, vs), \n    names_from = cyl, \n    values_from = mpg, \n    values_fn = list(mpg = mean),\n    values_fill = list(mpg = 9999) )\n\n# A tibble: 4 √ó 5\n     am    vs    `6`    `4`    `8`\n  <dbl> <dbl>  <dbl>  <dbl>  <dbl>\n1     1     0   20.6   26     15.4\n2     1     1 9999     28.4 9999  \n3     0     1   19.1   22.9 9999  \n4     0     0 9999   9999     15.0\n\nA ‚Äúpivot_wider‚Äù is the more intuitive next-generation version of the ‚Äúspread‚Äù command. However, ‚Äúspread‚Äù is still commonly used, so it remains in the package. Knowing how ‚Äúspread‚Äù works can help in understanding older code.\n\n\ntable2 %>% \n  spread(key = type, value = count) \n\n# A tibble: 6 √ó 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\npivot_longer & gather\nNow, imagine this: you‚Äôve got a bunch of data collected over a span of years. Usually, these surveys organize the measurements by year and put them in separate columns. So, you‚Äôd have a column for each specific year.\n\n\ntable4a\n\n# A tibble: 3 √ó 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\nHowever, storing data in this manner limits the information we can extract from it. By separating the measurements into different columns based on year, we lose the valuable variable - ‚Äútime‚Äù, which is imprisoned in the table header (imagine liquid metal T-1000 before passing prison bars), unable to be fully utilized.\n\nFor such a case, ‚Äúpivot_longer‚Äù takes multiple columns and combines them into just two columns - one with column names and one with survey values. This way, the information about time is freed up (T-1000 passing prison bars) and can be used for things like plotting or modeling.\n\n\ntable4a %>% \n  pivot_longer(cols = c(\"1999\", \"2000\"))\n\n# A tibble: 6 √ó 3\n  country     name   value\n  <chr>       <chr>  <dbl>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\nSo in this example, we kept it simple and just used two years. But let‚Äôs say you have a lot more data, like hundreds of columns. No problem, just use a ‚Äúcolon‚Äù between the first and last column names to quickly cover them all. And if you want to make things even clearer, you can use the ‚Äúnames_to‚Äù and ‚Äúvalues_to‚Äù arguments to give the new columns better names, like ‚Äúyear‚Äù and ‚Äúnumber of cases‚Äù for example.\n\n\ntable4a %>% \n  pivot_longer(cols = c(\"1999\" : \"2000\"), names_to = \"year\", values_to = \"number of cases\")\n\n# A tibble: 6 √ó 3\n  country     year  `number of cases`\n  <chr>       <chr>             <dbl>\n1 Afghanistan 1999                745\n2 Afghanistan 2000               2666\n3 Brazil      1999              37737\n4 Brazil      2000              80488\n5 China       1999             212258\n6 China       2000             213766\n\nAnd the ‚Äúpivot_longer‚Äù command is basically the new and improved version of the retired ‚Äúgather‚Äù command. But don‚Äôt worry, ‚Äúgather‚Äù is still around and people still use it, so it‚Äôs good to know how it works too.\n\n\ntable4a %>% \n  gather(2:3, key = \"year\", value = \"value\")\n\n# A tibble: 6 √ó 3\n  country     year   value\n  <chr>       <chr>  <dbl>\n1 Afghanistan 1999     745\n2 Brazil      1999   37737\n3 China       1999  212258\n4 Afghanistan 2000    2666\n5 Brazil      2000   80488\n6 China       2000  213766\n\n\n\n\nSplit or combine cells\nunite\n\nThe real-world data can be a bit messy (footbal star messy), which sometimes requires us to combine several columns into one. For example, separating a century and a year into different columns doesn‚Äôt make much sense, does it? That‚Äôs where the ‚Äúunite‚Äù command comes in handy.\nWith ‚Äúunite‚Äù, we can do a few things. First, we give a name to the new column. Second, we specify which columns to combine. Third, we determine what separator to use between the values. Fourth, we can decide whether to remove the input columns from the output data (but I like to keep them in to make sure everything is working properly). And finally, we can choose to remove any missing values.\nFor example, we could unite a ‚Äúcentury‚Äù and a ‚Äúyear‚Äù into a ‚Äúnew_year‚Äù column (imagine a picture of Santa here). After we‚Äôre confident that our code is working properly, we can set the ‚Äúremove‚Äù argument to TRUE and say goodbye to those old, separate columns.\n\n\ntable5 %>% \n  unite(\n    col = \"new_year\", \n    c(\"century\", \"year\"), \n    sep = \"\", \n    remove = FALSE, # remove = TRUE, \n    na.rm = TRUE)\n\n# A tibble: 6 √ó 5\n  country     new_year century year  rate             \n  <chr>       <chr>    <chr>   <chr> <chr>            \n1 Afghanistan 1999     19      99    745/19987071     \n2 Afghanistan 2000     20      00    2666/20595360    \n3 Brazil      1999     19      99    37737/172006362  \n4 Brazil      2000     20      00    80488/174504898  \n5 China       1999     19      99    212258/1272915272\n6 China       2000     20      00    213766/1280428583\n\nIsn‚Äôt that cool? However, it‚Äôs too early to celebrate just yet, because in the very next ‚Äúrate‚Äù column, we face an opposite problem where two variables ‚Äúcases‚Äù and ‚Äúpopulation‚Äù are trapped within the same column. Luckily, the ‚Äúseparate‚Äù command offers an intuitive solution to this issue.\nseparate\n\nLet me break this down for you. If you spot a separator within your data, you can utilize the ‚Äúsep‚Äù argument to inform the ‚Äúseparate‚Äù command where to split, and you can specify the names of new columns you want to create.\n\n\ntable5 %>% \n  unite(col = \"year\", c(\"century\", \"year\"), sep = \"\") %>% \n  separate(col = rate, sep = \"/\", into = c(\"cases\", \"population\"))\n\n# A tibble: 6 √ó 4\n  country     year  cases  population\n  <chr>       <chr> <chr>  <chr>     \n1 Afghanistan 1999  745    19987071  \n2 Afghanistan 2000  2666   20595360  \n3 Brazil      1999  37737  172006362 \n4 Brazil      2000  80488  174504898 \n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\nHowever, the best part is - ‚Äúseparate‚Äù is quite intelligent and can detect split-points that aren‚Äôt characters or letters automatically. This means that even if your dataset is super messy (football star messy), and different separators such as underscores, slashes, special characters or empty spaces are present, ‚Äúseparate‚Äù can still split it like a boss.\n\n\nblah <- tribble(\n  ~ID, ~month,\n  1,   \"a_c\",\n  2,   \"d/c\",\n  3,   \"7 & 8\",\n  4,   \"9 10\"\n)\n\nblah %>% \n  separate(month, into = c(\"month 1\", \"month 2\")) \n\n# A tibble: 4 √ó 3\n     ID `month 1` `month 2`\n  <dbl> <chr>     <chr>    \n1     1 a         c        \n2     2 d         c        \n3     3 7         8        \n4     4 9         10       \n\nBut wait, there‚Äôs more. There are actually two unique ways to separate a messy column:\nFirst, we can create several columns out of one, like we just did. But that‚Äôs only half of the story. Because we have some useful tricks that can make your life even easier. For instance, this output shows that two new columns are of the ‚Äúcharacter‚Äù type, put simply - text, because they inherited it from the original column. But I‚Äôd like them to be what they actually are - numbers. For this we can allow the ‚Äúseparate‚Äù command to ‚Äúconvert‚Äù them to a better format of it‚Äôs choice:\n\n\ntable3 %>% \n  separate(col = rate, into = c(\"cases\", \"population\"), convert = TRUE)\n\n# A tibble: 6 √ó 4\n  country      year  cases population\n  <chr>       <dbl>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nWe can also split a column without any separators by specifying where the values should be separated.\n\n\ntable1 %>% \n  separate(col = country, into = c(\"begin\", \"end\"), sep = 5) \n\n# A tibble: 6 √ó 5\n  begin end       year  cases population\n  <chr> <chr>    <dbl>  <dbl>      <dbl>\n1 Afgha \"nistan\"  1999    745   19987071\n2 Afgha \"nistan\"  2000   2666   20595360\n3 Brazi \"l\"       1999  37737  172006362\n4 Brazi \"l\"       2000  80488  174504898\n5 China \"\"        1999 212258 1272915272\n6 China \"\"        2000 213766 1280428583\n\nBut we have to be careful. Because, if we have too many pieces of text inside of a column, the automatic separation may leave out very important text.\n\n\nblah <- tribble(\n  ~sex, ~joke,\n  \"She\",   \"Darling, tell me something dirty!\",\n  \"He\",   \"Ahhhhh .... kitchen?\"\n)\n\nblah %>% \n  separate(col = joke, into = c(\"part 1\", \"part 2\"))\n\n# A tibble: 2 √ó 3\n  sex   `part 1` `part 2`\n  <chr> <chr>    <chr>   \n1 She   Darling  tell    \n2 He    Ahhhhh   kitchen \n\nTo prevent this, we can use the ‚Äúextra‚Äù option to ‚Äúmerge‚Äù the rest of the text together. And finally, to check the quality of separation, we again should use the ‚Äúremove = FALSE‚Äù option.\n\n\nblah %>% \n  separate(\n    col    = joke, \n    into   = c(\"part 1\", \"part 2\"),\n    extra  = \"merge\", \n    remove = FALSE) \n\n# A tibble: 2 √ó 4\n  sex   joke                              `part 1` `part 2`           \n  <chr> <chr>                             <chr>    <chr>              \n1 She   Darling, tell me something dirty! Darling  tell me something ‚Ä¶\n2 He    Ahhhhh .... kitchen?              Ahhhhh   kitchen?           \n\nSo, we now learned how to create several columns out of one. The second way to separate is to create several rows out of one. This is useful when messy data violates the third rule of a data set and contains multiple measurements in a single cell. It‚Äôs kind of ‚Äúpivot_longer‚Äù on steroids (Arni), because it makes a clean table from a single messy column, instead of multiple tidy columns.\n\n\nblah <- tribble(\n  ~season,    ~month,\n  \"Winter\",   \"Dez & Jan + Feb\",\n  \"Spring\",   \"Mar | Apr ? Mai\"\n)\n\nblah %>% \n  separate_rows(month) \n\n# A tibble: 6 √ó 2\n  season month\n  <chr>  <chr>\n1 Winter Dez  \n2 Winter Jan  \n3 Winter Feb  \n4 Spring Mar  \n5 Spring Apr  \n6 Spring Mai  \n\nNested data\nBut get ready for a plot twist! While storing several values in one cell is bad, storing several columns in one cell can actually be incredibly useful, because it helps to organize complex data and allows for more effective work with multiple sub-tables at once, using functions like ‚Äúmap‚Äù. For example, we can create multiple models and store their results for further use with just a few lines of code. I have also created a tutorial on multiple models, so feel free to check it out later. But for now let‚Äôs see how to create a nested data frame.\n\nCreate nested data\nCreating a nested data frame is as easy as just using ‚Äúgroup_by‚Äù and ‚Äúnest‚Äù functions to move the groups into a list-column. We can also specify the columns we want to ‚Äúnest‚Äù, which can help with large datasets. But what if we already have a nested data frame and need to access the data inside it?\n\n\nstorms %>%\n group_by(name) %>%\n nest()\n\n# A tibble: 258 √ó 2\n# Groups:   name [258]\n   name     data              \n   <chr>    <list>            \n 1 Amy      <tibble [31 √ó 12]>\n 2 Blanche  <tibble [20 √ó 12]>\n 3 Caroline <tibble [33 √ó 12]>\n 4 Doris    <tibble [29 √ó 12]>\n 5 Eloise   <tibble [46 √ó 12]>\n 6 Faye     <tibble [19 √ó 12]>\n 7 Gladys   <tibble [46 √ó 12]>\n 8 Hallie   <tibble [14 √ó 12]>\n 9 Belle    <tibble [18 √ó 12]>\n10 Dottie   <tibble [10 √ó 12]>\n# ‚Ñπ 248 more rows\n\nstorms %>%\n  group_by(name) %>%\n nest(data = c(year:long, category:hurricane_force_diameter))\n\n# A tibble: 1,035 √ó 3\n# Groups:   name [258]\n   name     status              data              \n   <chr>    <fct>               <list>            \n 1 Amy      tropical depression <tibble [8 √ó 11]> \n 2 Amy      tropical storm      <tibble [22 √ó 11]>\n 3 Amy      extratropical       <tibble [1 √ó 11]> \n 4 Blanche  tropical depression <tibble [9 √ó 11]> \n 5 Blanche  tropical storm      <tibble [4 √ó 11]> \n 6 Blanche  hurricane           <tibble [5 √ó 11]> \n 7 Blanche  extratropical       <tibble [2 √ó 11]> \n 8 Caroline tropical depression <tibble [22 √ó 11]>\n 9 Caroline tropical storm      <tibble [4 √ó 11]> \n10 Caroline hurricane           <tibble [7 √ó 11]> \n# ‚Ñπ 1,025 more rows\n\nReshape nested data\n\nWell, similarly to ‚Äúseparate_rows‚Äù which unpacks the single cell into several rows, {tidyr} package provides the ‚Äúunnest_longer‚Äù function, which unpacks nested data into multiple rows.\n\n\nstarwars %>%\n  select(name, films) %>%\n  filter(name %in% c(\"Luke Skywalker\", \"C-3PO\", \"R2-D2\")) %>%\n  unnest_longer(films)\n\n# A tibble: 18 √ó 2\n   name           films                  \n   <chr>          <chr>                  \n 1 Luke Skywalker The Empire Strikes Back\n 2 Luke Skywalker Revenge of the Sith    \n 3 Luke Skywalker Return of the Jedi     \n 4 Luke Skywalker A New Hope             \n 5 Luke Skywalker The Force Awakens      \n 6 C-3PO          The Empire Strikes Back\n 7 C-3PO          Attack of the Clones   \n 8 C-3PO          The Phantom Menace     \n 9 C-3PO          Revenge of the Sith    \n10 C-3PO          Return of the Jedi     \n11 C-3PO          A New Hope             \n12 R2-D2          The Empire Strikes Back\n13 R2-D2          Attack of the Clones   \n14 R2-D2          The Phantom Menace     \n15 R2-D2          Revenge of the Sith    \n16 R2-D2          Return of the Jedi     \n17 R2-D2          A New Hope             \n18 R2-D2          The Force Awakens      \n\nMoreover, we can easily turn each element of a list-column into a regular column via ‚Äúunnest_wider‚Äù function. But interestingly, that creates missing values, which uncovers a new problem.\n\n\nstarwars %>%\n  select(name, films) %>%\n  filter(name %in% c(\"Luke Skywalker\", \"C-3PO\", \"R2-D2\")) %>%\n  unnest_wider(films, names_sep = \"_\") %>% \n  flextable::regulartable()\n\nnamefilms_1films_2films_3films_4films_5films_6films_7Luke SkywalkerThe Empire Strikes BackRevenge of the SithReturn of the JediA New HopeThe Force AwakensC-3POThe Empire Strikes BackAttack of the ClonesThe Phantom MenaceRevenge of the SithReturn of the JediA New HopeR2-D2The Empire Strikes BackAttack of the ClonesThe Phantom MenaceRevenge of the SithReturn of the JediA New HopeThe Force Awakens\n\nExpand and complete tables\nexpand\nThe problem is that sometimes we think we have all the data we need, but in reality, we don‚Äôt. Let‚Äôs take, for example, a study on cars with different transmissions or gears. Before we dive in, we need to make sure we‚Äôre aware of how many unique values each variable has.\n\n\nxtabs(~ am, data = mtcars)\n\nam\n 0  1 \n19 13 \n\nxtabs(~ gear, data = mtcars)\n\ngear\n 3  4  5 \n15 12  5 \n\nHaving two categories from the transmission variable (‚Äúam‚Äù), and three from the gearbox (‚Äúgear‚Äù), you‚Äôd think we‚Äôd have six different combinations of cars, right? But if we look at the combination of both variables, we actually end up with only four distinct combinations.\n\n\nmtcars %>% distinct(am, gear)\n\n               am gear\nMazda RX4       1    4\nHornet 4 Drive  0    3\nMerc 240D       0    4\nPorsche 914-2   1    5\n\nAnd here‚Äôs where things get tricky. Even if we have a massive dataset, there are still some combos we might be missing. In our case we have zero cars with an automatic transmission and five gears, and not a single car with a manual transmission and three gears.\n\n\nxtabs(~ am + gear, data = mtcars)\n\n   gear\nam   3  4  5\n  0 15  4  0\n  1  0  8  5\n\nThis is a problem, because these missing values are kinda sneaky - they‚Äôre implicit, meaning we don‚Äôt even realize we‚Äôre missing them. And that‚Äôs the dangerous moment when we start to think we have all the data we need, when in reality, we don‚Äôt. Fortunately, ‚Äúexpand‚Äù function provides a solution for it, by finding all possible combinations of categorical variables we should have:\n\n\nmtcars %>% expand(am, gear)\n\n# A tibble: 6 √ó 2\n     am  gear\n  <dbl> <dbl>\n1     0     3\n2     0     4\n3     0     5\n4     1     3\n5     1     4\n6     1     5\n\nBut wouldn‚Äôt it be great if we could add those missing combos to our dataset, even if we don‚Äôt have any data for them yet? Well, yes! Why? Because then we could estimate what their values could probably be.\n\n\nd <- mtcars %>% \n  complete(cyl, am, gear) %>% \n  arrange(desc(is.na(mpg)))\n\nd\n\n# A tibble: 40 √ó 11\n     cyl    am  gear   mpg  disp    hp  drat    wt  qsec    vs  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     4     0     5  NA     NA     NA NA    NA     NA      NA    NA\n 2     4     1     3  NA     NA     NA NA    NA     NA      NA    NA\n 3     6     0     5  NA     NA     NA NA    NA     NA      NA    NA\n 4     6     1     3  NA     NA     NA NA    NA     NA      NA    NA\n 5     8     0     4  NA     NA     NA NA    NA     NA      NA    NA\n 6     8     0     5  NA     NA     NA NA    NA     NA      NA    NA\n 7     8     1     3  NA     NA     NA NA    NA     NA      NA    NA\n 8     8     1     4  NA     NA     NA NA    NA     NA      NA    NA\n 9     4     0     3  21.5  120.    97  3.7   2.46  20.0     1     1\n10     4     0     4  24.4  147.    62  3.69  3.19  20       1     2\n# ‚Ñπ 30 more rows\n\ncomplete\nAnd if you think that estimates are too uncertain, consider this: even a rough idea is often better than no information at all. That‚Äôs why it‚Äôs essential to fill in missing data to uncover new opportunities. Because, even when we have only three categorical variables from the ‚Äúmtcars‚Äù dataset, we‚Äôre missing eight combinations. Imagine how much would you miss, if you have 100 categorical variables with several categories each? This highlights the importance of dealing with missing values.\nLuckily for us {tidyr} package provides two ways to deal with missing values. The first is rather radical: we could simply remove all rows containing missing values, doesn‚Äôt matter implicit or explicit, using ‚Äúdrop_na‚Äù function. But that could eliminate most of our data, because missing values are very common, and we would have missed the opportunity to uncover something new and get a Nobel price.\n\n\nd %>% \n  drop_na()\n\n# A tibble: 32 √ó 11\n     cyl    am  gear   mpg  disp    hp  drat    wt  qsec    vs  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     4     0     3  21.5 120.     97  3.7   2.46  20.0     1     1\n 2     4     0     4  24.4 147.     62  3.69  3.19  20       1     2\n 3     4     0     4  22.8 141.     95  3.92  3.15  22.9     1     2\n 4     4     1     4  22.8 108      93  3.85  2.32  18.6     1     1\n 5     4     1     4  32.4  78.7    66  4.08  2.2   19.5     1     1\n 6     4     1     4  30.4  75.7    52  4.93  1.62  18.5     1     2\n 7     4     1     4  33.9  71.1    65  4.22  1.84  19.9     1     1\n 8     4     1     4  27.3  79      66  4.08  1.94  18.9     1     1\n 9     4     1     4  21.4 121     109  4.11  2.78  18.6     1     2\n10     4     1     5  26   120.     91  4.43  2.14  16.7     0     2\n# ‚Ñπ 22 more rows\n\nThe second way is to ‚Äúfill up‚Äù missing values with something useful. What is useful? Well, the average would do the job. So, let‚Äôs ‚Äúfill‚Äù the first three columns ‚Äúmpg, disp and hp‚Äù with their averages:\n\n\nmtcars %>% \n  complete(cyl, am, gear, \n           fill = list(mpg  = mean(mtcars$mpg), \n                       disp = mean(mtcars$disp), \n                       hp   = mean(mtcars$hp))) %>% \n  arrange(desc(is.na(wt)))\n\n# A tibble: 40 √ó 11\n     cyl    am  gear   mpg  disp    hp  drat    wt  qsec    vs  carb\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1     4     0     5  20.1  231.  147. NA    NA     NA      NA    NA\n 2     4     1     3  20.1  231.  147. NA    NA     NA      NA    NA\n 3     6     0     5  20.1  231.  147. NA    NA     NA      NA    NA\n 4     6     1     3  20.1  231.  147. NA    NA     NA      NA    NA\n 5     8     0     4  20.1  231.  147. NA    NA     NA      NA    NA\n 6     8     0     5  20.1  231.  147. NA    NA     NA      NA    NA\n 7     8     1     3  20.1  231.  147. NA    NA     NA      NA    NA\n 8     8     1     4  20.1  231.  147. NA    NA     NA      NA    NA\n 9     4     0     3  21.5  120.   97   3.7   2.46  20.0     1     1\n10     4     0     4  24.4  147.   62   3.69  3.19  20       1     2\n# ‚Ñπ 30 more rows\n\nHmm, the average may be better then nothing, but it‚Äôs far from perfect, right? But that‚Äôs not the point. The point is that you can first ‚Äúdiscover‚Äù your implicit missing values and then turn them into opportunity by filling them out with some sophisticated machine learning method, like chained random forest, which is unfortunately way outside of the scope of today‚Äôs topic. But if you want to learn more about imputing missing values with fancy machine learning algorithms, check out this video.\nThank you for reading!\nFurther readings and references\ncheat sheet on {tidyr}\nand the official page of {tidyr}\n\nAmazing and Free Book by Garrett Grolemund and Hadley Wickham: ‚ÄúR for Data Science‚Äù‚Ü©Ô∏é\n",
    "preview": "posts/2023-04-18-datawrangling3/dplyr_3_thumbnail.png",
    "last_modified": "2023-05-09T11:43:03+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2023-02-07-datawrangling2/",
    "title": "Advanced {dplyr}: 50+ Data Wrangling Techniques!",
    "description": "Have you ever been frustrated with messy data that seems impossible to analyze? Or have you ever spend hours cleaning and transforming data before you could even start producing results? Well, no worries! In this blog-post, I'll show you >50 Data Wrangling techniques, which will allow you to solve the most of your daily data manipulation challenges like a pro.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-04-23",
    "categories": [
      "videos",
      "statistics",
      "data wrangling"
    ],
    "contents": "\n\nContents\nThis post as a video\n1. Arrange\n2. Select\n3. Mutate\nWork across multiple columns simultaneously\nWork across multiple rows simultaneously\n\n4. Filter\nBonus chapter: Common Mistakes\n1. ‚Äú=‚Äù vs ‚Äú==‚Äù\n2. Specify all conditions\n3. Don‚Äôt ignore NAs!\n\n5. Group by\n6. Summarise\nFurther readings and references\n\n\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 17 minutes long.\n\n\n\n\n\n\n\nWe‚Äôll start with loading the tidyverse package. First, because dplyr package is part of it and secondly, tidyverse contains ready to use data-sets, such as ‚Äútable2‚Äù.\nWe‚Äôll use the ¬¥The big 6¬¥ English verbs (arrange, select, mutate, filter, group by and summarise), we learned from the first video of the series, but we now will dive deep into their arguments.\n\n\nlibrary(tidyverse)\n\n\n1. Arrange\n\nUsing the first verb arrange(), we can sort our table by one or more columns. We get the earliest year first, then within every year we‚Äôll display the lowest counts first.\n\n\n# this %>% is pipe, explained in the first video\ntable2 %>%  \n  arrange(year, count) \n\n# A tibble: 12 √ó 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Brazil       1999 cases           37737\n 3 China        1999 cases          212258\n 4 Afghanistan  1999 population   19987071\n 5 Brazil       1999 population  172006362\n 6 China        1999 population 1272915272\n 7 Afghanistan  2000 cases            2666\n 8 Brazil       2000 cases           80488\n 9 China        2000 cases          213766\n10 Afghanistan  2000 population   20595360\n11 Brazil       2000 population  174504898\n12 China        2000 population 1280428583\n\nThe only problem with that is, that arrange orders rows by values in the ascending (or - low to high) order by default. But we sometimes need the descending order to see the highest values first. For that we can use the desc function to sort a variable in the descending (or - high to low) order. While sorting several variables, only variables where you explicitly mention desc will be sorted in a descending order, the others will remain sorted in a default - ascending way. It is useful for the case when you‚Äôd want to see the earlier ears first, but at the same time the highest counts every year.\n\n\ntable2 %>% \n  arrange(year, desc(count)) \n\n# A tibble: 12 √ó 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 China        1999 population 1272915272\n 2 Brazil       1999 population  172006362\n 3 Afghanistan  1999 population   19987071\n 4 China        1999 cases          212258\n 5 Brazil       1999 cases           37737\n 6 Afghanistan  1999 cases             745\n 7 China        2000 population 1280428583\n 8 Brazil       2000 population  174504898\n 9 Afghanistan  2000 population   20595360\n10 China        2000 cases          213766\n11 Brazil       2000 cases           80488\n12 Afghanistan  2000 cases            2666\n\nAs you arrange your data, missing values will be neatly placed at the bottom of your table, regardless of whether you choose to sort in ascending or descending order.\n\n\nx <- tibble(y = c(1,2,NA,3,4))\nx %>% arrange(y) \n\n# A tibble: 5 √ó 1\n      y\n  <dbl>\n1     1\n2     2\n3     3\n4     4\n5    NA\n\nx %>% arrange(desc(y))\n\n# A tibble: 5 √ó 1\n      y\n  <dbl>\n1     4\n2     3\n3     2\n4     1\n5    NA\n\nAfter arranging data, the next most useful thing is to select the most useful columns via the‚Ä¶ drumroll‚Ä¶ select command.\n2. Select\n\n\nmtcars %>% \n  select(mpg, hp) %>% \n  glimpse()\n\nRows: 32\nColumns: 2\n$ mpg <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19‚Ä¶\n$ hp  <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180,‚Ä¶\n\nSelecting columns by simply writing their names into brackets seems like a great idea, but not when you‚Äôre dealing with a table of 300 columns. Writing out 100 column names in brackets is no fun, but don‚Äôt worry - there‚Äôs a solution. By using a colon between the variables you need, you can easily select multiple columns in the middle of your table.\n\n\nmtcars %>% \n  select(hp:vs) %>% \n  glimpse()\n\nRows: 32\nColumns: 5\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180‚Ä¶\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3‚Ä¶\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190,‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n\nCool, right? But sometimes, the columns in your table won‚Äôt be nicely grouped together, which means you can‚Äôt use the colon trick for selecting multiple columns. But there‚Äôs still an easy way to eliminate unwanted columns without typing out every single name. Just add a minus sign in front of the columns you want to remove. Some folks like to use an exclamation mark instead of the minus, but personally, I find the minus sign more intuitive.\n\n\nmtcars %>% \n  select(-(mpg:disp), !wt, -(am:carb)) %>% \n  glimpse()\n\nRows: 32\nColumns: 8\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180‚Ä¶\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3‚Ä¶\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190,‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1‚Ä¶\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,‚Ä¶\n\nStill a lot of typing? If you know the column numbers you need, you can actually code even faster. This way, your code won‚Äôt be dependent on specific column names, and you‚Äôll make less typing mistakes. However, let me give you a warning: unlike many other programming languages, in R, the first column is denoted by 1, not 0. I don‚Äôt know about you, but using 1 for the first column just makes more sense to me. So keep this in mind the next time you‚Äôre selecting columns in R using numbers.\n\n\nmtcars %>% \n  select(2:5, 7) \n\nmtcars %>% \n  select(-(2:5), -7)\n\n\nNow, with {dplyr}, you can make your life even easier if your column names follow certain patterns, such as ‚Äúmean_of_this‚Äù and ‚Äúmean_of_that‚Äù. Instead of tediously typing them all out, simply utilize {dplyr}‚Äôs powerful starts_with(), ends_with(), or contains() functions to quickly gather all the necessary columns. With this time-saving technique, you‚Äôll have more time to focus on the real work and get things done efficiently.\n\n\ndiamonds %>% \n  select(starts_with(\"c\")) %>% \n  glimpse()\n\nRows: 53,940\nColumns: 4\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22‚Ä¶\n$ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very‚Ä¶\n$ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J‚Ä¶\n$ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, ‚Ä¶\n\nmtcars %>% \n  select(ends_with(\"c\")) %>% \n  glimpse()\n\nRows: 32\nColumns: 1\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n\nmtcars %>% \n  select(contains(\"c\")) %>% \n  glimpse()\n\nRows: 32\nColumns: 3\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2‚Ä¶\n\nWant to apply multiple selection options at once? No problem - just use the ‚Äú&‚Äù or ‚Äú|‚Äù operators to define multiple conditions. You can also use the ‚Äúexclamation mark‚Äù to negate a condition. For example, if you want your final table to exclude any columns with the letter ‚Äúa‚Äù, but still include columns starting with ‚Äúc‚Äù, you can use the ‚Äúexclamation mark‚Äù before the first condition, ‚Äú&‚Äù between conditions, and no ‚Äúexclamation mark‚Äù before the second.\n\n\nmtcars %>%\n  select(!contains(\"a\") & contains(\"c\")) %>% \n  glimpse()\n\nRows: 32\nColumns: 2\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n\nSimilarly, if you only want to select variables starting with ‚Äúc‚Äù or ending with any letter except ‚Äúp‚Äù and ‚Äút‚Äù, you can use the ‚Äú|‚Äù operator and negate the endings with ‚Äúp‚Äù and ‚Äút‚Äù letters with the ‚Äúexclamation mark‚Äù.\n\n\nmtcars %>%\n  select(starts_with(\"c\") | !ends_with(c(\"p\", \"t\"))) %>% \n  glimpse()\n\nRows: 32\nColumns: 7\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2‚Ä¶\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4‚Ä¶\n\nIf you forget how to spell a variable, {dplyr} offers the one_of function to choose the correct name(s). This is helpful for managing ongoing flow of field data with common misspellings, which can prevent your pipeline from breaking down.\n\n\nmtcars %>% \n  select(one_of(\"weight\", \"wt\", \"wtf\", \"whatever\")) %>% \n  glimpse()\n\nRows: 32\nColumns: 1\n$ wt <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3‚Ä¶\n\nTo move important columns to the beginning of the table, list them first and add the everything() argument to include the rest.\n\n\nmtcars %>% \n  select(mpg, hp, everything()) %>% \n  glimpse()\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1‚Ä¶\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180‚Ä¶\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,‚Ä¶\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3‚Ä¶\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190,‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4‚Ä¶\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2‚Ä¶\n\nIf you prefer, you can use the function relocate instead, which does the same job as reordering columns manually.\n\n\nmtcars %>% \n  relocate(mpg, hp) %>% \n  glimpse()\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1‚Ä¶\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180‚Ä¶\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,‚Ä¶\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3‚Ä¶\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190,‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4‚Ä¶\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2‚Ä¶\n\nmtcars %>% \n  relocate(contains(\"c\"), hp) %>% \n  glimpse()\n\nRows: 32\nColumns: 11\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2‚Ä¶\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180‚Ä¶\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1‚Ä¶\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,‚Ä¶\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3‚Ä¶\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190,‚Ä¶\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4‚Ä¶\n\nTo fetch the last or third last column, simply request it with the last_col() function and use the offset argument. You see, with dplyr, programming feels entirely natural and instinctive.\n\n\nmtcars %>% \n  select(last_col()) %>% \n  glimpse()\n\nRows: 32\nColumns: 1\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2‚Ä¶\n\nmtcars %>% \n  select(last_col(offset = 3)) %>% \n  glimpse()\n\nRows: 32\nColumns: 1\n$ vs <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ‚Ä¶\n\nTo select specific columns that meet certain criteria, such as being numeric or textual, use select_if. This is particularly useful for large datasets with numerous columns, as it allows for easy selection of the desired variables without manual selection. For instance, when working with the diamonds dataset, we can avoid three ordered categorical columns by using select_if(is.numeric) in our pipeline.\n\n\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22‚Ä¶\n$ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very‚Ä¶\n$ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J‚Ä¶\n$ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, ‚Ä¶\n$ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1‚Ä¶\n$ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, ‚Ä¶\n$ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 33‚Ä¶\n$ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87‚Ä¶\n$ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78‚Ä¶\n$ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49‚Ä¶\n\ndiamonds %>% \n  select_if(is.numeric) %>% \n  glimpse()\n\nRows: 53,940\nColumns: 7\n$ carat <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, ‚Ä¶\n$ depth <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, ‚Ä¶\n$ table <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54‚Ä¶\n$ price <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339,‚Ä¶\n$ x     <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, ‚Ä¶\n$ y     <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, ‚Ä¶\n$ z     <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, ‚Ä¶\n\nWe can also transform particular variable names for our convenience, e.g.¬†we could change the case of the column names to upper (toupper) or to lower (tolower) cases by using select_at command.\n\n\nmtcars %>% \n  select_at(vars(mpg, hp), toupper) %>% \n  glimpse()\n\nRows: 32\nColumns: 2\n$ MPG <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19‚Ä¶\n$ HP  <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180,‚Ä¶\n\nEvery now and then we just want to have a look at the values of one specific column. To do so, the pull command comes in handy. And if you need to select a specific row, it‚Äôs as straightforward as specifying which one you want - whether it‚Äôs the first, last, ninth, or any other number.\n\n\nmtcars %>% pull(1)\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3\n[14] 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3\n[27] 26.0 30.4 15.8 19.7 15.0 21.4\n\n\n\nmtcars %>% first()\n\n  mpg cyl disp  hp drat   wt  qsec vs am gear carb\n1  21   6  160 110  3.9 2.62 16.46  0  1    4    4\n\nmtcars %>% last()\n\n   mpg cyl disp  hp drat   wt qsec vs am gear carb\n1 21.4   4  121 109 4.11 2.78 18.6  1  1    4    2\n\nmtcars %>% nth(9)\n\n   mpg cyl  disp hp drat   wt qsec vs am gear carb\n1 22.8   4 140.8 95 3.92 3.15 22.9  1  0    4    2\n\n3. Mutate\nNow, it‚Äôs time to transition away from reducing our dataset using ‚Äòselect‚Äô and ‚Äòfilter‚Äô commands and instead utilize the ‚Äòmutate‚Äô function to expand it. This command is intuitive because anything new in nature arises through mutations.\n\nBut before proceeding to the ‚Äòmutate()‚Äô function, we can add a new column using the ‚Äòadd_columns‚Äô function. It‚Äôs useful for adding IDs to each row. By default, the ‚Äòmutate()‚Äô command places new columns on the right-hand side, which can be difficult to keep track of with many columns. However, you can use the ‚Äò.before‚Äô argument to add new columns on the left-hand side. In case you want to add a new column ‚Äòbefore‚Äô or ‚Äòafter‚Äô a specific column, it‚Äôs preferable to use ‚Äòmutate‚Äô instead of ‚Äòadd_column‚Äô.\n\n\nmtcars %>% \n  select(1:3) %>% \n  add_column(new = 1:32)\n\n                     mpg cyl  disp new\nMazda RX4           21.0   6 160.0   1\nMazda RX4 Wag       21.0   6 160.0   2\nDatsun 710          22.8   4 108.0   3\nHornet 4 Drive      21.4   6 258.0   4\nHornet Sportabout   18.7   8 360.0   5\nValiant             18.1   6 225.0   6\nDuster 360          14.3   8 360.0   7\nMerc 240D           24.4   4 146.7   8\nMerc 230            22.8   4 140.8   9\nMerc 280            19.2   6 167.6  10\nMerc 280C           17.8   6 167.6  11\nMerc 450SE          16.4   8 275.8  12\nMerc 450SL          17.3   8 275.8  13\nMerc 450SLC         15.2   8 275.8  14\nCadillac Fleetwood  10.4   8 472.0  15\nLincoln Continental 10.4   8 460.0  16\nChrysler Imperial   14.7   8 440.0  17\nFiat 128            32.4   4  78.7  18\nHonda Civic         30.4   4  75.7  19\nToyota Corolla      33.9   4  71.1  20\nToyota Corona       21.5   4 120.1  21\nDodge Challenger    15.5   8 318.0  22\nAMC Javelin         15.2   8 304.0  23\nCamaro Z28          13.3   8 350.0  24\nPontiac Firebird    19.2   8 400.0  25\nFiat X1-9           27.3   4  79.0  26\nPorsche 914-2       26.0   4 120.3  27\nLotus Europa        30.4   4  95.1  28\nFord Pantera L      15.8   8 351.0  29\nFerrari Dino        19.7   6 145.0  30\nMaserati Bora       15.0   8 301.0  31\nVolvo 142E          21.4   4 121.0  32\n\nmtcars %>% \n  select(1:3) %>% \n  add_column(ID = 1:32, .before = T) %>% \n  glimpse()\n\nRows: 32\nColumns: 4\n$ ID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1‚Ä¶\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,‚Ä¶\n\nmtcars %>% \n  select(1:3) %>% \n  mutate(ID = 1:32, .after = cyl) %>% \n  glimpse()\n\nRows: 32\nColumns: 4\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1‚Ä¶\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ ID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, ‚Ä¶\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,‚Ä¶\n\nIn addition, you can either fill new columns with with whatever you want (playing God) or use existing columns to calculate new ones. And the best part is, right after you create a new column, you can use it for further calculations in the same chunk of code.\n\n\nmtcars %>% \n  mutate(make_new_column  = \"with whatever you want\") %>% \n  mutate(mpg_hp           = mpg / hp) %>% \n  mutate(use_new_column   = mpg_hp * 100) %>% \n  select(make_new_column, mpg_hp, use_new_column, everything()) %>% \n  glimpse()\n\nRows: 32\nColumns: 14\n$ make_new_column <chr> \"with whatever you want\", \"with whatever you‚Ä¶\n$ mpg_hp          <dbl> 0.19090909, 0.19090909, 0.24516129, 0.194545‚Ä¶\n$ use_new_column  <dbl> 19.090909, 19.090909, 24.516129, 19.454545, ‚Ä¶\n$ mpg             <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24‚Ä¶\n$ cyl             <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8,‚Ä¶\n$ disp            <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 36‚Ä¶\n$ hp              <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 12‚Ä¶\n$ drat            <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.‚Ä¶\n$ wt              <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.‚Ä¶\n$ qsec            <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15‚Ä¶\n$ vs              <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0,‚Ä¶\n$ am              <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,‚Ä¶\n$ gear            <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3,‚Ä¶\n$ carb            <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4,‚Ä¶\n\nAlthough mutate() keeps the old columns used to create new ones, you can use the transmute() command if you only want to retain the new variables that you have created.\n\n\nmtcars %>% \n  transmute(mpg_hp = mpg / hp) %>% \n  glimpse()\n\nRows: 32\nColumns: 1\n$ mpg_hp <dbl> 0.19090909, 0.19090909, 0.24516129, 0.19454545, 0.106‚Ä¶\n\nSimilarly, you can .keep only the ‚Äúused‚Äù columns. This retains only the columns involved or created via mutate(). It‚Äôs especially handy for large tables.\n\n\nmtcars %>% \n  mutate(mpg_hp = mpg / hp,\n    .keep = \"used\") %>% \n  glimpse()\n\nRows: 32\nColumns: 3\n$ mpg    <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8,‚Ä¶\n$ hp     <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 1‚Ä¶\n$ mpg_hp <dbl> 0.19090909, 0.19090909, 0.24516129, 0.19454545, 0.106‚Ä¶\n\nEven more convenient is the possibility to create multiple columns with a comma instead of repeating the mutate command for each new column. Near the usual arithmetic operations, like\nsubtract or multiply, you can easily\nrank the values in any columns or get a\ncumulative mean, cumulative sum, or cumulative product and even\nget a column with lagged or leading values if you need to and many more\n\n\nd <- tibble(\n  a = 1:5,\n  b = 6:10)\nd\n\n# A tibble: 5 √ó 2\n      a     b\n  <int> <int>\n1     1     6\n2     2     7\n3     3     8\n4     4     9\n5     5    10\n\nd %>% \n  mutate(substr  = b - mean(a),\n         divide  = b * sum(a),\n         rank    = rank(a),\n         cummean = cummean(a), # Cumulative mean\n         cumsum  = cumsum(a),\n         cumprod = cumprod(a),\n         lag_a   = lag(a),  \n         lead_a  = lead(a), \n         # or: <, <=, >, >=, !=, and == \n         logical = a > 3) %>%\n  glimpse()\n\nRows: 5\nColumns: 11\n$ a       <int> 1, 2, 3, 4, 5\n$ b       <int> 6, 7, 8, 9, 10\n$ substr  <dbl> 3, 4, 5, 6, 7\n$ divide  <int> 90, 105, 120, 135, 150\n$ rank    <dbl> 1, 2, 3, 4, 5\n$ cummean <dbl> 1.0, 1.5, 2.0, 2.5, 3.0\n$ cumsum  <int> 1, 3, 6, 10, 15\n$ cumprod <dbl> 1, 2, 6, 24, 120\n$ lag_a   <int> NA, 1, 2, 3, 4\n$ lead_a  <int> 2, 3, 4, 5, NA\n$ logical <lgl> FALSE, FALSE, FALSE, TRUE, TRUE\n\nFinally, before we go to the really cool stuff, I have to tell you that sometimes it‚Äôs useful to be able to transfer the row names to a real column, or vice versa, any column to row-names. Well, with {dplyr} it could not be more intuitive, because programming in {dplyr} is very close to normal English:\n\n\nmtcars %>% \n  select(-1:4) %>% \n  rownames_to_column() %>% \n  head()   # \"head\" shows only first 6 rows of a dataset, \"tail\" the last 6 row\n\n            rowname cyl disp  hp\n1         Mazda RX4   6  160 110\n2     Mazda RX4 Wag   6  160 110\n3        Datsun 710   4  108  93\n4    Hornet 4 Drive   6  258 110\n5 Hornet Sportabout   8  360 175\n6           Valiant   6  225 105\n\nd %>% \n  column_to_rownames(\"b\") \n\n   a\n6  1\n7  2\n8  3\n9  4\n10 5\n\nWork across multiple columns simultaneously\nSo, we have learned a few good techniques already. But you can become even more effective, when you work with several columns at the same time! For example, you can select particular numeric variables in your table with mutate_at() function and make them categorical using factor argument. In order to get averages for ALL variables, use summarise_all(). Or, when you want to transform all numeric columns with a logarithm,\nuse mutate_if function,\nthen provide an argument is.numeric and finally\ngive a command of your choice, for example log\n\n\nmtcars %>%\n  mutate_at(vars(cyl, am, gear), factor) %>% \n  glimpse()\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1‚Ä¶\n$ cyl  <fct> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,‚Ä¶\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180‚Ä¶\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3‚Ä¶\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190,‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ am   <fct> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ gear <fct> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4‚Ä¶\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2‚Ä¶\n\nmtcars %>%\n  summarise_all(mean) \n\n       mpg    cyl     disp       hp     drat      wt     qsec     vs\n1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375\n       am   gear   carb\n1 0.40625 3.6875 2.8125\n\niris %>%\n  mutate_if(is.numeric, log) %>% \n  glimpse()\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 1.629241, 1.589235, 1.547563, 1.526056, 1.60943‚Ä¶\n$ Sepal.Width  <dbl> 1.252763, 1.098612, 1.163151, 1.131402, 1.28093‚Ä¶\n$ Petal.Length <dbl> 0.33647224, 0.33647224, 0.26236426, 0.40546511,‚Ä¶\n$ Petal.Width  <dbl> -1.6094379, -1.6094379, -1.6094379, -1.6094379,‚Ä¶\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa,‚Ä¶\n\nSince R is a very intuitive programming language and since _at, _if and _all allow you to make any calculation across multiple columns, it didn‚Äôt take long until a function named - across was created, which unites the capabilities of _at, _if and _all. For instance we can\neasily round particular columns or\ncalculate any statistics, like mean or standard deviation, for all groups of any categorical variable:\n\n\nmtcars %>%\n  mutate(across(c(mpg, wt), round)) %>% \n  glimpse()\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21, 21, 23, 21, 19, 18, 14, 24, 23, 19, 18, 16, 17, 15,‚Ä¶\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4‚Ä¶\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,‚Ä¶\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180‚Ä¶\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3‚Ä¶\n$ wt   <dbl> 3, 3, 2, 3, 3, 3, 4, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 2, 2‚Ä¶\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,‚Ä¶\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1‚Ä¶\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4‚Ä¶\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2‚Ä¶\n\niris %>%\n  group_by(Species) %>%\n  summarise(\n    across(\n      starts_with(\"Sepal\"), \n      list(mean = mean, sd = sd) ) )\n\n# A tibble: 3 √ó 5\n  Species    Sepal.Length_mean Sepal.Length_sd Sepal.Width_m‚Ä¶¬π Sepal‚Ä¶¬≤\n  <fct>                  <dbl>           <dbl>           <dbl>   <dbl>\n1 setosa                  5.01           0.352            3.43   0.379\n2 versicolor              5.94           0.516            2.77   0.314\n3 virginica               6.59           0.636            2.97   0.322\n# ‚Ä¶ with abbreviated variable names ¬π‚ÄãSepal.Width_mean,\n#   ¬≤‚ÄãSepal.Width_sd\n\nWork across multiple rows simultaneously\nSo, summarizing values within columns is cool, but what if we want to summarize values of rows across multiple columns? Well, we can easily do that by using the rowwise()and c_across() functions to perform row-wise aggregations.\n\n\ndf <- tibble(a = 1:4, b = 4:1, c = c(5:8))\n\ndf %>%\n  rowwise() %>%\n  mutate(\n    sum = sum(c_across(a:c)),\n    sd  = sd(c_across(a:c))\n  )\n\n# A tibble: 4 √ó 5\n# Rowwise: \n      a     b     c   sum    sd\n  <int> <int> <int> <int> <dbl>\n1     1     4     5    10  2.08\n2     2     3     6    11  2.08\n3     3     2     7    12  2.65\n4     4     1     8    13  3.51\n\n4. Filter\n\nSpeaking of rows: sometimes we need only a few particular rows. Using the filter function and logical operators such as < (less than), <= (less than or equal to), > (greater than), >= (greater than or equal to), != (not equal), == (equal), & (and), | (or), we can tell R exactly what data we want. We may even employ multiple operators simultaneously, should the need arise, by using the ‚Äú&‚Äù operator to verify several conditions, or by using the vertical line ‚Äú|‚Äù to determine either condition:\n\n\n\nmtcars %>% \n  filter(cyl == 6, vs == 0)\n\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nFerrari Dino  19.7   6  145 175 3.62 2.770 15.50  0  1    5    6\n\nmtcars %>% \n  filter(cyl == 6 & vs == 0 & gear == 4)\n\n              mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21   6  160 110  3.9 2.875 17.02  0  1    4    4\n\nmtcars %>% \n  filter(cyl == 4 & gear != 4)\n\n               mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nToyota Corona 21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nPorsche 914-2 26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa  30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n\nmtcars %>% \n  filter(mpg < 30, hp > 200, am == 0) %>% \n  glimpse()\n\nRows: 5\nColumns: 11\n$ mpg  <dbl> 14.3, 10.4, 10.4, 14.7, 13.3\n$ cyl  <dbl> 8, 8, 8, 8, 8\n$ disp <dbl> 360, 472, 460, 440, 350\n$ hp   <dbl> 245, 205, 215, 230, 245\n$ drat <dbl> 3.21, 2.93, 3.00, 3.23, 3.73\n$ wt   <dbl> 3.570, 5.250, 5.424, 5.345, 3.840\n$ qsec <dbl> 15.84, 17.98, 17.82, 17.42, 15.41\n$ vs   <dbl> 0, 0, 0, 0, 0\n$ am   <dbl> 0, 0, 0, 0, 0\n$ gear <dbl> 3, 3, 3, 3, 3\n$ carb <dbl> 4, 4, 4, 4, 4\n\nmtcars %>% \n  filter(cyl == 8 | am == 0) %>% \n  glimpse()\n\nRows: 21\nColumns: 11\n$ mpg  <dbl> 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8, 16.4, 1‚Ä¶\n$ cyl  <dbl> 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 8, 8, 8, 8‚Ä¶\n$ disp <dbl> 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 167.6, 167.6,‚Ä¶\n$ hp   <dbl> 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180, 20‚Ä¶\n$ drat <dbl> 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92, 3.07, 3‚Ä¶\n$ wt   <dbl> 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.440, 3.440,‚Ä¶\n$ qsec <dbl> 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18.30, 18.90,‚Ä¶\n$ vs   <dbl> 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0‚Ä¶\n$ am   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0‚Ä¶\n$ gear <dbl> 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3‚Ä¶\n$ carb <dbl> 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 2, 4, 2‚Ä¶\n\nmtcars %>% \n  filter(mpg > 30 | hp < 90)\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMerc 240D      24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nFiat 128       32.4   4  78.7  66 4.08 2.200 19.47  1  1    4    1\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nToyota Corolla 33.9   4  71.1  65 4.22 1.835 19.90  1  1    4    1\nFiat X1-9      27.3   4  79.0  66 4.08 1.935 18.90  1  1    4    1\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\n\nBut what if we need to filter out several values or categories? Well, in this case we‚Äôll utilize the %in% argument with the concatinate argument c():\n\n\nmtcars %>% \n  filter(cyl %in% c(4,6) & gear %in% c(3,5))\n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nValiant        18.1   6 225.0 105 2.76 3.460 20.22  1  0    3    1\nToyota Corona  21.5   4 120.1  97 3.70 2.465 20.01  1  0    3    1\nPorsche 914-2  26.0   4 120.3  91 4.43 2.140 16.70  0  1    5    2\nLotus Europa   30.4   4  95.1 113 3.77 1.513 16.90  1  1    5    2\nFerrari Dino   19.7   6 145.0 175 3.62 2.770 15.50  0  1    5    6\n\nFurthermore, you can slice the dataset to retrieve specific rows, such as rows 1 to 3. Additionally, you can easily select random rows using the slice_sample() function. With the n argument, you can specify the number of random rows to retrieve, or with the prop argument, you can specify the proportion of the data to retrieve. Furthermore, you can use slice_head() to extract the top two rows and slice_tail() to retrieve the last three values. To obtain the two smallest or three largest values from any column, you can use the slice_min() and slice_max() commands, which are even more intuitive.\n\n\nmtcars %>% \n  slice(1:3)\n\n               mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n\nmtcars %>% \n  slice_sample(n = 4)\n\n                    mpg cyl  disp  hp drat    wt  qsec vs am gear\nDuster 360         14.3   8 360.0 245 3.21 3.570 15.84  0  0    3\nHornet 4 Drive     21.4   6 258.0 110 3.08 3.215 19.44  1  0    3\nCadillac Fleetwood 10.4   8 472.0 205 2.93 5.250 17.98  0  0    3\nMerc 450SE         16.4   8 275.8 180 3.07 4.070 17.40  0  0    3\n                   carb\nDuster 360            4\nHornet 4 Drive        1\nCadillac Fleetwood    4\nMerc 450SE            3\n\nmtcars %>% \n  slice_sample(prop = .1) # since we have 32 rows, 10% of it would be 3 \n\n                mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nDuster 360     14.3   8 360.0 245 3.21 3.570 15.84  0  0    3    4\nHonda Civic    30.4   4  75.7  52 4.93 1.615 18.52  1  1    4    2\nHornet 4 Drive 21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\n\nmtcars %>% \n  slice_head(n = 2)\n\n              mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4      21   6  160 110  3.9 2.620 16.46  0  1    4    4\nMazda RX4 Wag  21   6  160 110  3.9 2.875 17.02  0  1    4    4\n\nmtcars %>% \n  slice_tail(n = 3)\n\n               mpg cyl disp  hp drat   wt qsec vs am gear carb\nFerrari Dino  19.7   6  145 175 3.62 2.77 15.5  0  1    5    6\nMaserati Bora 15.0   8  301 335 3.54 3.57 14.6  0  1    5    8\nVolvo 142E    21.4   4  121 109 4.11 2.78 18.6  1  1    4    2\n\nmtcars %>% \n  group_by(am) %>% \n  slice_min(mpg, n = 2)\n\n# A tibble: 4 √ó 11\n# Groups:   am [2]\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  10.4     8   472   205  2.93  5.25  18.0     0     0     3     4\n2  10.4     8   460   215  3     5.42  17.8     0     0     3     4\n3  15       8   301   335  3.54  3.57  14.6     0     1     5     8\n4  15.8     8   351   264  4.22  3.17  14.5     0     1     5     4\n\nmtcars %>% \n  group_by(am) %>% \n  slice_max(hp, n = 3)\n\n# A tibble: 6 √ó 11\n# Groups:   am [2]\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  14.3     8   360   245  3.21  3.57  15.8     0     0     3     4\n2  13.3     8   350   245  3.73  3.84  15.4     0     0     3     4\n3  14.7     8   440   230  3.23  5.34  17.4     0     0     3     4\n4  15       8   301   335  3.54  3.57  14.6     0     1     5     8\n5  15.8     8   351   264  4.22  3.17  14.5     0     1     5     4\n6  19.7     6   145   175  3.62  2.77  15.5     0     1     5     6\n\nAnd final thing for slicing is that, using prop argument you can easily obtain the top 10% or the lowest 10% of specific values in each group:\n\n\nmtcars %>% \n  group_by(cyl) %>% \n  slice_max(mpg, prop = 0.1)\n\n# A tibble: 2 √ó 11\n# Groups:   cyl [2]\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4     1\n2  19.2     8 400     175  3.08  3.84  17.0     0     0     3     2\n\nAnother useful way to filter is when you want to determine how many distinct categories or values a particular column has. This also works for multiple columns. Additionally, if you want to retain other columns when filtering for unique rows, you can use the .keep_all = TRUE option. Alternatively, if you want to know the frequency of unique values in your table, use the count() function instead of distinct().\n\n\nmtcars %>% \n  distinct(cyl) \n\n                  cyl\nMazda RX4           6\nDatsun 710          4\nHornet Sportabout   8\n\nmtcars %>% \n  distinct(cyl, am) \n\n                  cyl am\nMazda RX4           6  1\nDatsun 710          4  1\nHornet 4 Drive      6  0\nHornet Sportabout   8  0\nMerc 240D           4  0\nFord Pantera L      8  1\n\nmtcars %>% \n  distinct(cyl, am, .keep_all = TRUE) \n\n                   mpg cyl  disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6 160.0 110 3.90 2.620 16.46  0  1    4    4\nDatsun 710        22.8   4 108.0  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6 258.0 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8 360.0 175 3.15 3.440 17.02  0  0    3    2\nMerc 240D         24.4   4 146.7  62 3.69 3.190 20.00  1  0    4    2\nFord Pantera L    15.8   8 351.0 264 4.22 3.170 14.50  0  1    5    4\n\nmtcars %>% \n  count(cyl, am) \n\n  cyl am  n\n1   4  0  3\n2   4  1  8\n3   6  0  4\n4   6  1  3\n5   8  0 12\n6   8  1  2\n\nThen, we sometime need to remove duplicate rows from our dataset, right? We can easily do that by either using distinct_all() command, or, by using even more intuitive unique() command:\n\n\nbla <- tibble(y = c(2,2,NA,3,4,4),\n       y2 = c(2,2,2,NA,4,4))\n\ndistinct_all(bla)\n\n# A tibble: 4 √ó 2\n      y    y2\n  <dbl> <dbl>\n1     2     2\n2    NA     2\n3     3    NA\n4     4     4\n\nunique(bla)\n\n# A tibble: 4 √ó 2\n      y    y2\n  <dbl> <dbl>\n1     2     2\n2    NA     2\n3     3    NA\n4     4     4\n\nFinally, similarly to add_column command, you can add_rows to your dataset:\n\n\nbla %>% \n  add_row(y = 100, y2 = 999)\n\n# A tibble: 7 √ó 2\n      y    y2\n  <dbl> <dbl>\n1     2     2\n2     2     2\n3    NA     2\n4     3    NA\n5     4     4\n6     4     4\n7   100   999\n\nBonus chapter: Common Mistakes\nBy the way, here are three common mistakes I used to make often, which caused me a lot of frustration, and which I want to save you from.\n1. ‚Äú=‚Äù vs ‚Äú==‚Äù\nThe first mistake is using the assignment operator (single equal sign) ‚Äú=‚Äù instead of the comparison operator (double equal sign) ‚Äú==‚Äù while testing for equality. Fortunately, filter() command will let you know about this mistake with an informative error message:\n\n\nmtcars |>        # |> is similar to %>% \n  filter(am = 1)\n\n\n\"Error in `filter()`:\n! We detected a named input.\n‚Ñπ This usually means that you've used `=` instead\n  of `==`.\n‚Ñπ Did you mean `am == 1`?\nRun `rlang::last_error()` to see where the error occurred.\"\n2. Specify all conditions\nThe second mistake is to specify the first condition and then use ‚Äúor‚Äù without explicitly specifying the second condition. Although it ‚Äúworks‚Äù without throwing an error, it does not achieve the intended purpose because ‚Äú|‚Äù first checks the more important condition ‚Äúmpg == 14.7‚Äù before checking the less important second condition.\n\n\n# wrong\nmtcars |> \n  filter(mpg == 14.7 | 16.4)\n\n# correct\nmtcars |> \n  filter(mpg == 14.7 | cyl == 16.4)\n\n\n3. Don‚Äôt ignore NAs!\n‚ÄúNow listen to me very carefully‚Äù (Arnold Schwarzenegger from Terminator 2), because what I am about to tell you could save a lot of frustration. ‚ÄúNAs‚Äù, which is the abbreviation for Not Available or simply for missing values, are highly contagious. Any operation involving an unknown value will also become unknown. To safeguard your data analysis, remove these ‚ÄúNAs‚Äù with the na.rm argument.\n\n\nx <- tibble(\n  y  = c(2,2,NA,3,4,4),\n  y2 = c(2,2,2,NA,4,4)) \n\nx %>% summarise(mean(y), mean(y2))\n\n# A tibble: 1 √ó 2\n  `mean(y)` `mean(y2)`\n      <dbl>      <dbl>\n1        NA         NA\n\nx %>% summarise(mean(y, na.rm = TRUE), mean(y2, na.rm = TRUE))\n\n# A tibble: 1 √ó 2\n  `mean(y, na.rm = TRUE)` `mean(y2, na.rm = TRUE)`\n                    <dbl>                    <dbl>\n1                       3                      2.8\n\n5. Group by\n\nThe group_by() divides your dataset into groups meaningful for your analysis. The top_n is the older version of slice_max() we just learned in the previous section, and it also gives you top values for every group. For instance, let‚Äôs pick the two most effective cars (highest mpg) for every transmission type (am):\n\n\nmtcars %>% \n  group_by(am) %>% \n  top_n(2, mpg)\n\n# A tibble: 4 √ó 11\n# Groups:   am [2]\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  24.4     4 147.     62  3.69  3.19  20       1     0     4     2\n2  22.8     4 141.     95  3.92  3.15  22.9     1     0     4     2\n3  32.4     4  78.7    66  4.08  2.2   19.5     1     1     4     1\n4  33.9     4  71.1    65  4.22  1.84  19.9     1     1     4     1\n\ngroup_by_all is particularly useful for identifying duplicates in your dataset, because after grouping, a simple count() will reveal the frequencies, which can be sorted in descending order using desc option to bring the duplicates to the top of the resulting table. Than, you could filter out all values with n > 1 to keep only unique rows.\n\n\nblup <- tibble(y = c(2,2,NA,3,4,4),\n       y2 = c(2,2,2,NA,4,4)) %>% \n  group_by_all() %>% \n  count() %>% \n  arrange(desc(n))\n\nblup\n\n# A tibble: 4 √ó 3\n# Groups:   y, y2 [4]\n      y    y2     n\n  <dbl> <dbl> <int>\n1     2     2     2\n2     4     4     2\n3     3    NA     1\n4    NA     2     1\n\nGroups are excellent for summarizing data, but at times, they might hinder calculations. Therefore, if needed, you can always ungroup any dataset before complaining about R being stupid. That‚Äôs what I used to do before I learned about ungrouping ;)\n\n\nblup %>% \n  summarise(mean(n)) \n\n# A tibble: 4 √ó 3\n# Groups:   y [4]\n      y    y2 `mean(n)`\n  <dbl> <dbl>     <dbl>\n1     2     2         2\n2     3    NA         1\n3     4     4         2\n4    NA     2         1\n\nblup %>% \n  ungroup() %>% \n  summarise(mean(n)) \n\n# A tibble: 1 √ó 1\n  `mean(n)`\n      <dbl>\n1       1.5\n\nSimilar, to select and mutate you can group variables conditionally using group_by_if, group_by_at and group_by_all. For instance, let‚Äôs change three variables to factors (categories) and group by only categorical (factor) variables after it. The dplyr package will find all factors by itself, which is particularly useful for big datasets. Once grouped, computing group-wise averages becomes a very simple task.\n\n\nmtcars %>% \n  mutate_at(vars(cyl, gear, am), factor) %>% \n  group_by_if(is.factor) %>% \n  summarise(avg = mean(mpg))\n\n# A tibble: 10 √ó 4\n# Groups:   cyl, am [6]\n   cyl   am    gear    avg\n   <fct> <fct> <fct> <dbl>\n 1 4     0     3      21.5\n 2 4     0     4      23.6\n 3 4     1     4      28.0\n 4 4     1     5      28.2\n 5 6     0     3      19.8\n 6 6     0     4      18.5\n 7 6     1     4      21  \n 8 6     1     5      19.7\n 9 8     0     3      15.0\n10 8     1     5      15.4\n\n\n\n\n\n\n\n\nAs mentioned in the previous video (see above if you want), group_by() is the most effective if used with summarise() function, because they allow you to aggregate data and calculate any summary statistics you want.\n6. Summarise\nHere‚Äôs an example: let‚Äôs say you have a dataset about flowers in a field with three different species. You could use the ‚Äúsummarise‚Äù function to calculate the total lengths and widths of flowers for each species separately.\n\n\niris %>% \n  group_by(Species) %>% \n  summarise(total_length = sum(Sepal.Length))\n\n# A tibble: 3 √ó 2\n  Species    total_length\n  <fct>             <dbl>\n1 setosa             250.\n2 versicolor         297.\n3 virginica          329.\n\nIn this code, ‚Äúgroup_by‚Äù specifies the column by which the data should be grouped, namely ‚ÄúSpecies‚Äù and ‚Äúsummarise‚Äù calculates the total lengths of flowers for each Species.\nYou can also do conditional summarizing across multiple columns at once using summarise_at, summarise_if, and summarise_all. This can save you a lot of time if you have a table with lots of columns.\n\n\nmtcars %>% \n  group_by(am) %>% \n  summarise_at(vars(mpg, hp, wt), mean)\n\n# A tibble: 2 √ó 4\n     am   mpg    hp    wt\n  <dbl> <dbl> <dbl> <dbl>\n1     0  17.1  160.  3.77\n2     1  24.4  127.  2.41\n\nmtcars %>% \n  group_by(cyl) %>% \n  summarise_if(is.numeric, mean, na.rm = TRUE)\n\n# A tibble: 3 √ó 11\n    cyl   mpg  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1     4  26.7  105.  82.6  4.07  2.29  19.1 0.909 0.727  4.09  1.55\n2     6  19.7  183. 122.   3.59  3.12  18.0 0.571 0.429  3.86  3.43\n3     8  15.1  353. 209.   3.23  4.00  16.8 0     0.143  3.29  3.5 \n\nmtcars %>% \n  group_by(vs) %>% \n  summarise_all(list(med = median))\n\n# A tibble: 2 √ó 11\n     vs mpg_med cyl_med disp_med hp_med drat_med wt_med qsec_‚Ä¶¬π am_med\n  <dbl>   <dbl>   <dbl>    <dbl>  <dbl>    <dbl>  <dbl>   <dbl>  <dbl>\n1     0    15.6       8     311     180     3.18   3.57    17.0    0  \n2     1    22.8       4     121.     96     3.92   2.62    19.2    0.5\n# ‚Ä¶ with 2 more variables: gear_med <dbl>, carb_med <dbl>, and\n#   abbreviated variable name ¬π‚Äãqsec_med\n\nBut ‚Äúsummarize‚Äù is even more useful when you need to calculate multiple summary statistics for multiple variables at once. For instance, consider a dataset about cars. By using ‚Äúsummarise,‚Äù you can count the number of cars with different cylinders, obtain the mean and standard deviation for each cylinder separately to compare mileage, and find any quantile of horsepower. The median, which might be more useful than the average, is taken at the 0.5 quantile. Additionally, you can define what makes a car ‚Äústrong‚Äù and count how many strong cars each category contains. It‚Äôs amazing how just a few lines of code can help you quickly explore your dataset. However, if you want to explore your dataset in just one line of code and get even more valuable insights, check out this video on the automated exploratory analysis.\n\n\nmtcars %>% \n  group_by(cyl) %>% \n  summarise(count     = n(), \n            aver_mpg  = mean(mpg),\n            sd_mpg    = sd(mpg),\n            Q_0.25    = quantile(hp, 0.5), \n            n_of_strong_cars = sum(hp > 100))\n\n# A tibble: 3 √ó 6\n    cyl count aver_mpg sd_mpg Q_0.25 n_of_strong_cars\n  <dbl> <int>    <dbl>  <dbl>  <dbl>            <int>\n1     4    11     26.7   4.51    91                 2\n2     6     7     19.7   1.45   110                 7\n3     8    14     15.1   2.56   192.               14\n\nFurther readings and references\nAmazing and Free Book by Garrett Grolemund and Hadley Wickham: ‚ÄúR for Data Science‚Äù\nfor more info on dplyr package go here\nto download a Cheat Sheet (they both are really good) go here or there\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2023-02-07-datawrangling2/dplyr_2_thumbnail.png",
    "last_modified": "2023-04-23T00:53:53+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2023-01-31-datawrangling1/",
    "title": "Top 10 Must-Know {dplyr} Commands for Data Wrangling in R!",
    "description": "The {dplyr} is one of the most useful R packages outthere. For me R is {dplyr} and {tidyverse}. So, here we'll use the most frequently used command.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-02-05",
    "categories": [
      "videos",
      "statistics",
      "data wrangling"
    ],
    "contents": "\n\nContents\nThis post as a video\nWhy do we need it? It‚Äôs effective!\n1. glimpse\n2. view(table2)\n3. arrange\n4. select\n5. filter\n6. %>% - pipe operator\n7. mutate\n8. rename\n9. summarise\n10. group_by\n\nConclusion\nWhat‚Äôs next?\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs only 8 minutes long.\n\n\n\n\n\n\n\nWhy do we need it? It‚Äôs effective!\nWith just 10 commands at your fingertips, you can select, rename and create new columns (mutate), arrange or filter out rows, group rows together and then summarize those groups and be able to combine all these data wrangling techniques in a single pipeline like never before. And the best part? These commands are very easy to master, as most of them are common English verbs like ‚Äúselect‚Äù or ‚Äúsummarise‚Äù. These verbs will allow you to solve the vast majority of your data manipulation challenges. So, let‚Äôs get into it.\nWe‚Äôll start with loading the tidyverse package. First, because dplyr package is part of it and secondly, tidyverse contains ready to use datasets. So, let‚Äôs discover the art of data manipulation with a quick glimpse on the first command, which name is in fact ‚Äúglimpse‚Äù.\n\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n\n1. glimpse\n\n\nglimpse(table2)\n\nRows: 12\nColumns: 4\n$ country <chr> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghan‚Ä¶\n$ year    <int> 1999, 1999, 2000, 2000, 1999, 1999, 2000, 2000, 1999‚Ä¶\n$ type    <chr> \"cases\", \"population\", \"cases\", \"population\", \"cases‚Ä¶\n$ count   <int> 745, 19987071, 2666, 20595360, 37737, 172006362, 804‚Ä¶\n\nglimpse gives you a quick look at your data. It shows all variables, their first values, and their types, whether integer or ordered categorical. It also reveals the number of observations (rows) and variables (columns). The beauty of glimpse is that it doesn‚Äôt overload your screen if your data is large. It presents only the crucial information. To view the whole dataset, you can actually use the view command.\n2. view(table2)\nOne useful feature of such table view is the ability to arrange values by clicking on the header‚Äôs variable name.\n3. arrange\nBy the way, you can also arrange values while working with it. For example, ‚Äútable2‚Äù is arranged alphabetically by countries. However, the whole table can be easily rearranged using one or several columns.\n\n\ntable2\n\n# A tibble: 12 √ó 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable2 %>% \n  arrange(year)\n\n# A tibble: 12 √ó 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Brazil       1999 cases           37737\n 4 Brazil       1999 population  172006362\n 5 China        1999 cases          212258\n 6 China        1999 population 1272915272\n 7 Afghanistan  2000 cases            2666\n 8 Afghanistan  2000 population   20595360\n 9 Brazil       2000 cases           80488\n10 Brazil       2000 population  174504898\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\ntable2 %>% \n  arrange(year, type)\n\n# A tibble: 12 √ó 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Brazil       1999 cases           37737\n 3 China        1999 cases          212258\n 4 Afghanistan  1999 population   19987071\n 5 Brazil       1999 population  172006362\n 6 China        1999 population 1272915272\n 7 Afghanistan  2000 cases            2666\n 8 Brazil       2000 cases           80488\n 9 China        2000 cases          213766\n10 Afghanistan  2000 population   20595360\n11 Brazil       2000 population  174504898\n12 China        2000 population 1280428583\n\n‚ÄúOh God, what is this strange %>% symbol?!,‚Äù you might ask. Well, it‚Äôs a highly useful one! But I‚Äôd prefer to demonstrate its use before explaining it. Until then, let‚Äôs select the most useful columns using the‚Ä¶ drumroll‚Ä¶ select command.\n4. select\n\n\ntable1 %>% \n  select(country, population)\n\n# A tibble: 6 √ó 2\n  country     population\n  <chr>            <int>\n1 Afghanistan   19987071\n2 Afghanistan   20595360\n3 Brazil       172006362\n4 Brazil       174504898\n5 China       1272915272\n6 China       1280428583\n\nUseless columns will be automatically discarded. But what if you‚Äôve got 300 (picture of the movie 300 ;) useful columns and just want to eliminate a couple? Rather than typing out all 300 names in the select command, which can be cumbersome, prone to typing mistakes, and hard on your fingertips ;) , you can simply add a minus sign in front of the unwanted column names to deselect them.\n\n\ntable1 %>% \n  select(-cases, -year)\n\n# A tibble: 6 √ó 2\n  country     population\n  <chr>            <int>\n1 Afghanistan   19987071\n2 Afghanistan   20595360\n3 Brazil       172006362\n4 Brazil       174504898\n5 China       1272915272\n6 China       1280428583\n\nSelecting columns is great. But what if we need only specific rows, such as only those from the year 2000?\n5. filter\nTo achieve this, we can use the filter command with a double equal sign. Keep in mind that the double equal sign means ‚Äúequal to‚Äù, while the single equal sign assigns a value to a name and won‚Äôt allow for filtering. The arguments here determine the conditions that must be true to keep the row, for example we want to see only rows with the year 2000.\n\n\ntable1 %>% \n  filter(year == 2000)\n\n# A tibble: 3 √ó 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  2000   2666   20595360\n2 Brazil       2000  80488  174504898\n3 China        2000 213766 1280428583\n\nHowever, if you want to eliminate any undesired observations, for example to see everything except the year 2000, the != (exclamation mark equals) operator is your friend. It stands for ‚Äúnot equal to.‚Äù Cool, write?\n\n\ntable1 %>% \n  filter(year != 2000)\n\n# A tibble: 3 √ó 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Brazil       1999  37737  172006362\n3 China        1999 212258 1272915272\n\nBut you know what‚Äôs even cooler? We can do both select and filter at the same time by using this funky symbol %>%, called the pipe operator.\n\n\ntable1 %>% select(year, population) %>% filter(year == 2000)\n\n# A tibble: 3 √ó 2\n   year population\n  <int>      <int>\n1  2000   20595360\n2  2000  174504898\n3  2000 1280428583\n\n6. %>% - pipe operator\nThe pipe operator, for me, is just another way of saying then. It allows you to transform your data in a step-by-step manner, by doing one thing first, then another, and so on. For example, this code takes the table1 dataset, then selects two columns, then filters only the year 2000. The operator is called the pipe operator because it pipes the values on the left side into the transformed values on the right side. This replaces the less intuitive f(x) with the much more readable x %>% f. The left-to-right nature of x %>% f is easier to write and understand, as it follows the natural way we read, while f(x) reads in a counterintuitive inside-out manner, making it harder for people to grasp, except for programmers (funny picture of a programming nerd). Thus, improving the readability and understanding of code is a great advantage of this operator. Additionally, you can chain multiple simple steps to achieve a complex result easily and making the process faster (we‚Äôll see it in a moment). I have to admit, it didn‚Äôt like the pipe in the beginning, but now I absolutely love it! ü•∞\nThe good thing is, the pipe will quickly become invisible, and your fingers will type it without your brain having to think about it. And the best part? There‚Äôs even a keyboard shortcut for it - Shift-Cmd/Ctrl-M! Give it a try.\nSo far, so good. Removing some useless columns or rows is useful. But what if we want to create new columns? Well, we do it in a similar way evolution does, through mutations.\n7. mutate\nThe mutate command literally means - creating a new column. We can either fill the new column with whatever we want or use existing (parent) columns to calculate new (offspring) ones.\n\n\ntable1 %>% \n  mutate(fill_it = \"with a smile ;)\") %>% \n  mutate(popul_per_case   = population / cases)\n\n# A tibble: 6 √ó 6\n  country      year  cases population fill_it         popul_per_case\n  <chr>       <int>  <int>      <int> <chr>                    <dbl>\n1 Afghanistan  1999    745   19987071 with a smile ;)         26828.\n2 Afghanistan  2000   2666   20595360 with a smile ;)          7725.\n3 Brazil       1999  37737  172006362 with a smile ;)          4558.\n4 Brazil       2000  80488  174504898 with a smile ;)          2168.\n5 China        1999 212258 1272915272 with a smile ;)          5997.\n6 China        2000 213766 1280428583 with a smile ;)          5990.\n\nNot only can you give your column any name you desire while creating it, but you can also change its name whenever you want using the ‚Äòrename‚Äô command.\n8. rename\n\n\ntable1 %>% \n  mutate(fill_it = \"with a smile ;)\") %>%\n  rename(much_better_name = fill_it)\n\n# A tibble: 6 √ó 5\n  country      year  cases population much_better_name\n  <chr>       <int>  <int>      <int> <chr>           \n1 Afghanistan  1999    745   19987071 with a smile ;) \n2 Afghanistan  2000   2666   20595360 with a smile ;) \n3 Brazil       1999  37737  172006362 with a smile ;) \n4 Brazil       2000  80488  174504898 with a smile ;) \n5 China        1999 212258 1272915272 with a smile ;) \n6 China        2000 213766 1280428583 with a smile ;) \n\nWith the eight commands we just learned, you‚Äôll have your ‚Äúperfectly‚Äù transformed dataset. But having data alone isn‚Äôt much use, especially if there‚Äôs a lot of it. So, it‚Äôs common to want to summarize the data in some way - like counting, finding the sum or average.\n9. summarise\nTo do this, you simply choose a column to summarize and a function to apply to it, like ‚Äòmean‚Äô.\n\n\ntable1 %>% \n  summarise(average = mean(population))\n\n# A tibble: 1 √ó 1\n     average\n       <dbl>\n1 490072924.\n\nWith dplyr package, you can take things a step further and summarize different columns in different ways. For example, use sum() to get an overall summary of all values in the column and n() to count the number of rows.\n\n\ntable1 %>% \n  summarise(avr_popul = mean(population),\n            all_cases = sum(cases),\n            how_many  = n())\n\n# A tibble: 1 √ó 3\n   avr_popul all_cases how_many\n       <dbl>     <int>    <int>\n1 490072924.    547660        6\n\nNow it‚Äôs getting exciting, because you can generate new numbers (making descriptive statistics) and get a comprehensive view of your dataset using plain English as computer code. However, a single-row overview of our data is not very useful since we have data from two different years and three countries. It would be much more helpful to know the averages (or any other summary statistics) for each year or each country. And that‚Äôs where the final, and perhaps the most useful, group_by command comes into play‚Ä¶ Hallelujah!\n10. group_by\n\n\ntable1 %>% \n  group_by(year) %>% \n  summarise(avr_popul = mean(population),\n            all_cases = sum(cases),\n            how_many  = n())\n\n# A tibble: 2 √ó 4\n   year  avr_popul all_cases how_many\n  <int>      <dbl>     <int>    <int>\n1  1999 488302902.    250740        3\n2  2000 491842947     296920        3\n\ntable1 %>% \n  group_by(country) %>% \n  summarise(avr_popul = mean(population),\n            all_cases = sum(cases),\n            how_many  = n())\n\n# A tibble: 3 √ó 4\n  country       avr_popul all_cases how_many\n  <chr>             <dbl>     <int>    <int>\n1 Afghanistan   20291216.      3411        2\n2 Brazil       173255630     118225        2\n3 China       1276671928.    426024        2\n\nAnd it gets even better - you can group_by multiple variables at once! Let‚Äôs take the mtcars dataset as an example and find out how many cars with 4, 6, or 8 cylinders (cyl) and different gearboxes (am) are in the dataset. And let‚Äôs get the average fuel efficiency (mpg) and average horsepower (hp) for each combination of cylinders and gearboxes. It‚Äôs like hitting two birds with one stone!\n\n\nmtcars %>% \n  group_by(cyl, am) %>% \n  summarise(avr_mpg = mean(mpg),\n            avr_hp  = mean(hp),\n            count   = n())\n\n# A tibble: 6 √ó 5\n# Groups:   cyl [3]\n    cyl    am avr_mpg avr_hp count\n  <dbl> <dbl>   <dbl>  <dbl> <int>\n1     4     0    22.9   84.7     3\n2     4     1    28.1   81.9     8\n3     6     0    19.1  115.      4\n4     6     1    20.6  132.      3\n5     8     0    15.0  194.     12\n6     8     1    15.4  300.      2\n\nSo, using group_by and summarize together is much more powerful than using them separately. And this goes for any function we‚Äôve learned today. The more functions you string together using the pipe-operator %>%, the more advanced your data manipulation becomes. For example, don‚Äôt look at this chunk of code as a massive block, but rather read one line at a time and you‚Äôll have no trouble understanding it. Once you grasp this piece of code, you‚Äôll be able to easily create your own code in no time.\n\n\nfancy_table = diamonds %>% \n  select(-x, -y, -z, -table) %>%\n  # \"c\" means concatenate == use several categories\n  filter(color == c(\"E\", \"I\"), clarity == \"SI2\") %>%  \n  mutate(price_carat = price / carat) %>% \n  rename(new_name = depth) %>% \n  group_by(cut, color, clarity) %>% \n  # remove NAs, otherwise you'll get NAs\n  summarise(avg_price = mean(price, na.rm = TRUE),\n            count     = n()) \n\nfancy_table\n\n# A tibble: 10 √ó 5\n# Groups:   cut, color [10]\n   cut       color clarity avg_price count\n   <ord>     <ord> <ord>       <dbl> <int>\n 1 Fair      E     SI2         4507.    41\n 2 Fair      I     SI2         6874.    26\n 3 Good      E     SI2         3761.    91\n 4 Good      I     SI2         6449.    41\n 5 Very Good E     SI2         4150.   222\n 6 Very Good I     SI2         6480.   113\n 7 Premium   E     SI2         4622.   286\n 8 Premium   I     SI2         6652.   156\n 9 Ideal     E     SI2         4010.   228\n10 Ideal     I     SI2         6301.   136\n\nYou can also name the resulting table and save it as an object for future use, by using the assignment operator, = or <-. It‚Äôs cool, right? But what if you need even more descriptive stats, like standard error, median, IQR, skewness, and so on, for ALL numeric variables in your dataset, and for ALL of your groups simultaneously? Sounds like a lot of work, right? But hold on! You can actually do it with a single intuitive word, describe, which delivers ALL the descriptive stats you can think of for ANY combination of groups and for EVERY variable in your dataset. And that‚Äôs just the tip of the iceberg of what the dplyr package can do. Want to know even more powerful functions? Check out my review of the dlookr package.\n\n\nmtcars %>% \n  group_by(cyl, am) %>% \n  dlookr::describe(-cyl, -am)\n\n\n\ndescribed_variablescylamnnameansdse_meanIQRskewnesskurtosisp00p01p05p10p20p25p30p40p50p60p70p75p80p90p95p99p100carb40301.6666670.577350270.333333330.50000-1.73205081.0001.020001.100001.20001.4001.500001.60001.8002.00002.0002.00002.00002.0002.00002.000002.000002.000carb41801.5000000.534522480.188982241.000000.0000000-2.79999999999999982236431.0001.000001.000001.00001.0001.000001.00001.0001.50002.0002.00002.00002.0002.00002.000002.000002.000carb60402.5000001.732050810.866025403.000000.0000000-6.00000000000000000000001.0001.000001.000001.00001.0001.000001.00001.6002.50003.4004.00004.00004.0004.00004.000004.000004.000carb61304.6666671.154700540.666666671.000001.73205084.0004.000004.000004.00004.0004.000004.00004.0004.00004.4004.80005.00005.2005.60005.800005.960006.000carb801203.0833330.900336640.259904802.00000-0.1847699-1.86545549829679413456062.0002.000002.000002.00002.0002.000002.30003.0003.00003.6004.00004.00004.0004.00004.000004.000004.000carb81206.0000002.828427122.000000002.000004.0004.040004.200004.40004.8005.000005.20005.6006.00006.4006.80007.00007.2007.60007.800007.960008.000disp4030135.86666713.969371268.0652202613.30000-1.3909912120.100120.51400122.17000124.2400128.380130.45000132.5200136.660140.8000141.980143.1600143.7500144.340145.5200146.11000146.58200146.700disp418093.61250020.476776927.2396339133.125000.4201412-1.842985456237834274162471.10071.4220072.7100074.320076.90077.9500078.730078.94087.050097.680106.7100111.0750115.380120.5100120.75500120.95100121.000disp6040204.55000044.7426344022.3713172065.650000.4492392-3.4650364202691479320606167.600167.60000167.60000167.6000167.600167.60000167.6000179.080196.3000213.520228.3000233.2500238.200248.1000253.05000257.01000258.000disp6130155.0000008.660254045.000000007.50000-1.7320508145.000145.30000146.50000148.0000151.000152.50000154.0000157.000160.0000160.000160.0000160.0000160.000160.0000160.00000160.00000160.000disp80120357.61666771.8234936720.73365670113.050000.3960937-1.2176693975842989292602275.800275.80000275.80000275.8000281.440296.95000308.2000330.800355.0000360.000388.0000410.0000432.000458.0000465.40000470.68000472.000disp8120326.00000035.3553390625.0000000025.00000301.000301.50000303.50000306.0000311.000313.50000316.0000321.000326.0000331.000336.0000338.5000341.000346.0000348.50000350.50000351.000drat40303.7700000.130000000.075055530.115001.72052803.6903.690203.691003.69203.6943.695003.69603.6983.70003.7443.78803.81003.8323.87603.898003.915603.920drat41804.1837500.364179610.128756930.250001.26163232.03474917336389848898653.7703.775603.798003.82603.9424.022504.08004.0804.09504.1324.20904.27254.3464.58004.755004.895004.930drat60403.4200000.591945940.295972970.92000-0.2468447-4.59218948729175657774702.7602.769602.808002.85602.9523.000003.04803.2483.50003.7523.92003.92003.9203.92003.920003.920003.920drat61303.8066670.161658080.093333330.14000-1.73205083.6203.625603.648003.67603.7323.760003.78803.8443.90003.9003.90003.90003.9003.90003.900003.900003.900drat801203.1208330.230274870.066474630.112501.53625444.73279738005022920788182.7602.778702.853502.93703.0143.052503.07003.0703.07503.1223.15003.16503.1983.22803.455003.675003.730drat81203.8800000.480832610.340000000.340003.5403.546803.574003.60803.6763.710003.74403.8123.88003.9484.01604.05004.0844.15204.186004.213204.220gear40303.6666670.577350270.333333330.50000-1.73205083.0003.020003.100003.20003.4003.500003.60003.8004.00004.0004.00004.00004.0004.00004.000004.000004.000gear41804.2500000.462910050.163663420.250001.44016460.00000000000000041448334.0004.000004.000004.00004.0004.000004.00004.0004.00004.0004.00004.25004.6005.00005.000005.000005.000gear60403.5000000.577350270.288675131.000000.0000000-6.00000000000000000000003.0003.000003.000003.00003.0003.000003.00003.2003.50003.8004.00004.00004.0004.00004.000004.000004.000gear61304.3333330.577350270.333333330.500001.73205084.0004.000004.000004.00004.0004.000004.00004.0004.00004.2004.40004.50004.6004.80004.900004.980005.000gear801203.0000000.000000000.000000000.000003.0003.000003.000003.00003.0003.000003.00003.0003.00003.0003.00003.00003.0003.00003.000003.000003.000gear81205.0000000.000000000.000000000.000005.0005.000005.000005.00005.0005.000005.00005.0005.00005.0005.00005.00005.0005.00005.000005.000005.000hp403084.66666719.6553639811.3480296917.50000-1.711897762.00062.6600065.3000068.600075.20078.5000081.800088.40095.000095.40095.800096.000096.20096.600096.8000096.9600097.000hp418081.87500022.655415638.0098990131.250000.2092817-1.637532481554309526927752.00052.9100056.5500061.100065.40065.7500066.000066.00078.500091.40092.800097.0000102.600110.2000111.60000112.72000113.000hp6040115.2500009.178779884.5893899414.25000-0.2505456-4.5713530981165630961982105.000105.15000105.75000106.5000108.000108.75000109.5000112.600116.5000120.400123.0000123.0000123.000123.0000123.00000123.00000123.000hp6130131.66666737.5277675021.6666666732.500001.7320508110.000110.00000110.00000110.0000110.000110.00000110.0000110.000110.0000123.000136.0000142.5000149.000162.0000168.50000173.70000175.000hp80120194.16666733.359837959.6301557143.750000.3646930-1.0807496521021451307831150.000150.00000150.00000152.5000175.000175.00000176.5000180.000180.0000195.000212.0000218.7500227.000243.5000245.00000245.00000245.000hp8120299.50000050.2045814635.5000000035.50000264.000264.71000267.55000271.1000278.200281.75000285.3000292.400299.5000306.600313.7000317.2500320.800327.9000331.45000334.29000335.000mpg403022.9000001.452583900.838649711.450000.308324621.50021.5260021.6300021.760022.02022.1500022.280022.54022.800023.12023.440023.600023.76024.080024.2400024.3680024.400mpg418028.0750004.483859941.585283895.70000-0.3167379-1.220453032988328301300421.40021.4980021.8900022.380024.08025.2000026.130027.04028.850030.40030.400030.900031.60032.850033.3750033.7950033.900mpg604019.1250001.631716890.815858441.725001.28468621.064145121117945347322117.80017.8090017.8450017.890017.98018.0250018.070018.32018.650018.98019.420019.750020.08020.740021.0700021.3340021.400mpg613020.5666670.750555350.433333330.65000-1.732050819.70019.7260019.8300019.960020.22020.3500020.480020.74021.000021.00021.000021.000021.00021.000021.0000021.0000021.000mpg8012015.0500002.774395920.800899122.57500-0.3722170-0.182580828804195988146510.40010.4000010.4000010.690013.50014.0500014.420014.90015.200015.38016.130016.625017.12018.560018.9250019.1450019.200mpg812015.4000000.565685420.400000000.4000015.00015.0080015.0400015.080015.16015.2000015.240015.32015.400015.48015.560015.600015.64015.720015.7600015.7920015.800qsec403020.9700001.671436510.965004321.450001.731981120.00020.0002020.0010020.002020.00420.0050020.006020.00820.010020.58821.166021.455021.74422.322022.6110022.8422022.900qsec418018.4500001.125255530.397837910.92750-0.6516514-0.474158092295855537745616.70016.7140016.7700016.840017.54818.1150018.528018.58418.605018.66818.871019.042519.24219.599019.7495019.8699019.900qsec604019.2150000.815904410.407952200.885000.2812606-0.460928914749751061918918.30018.3180018.3900018.480018.66018.7500018.840019.00819.170019.33219.518019.635019.75219.986020.1030020.1966020.220qsec613016.3266670.768721880.443821790.76000-0.757035115.50015.5192015.5960015.692015.88415.9800016.076016.26816.460016.57216.684016.740016.79616.908016.9640017.0088017.020qsec8012017.1425000.801647450.231415690.67250-1.22093370.999883294755610663173715.41015.4573015.6465015.943016.90016.9825017.029017.15017.350017.41217.546017.655017.77617.964017.9890017.9978018.000qsec812014.5500000.070710680.050000000.0500014.50014.5010014.5050014.510014.52014.5250014.530014.54014.550014.56014.570014.575014.58014.590014.5950014.5990014.600vs40301.0000000.000000000.000000000.000001.0001.000001.000001.00001.0001.000001.00001.0001.00001.0001.00001.00001.0001.00001.000001.000001.000vs41800.8750000.353553390.125000000.00000-2.82842718.00000000000000177635680.0000.070000.350000.70001.0001.000001.00001.0001.00001.0001.00001.00001.0001.00001.000001.000001.000vs60401.0000000.000000000.000000000.000001.0001.000001.000001.00001.0001.000001.00001.0001.00001.0001.00001.00001.0001.00001.000001.000001.000vs61300.0000000.000000000.000000000.000000.0000.000000.000000.00000.0000.000000.00000.0000.00000.0000.00000.00000.0000.00000.000000.000000.000vs801200.0000000.000000000.000000000.000000.0000.000000.000000.00000.0000.000000.00000.0000.00000.0000.00000.00000.0000.00000.000000.000000.000vs81200.0000000.000000000.000000000.000000.0000.000000.000000.00000.0000.000000.00000.0000.00000.0000.00000.00000.0000.00000.000000.000000.000wt40302.9350000.407523010.235283520.36250-1.71329692.4652.478702.533502.60202.7392.807502.87603.0133.15003.1583.16603.17003.1743.18203.186003.189203.190wt41802.0422500.409348520.144726560.450000.53164640.18765104229876658492591.5131.520141.548701.58441.7031.780001.84501.9152.03752.1522.19402.23002.2722.45802.619002.747802.780wt60403.3887500.116216390.058108200.06125-1.95985463.87665744238499732077233.2153.221753.248753.28253.3503.383753.41753.4403.44003.4403.44203.44503.4483.45403.457003.459403.460wt61302.7550000.128160060.073993240.12750-0.51947032.6202.623002.635002.65002.6802.695002.71002.7402.77002.7912.81202.82252.8332.85402.864502.872902.875wt801204.1040830.768306930.221791110.807501.1182347-0.52259349604471616590473.4353.435553.437753.44803.5303.557503.61803.7503.81003.8434.00254.36505.0145.33555.380555.415315.424wt81203.3700000.282842710.200000000.200003.1703.174003.190003.21003.2503.270003.29003.3303.37003.4103.45003.47003.4903.53003.550003.566003.570\n\nConclusion\n80% of working with data is pre-processing, such as cleaning, transforming, and wrangling. Although this work is rarely seen in the final result, like a model or a scientific paper, it is often the most important part. The phrase ‚ÄúGarbage in, garbage out‚Äù emphasizes the importance of having clean and accurate data. Making pre-processing intuitive and enjoyable can prevent wrong conclusions, boredom, and pain, and make the world a better place.\nThese 10 verbs will cover most of your daily data manipulation needs, and make your code easy to read and understand for your colleagues and even for your future self. This simple, powerful, and clear data pre-processing will allow for the use of statistical and machine learning models, resulting in valuable inferences and predictions. In the era of big data, there are too many unused or chaotic data that goes to waste. Data is the most valuable resource in the 21st century, and it‚Äôs important to handle it sustainably. Let‚Äôs make it useful.\nWhat‚Äôs next?\nData Wrangling Vol. 2: ¬¥The big 6¬¥ and their babies\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2023-01-31-datawrangling1/dplyr_1_thumbnail.png",
    "last_modified": "2023-02-05T21:50:52+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-11-29-emmeans/",
    "title": "{emmeans} Game-Changing R-package Squeezes Hidden Knowledge out of Models!",
    "description": "{emmeans} is one of the most capable, but at the same time one of the most mysterious and therefore underrated R packages. Let's demistify {emmeans} and uncover it's power!",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-01-31",
    "categories": [
      "videos",
      "statistics",
      "models"
    ],
    "contents": "\n\nContents\nThis post as a video\nWhy do we need {emmeans}?\nUnderstand what ‚Äúemmeans‚Äù actually stands for\nSingle categorical predictor\nSummary of ‚Ä¶ summaries ;)\nSingle numeric predictor + altering the reference grid\nOne categorical + One numeric predictors\nTwo categorical predictors\nTwo numeric predictors\nWhat‚Äôs next? ‚Ä¶ Interactions!\nSome bonus content\n‚ÄúCohen‚Äôs d‚Äù Effect sizes\n\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 14 minutes long.\n\n\n\n\n\n\n\n\n\n\nWhy do we need {emmeans}?\nHave you ever wondered, why a ‚Äúsummary‚Äù function of an omnibus test like ANOVA, tells you that ‚Äúthe number of cylinders‚Äù significantly affects ‚Äúcar mileage‚Äù without explaining how?\nOr have you ever been puzzled why a model compares all categories of a categorical predictor to only the reference category, without comparing categories to each other?\nAnd what about those mysterious slopes with standard errors we get as model coefficients instead of the averages per category with 95% CIs that we actually want?\nMoreover, ‚Äúsummary‚Äù function doesn‚Äôt adjust p-values for multiple comparisons, which increases the probability of discovering nonsense by making too many type-I errors.\n\nSummary function doesn‚Äôt plot the results of a model and\nmakes it almost impossible to interpret interactions!\nSo, if you‚Äôve ever been frustrated due to similar issues, you‚Äôre definitely not alone! The ‚Äúsummary‚Äù function doesn‚Äôt actually provide a very useful summary, and that‚Äôs why we need {emmeans} package, which solves all those problems.\n\n\nlibrary(tidyverse)       # for everything good in R\ntheme_set(theme_test())  # beautifies plots \nlibrary(emmeans)         # unleash power of your results!\n\nd <- mtcars %>% \n  mutate(cyl  = factor(cyl),\n         am   = factor(am),\n         gear = factor(gear))\n\n# omnibus test\naov(mpg ~ cyl, d) %>% summary()\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \ncyl          2  824.8   412.4    39.7 4.98e-09 ***\nResiduals   29  301.3    10.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# linear model\nlm(mpg ~ cyl, d) %>% summary()\n\n\nCall:\nlm(formula = mpg ~ cyl, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.2636 -1.8357  0.0286  1.3893  7.2364 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  26.6636     0.9718  27.437  < 2e-16 ***\ncyl6         -6.9208     1.5583  -4.441 0.000119 ***\ncyl8        -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.223 on 29 degrees of freedom\nMultiple R-squared:  0.7325,    Adjusted R-squared:  0.714 \nF-statistic:  39.7 on 2 and 29 DF,  p-value: 4.979e-09\n\n# lm(mpg ~ cyl, d) %>% summary() %>% plot()\n\n\nEven the simplest ‚Äúemmeans‚Äù application provides averages with CIs, compares all categories to each other pair-wisely, adjusts p-values for multiple comparisons and can be easily plotted. And that‚Äôs just a beginning, because {emmeans} can do soo much more! But before we unleash the full power of {emmeans} package, we need to ‚Ä¶\n\n\n# emmeans helps ;)\nlm(mpg ~ cyl, d) %>% emmeans(pairwise ~ cyl) \n\n$emmeans\n cyl emmean    SE df lower.CL upper.CL\n 4     26.7 0.972 29     24.7     28.7\n 6     19.7 1.218 29     17.3     22.2\n 8     15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0003\n cyl4 - cyl8    11.56 1.30 29   8.905  <.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\nlm(mpg ~ cyl, d) %>% emmeans(pairwise ~ cyl) %>% plot()\n\n\n\nUnderstand what ‚Äúemmeans‚Äù actually stands for\nThe ‚Äúemmeans‚Äù is an abbreviation for Estimated Marginal MEANS (EMMs).\n‚Äúestimated‚Äù is part of the name, since results are estimated (or predicted) only from models, not from data\nthe ‚Äúmeans‚Äù is part of of the name, because the averages themselves are often estimated. However, the term ‚Äúmeans‚Äù is just a generalization, because for a median-based regression {emmeans} would estimate the marginal medians, while for a logistic regression {emmeans} calculates marginal probabilities. Cool, right?\n\n\nlibrary(quantreg)\nrq(mpg ~ cyl, d, tau = .5) %>% emmeans(~ cyl)\n\n cyl emmean    SE df lower.CL upper.CL\n 4     26.0 1.872 29     22.2     29.8\n 6     19.7 0.895 29     17.9     21.5\n 8     15.2 0.873 29     13.4     17.0\n\nConfidence level used: 0.95 \n\nglm(am ~ cyl, d, family = binomial) %>% \n  emmeans(~ cyl, type = \"response\")\n\n cyl  prob     SE  df asymp.LCL asymp.UCL\n 4   0.727 0.1343 Inf     0.414     0.910\n 6   0.429 0.1870 Inf     0.144     0.770\n 8   0.143 0.0935 Inf     0.036     0.427\n\nConfidence level used: 0.95 \nIntervals are back-transformed from the logit scale \n\nso, the last part of the name is called ‚Äúmarginal‚Äù, which describes a group of values that we want to compute an average for. For a categorical predictor, each category is a ‚Äúmargin‚Äù. And for a numeric predictor, an average of that predictor is the ‚Äúmargin‚Äù. For example, a marginal mean of ‚Äúmiles per gallon‚Äù for 4 cylinders is 26.7, and for 8 cylinders only 15.1, while the average ‚Äúmpg‚Äù will be estimated for an average horsepower of 147.\n\n\nm <- lm(mpg ~ gear + am + hp, d)\nref_grid(m)\n\n'emmGrid' object with variables:\n    gear = 3, 4, 5\n    am = 0, 1\n    hp = 146.69\n\nmean(d$hp)\n\n[1] 146.6875\n\nAll margins of a model are combined in a REFERENCE GRID, which is the FOUNDATION for EMMs, because we can estimate the mean at each point in the reference grid, or even define new points of the reference grid we want to estimate the means for. But let‚Äôs take it one step at the time, starting with a ‚Ä¶\nSingle categorical predictor\n\n\nm <- lm(mpg ~ cyl, d)\nref_grid(m)\n\n'emmGrid' object with variables:\n    cyl = 4, 6, 8\n\nemmeans(m, pairwise ~ cyl)\n\n$emmeans\n cyl emmean    SE df lower.CL upper.CL\n 4     26.7 0.972 29     24.7     28.7\n 6     19.7 1.218 29     17.3     22.2\n 8     15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0003\n cyl4 - cyl8    11.56 1.30 29   8.905  <.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0112\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\nThe reference grid of this model shows 3 points we can estimate mean ‚Äúmpg‚Äù for, namely cylinders 4, 6 and 8. The ‚Äúemmeans‚Äù function displays those means with their 95% CIs, compares the mileage of cylinders among each other pair-wisely and adjusts p-values for multiple comparison with a Tukey method by default.\nThe default method of adjustment can easily be changed thought, which is very useful for several cases. First of all, people might want to use a famous Bonferroni method. However, Bonferroni correction is quite conservative and produces higher p-values as compared to Tukey, which is dangerous, because it increases chances to make a type II error - namely missing a discovery.\n\nThus, Bonferroni correction is more useful when you have a lot of data. But if you only have a few observations or conduct an exploratory pilot study, you can even stop correcting for multiple comparisons by using, adjust = ‚Äúnone‚Äù, argument. I personally prefer Benjamini & Hochberg (1995) method (‚ÄúBH‚Äù or its alias ‚Äúfdr‚Äù) to control the false discovery rate.\n\n\nemmeans(m, pairwise ~ cyl, adjust = \"bonferroni\")$contrasts\n\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0004\n cyl4 - cyl8    11.56 1.30 29   8.905  <.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0125\n\nP value adjustment: bonferroni method for 3 tests \n\nemmeans(m, pairwise ~ cyl, adjust = \"none\")$contrasts\n\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0001\n cyl4 - cyl8    11.56 1.30 29   8.905  <.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0042\n\nemmeans(m, pairwise ~ cyl, adjust = \"BH\") # = ‚Äúfdr‚Äù\n\n$emmeans\n cyl emmean    SE df lower.CL upper.CL\n 4     26.7 0.972 29     24.7     28.7\n 6     19.7 1.218 29     17.3     22.2\n 8     15.1 0.861 29     13.3     16.9\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29   4.441  0.0002\n cyl4 - cyl8    11.56 1.30 29   8.905  <.0001\n cyl6 - cyl8     4.64 1.49 29   3.112  0.0042\n\nP value adjustment: BH method for 3 tests \n\nSummary of ‚Ä¶ summaries ;)\nInterestingly, the ‚Äúemmeans‚Äù function shows us 95% CIs of the means, but not of the estimates of contrasts, which are differences between cylinders. On the other hand, the contrasts have p-values which test the null hypothesis that the difference is literally zero, while the ‚Äúemmeans‚Äù section does not test any hypothesis and therefore has no p-values.\nWe could greatly enhance the output of ‚Äúemmeans‚Äù by using the ‚Äúinfer = TRUE‚Äù argument. This produces 95% CIs for the differences among cylinders, which is more useful than the standard error (SE). It also tests the null hypothesis that the means are actually zero. However, testing ‚Äúemmeans‚Äù against zero is not particularly useful ‚Äì which is why p-values are not usually shown. But it makes a lot more sense to test them against some target mileage, let‚Äôs say 14. For that, we can use the ‚Äúnull‚Äù argument to set the null hypothesis to 14. This makes cylinder 8, with a mean mileage of 15.1, no longer significantly different from our new null hypothesis. The ‚Äúpairwise‚Äù argument is removed, because otherwise the contrasts would also have been tested against 14.\n\n\nemmeans(m, pairwise ~ cyl, infer = T)\n\n$emmeans\n cyl emmean    SE df lower.CL upper.CL t.ratio p.value\n 4     26.7 0.972 29     24.7     28.7  27.437  <.0001\n 6     19.7 1.218 29     17.3     22.2  16.206  <.0001\n 8     15.1 0.861 29     13.3     16.9  17.529  <.0001\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate   SE df lower.CL upper.CL t.ratio p.value\n cyl4 - cyl6     6.92 1.56 29    3.072    10.77   4.441  0.0003\n cyl4 - cyl8    11.56 1.30 29    8.356    14.77   8.905  <.0001\n cyl6 - cyl8     4.64 1.49 29    0.958     8.33   3.112  0.0112\n\nConfidence level used: 0.95 \nConf-level adjustment: tukey method for comparing a family of 3 estimates \nP value adjustment: tukey method for comparing a family of 3 estimates \n\nemmeans(m, ~ cyl, infer = T, null = 14, adjust = \"mvt\")\n\n cyl emmean    SE df lower.CL upper.CL null t.ratio p.value\n 4     26.7 0.972 29     24.2     29.1   14  13.031  <.0001\n 6     19.7 1.218 29     16.7     22.8   14   4.714  0.0002\n 8     15.1 0.861 29     12.9     17.3   14   1.277  0.5018\n\nConfidence level used: 0.95 \nConf-level adjustment: mvt method for 3 estimates \nP value adjustment: mvt method for 3 tests \n\nAs you can see, the ‚Äúemmeans‚Äù function does a great job of summarizing all the important results, while the ‚Äúsummary‚Äù function misses most of them. We can even go in the opposite direction and summarize the ‚Äúemmeans‚Äù results themselves to include only the most essential information. For that we‚Äôll use the Pairwise P-value matrix (‚Äúpwpm‚Äù) function, which presents results from ‚Äúemmeans‚Äù and pairwise comparisons thereof in a most compact way, where:\nthe upper triangle displays the ‚ÄúTukey‚Äù adjusted P values,\nthe diagonal shows the Estimates (EMMs), and\nthe lower triangle compares the estimates between levels.\nIn this way, rarely used information such as ‚Äúdf‚Äù or ‚Äút.ratios‚Äù is left out.\n\n\nemmeans(m, ~ cyl) %>% pwpm()\n\n       4      6      8\n4 [26.7] 0.0003 <.0001\n6   6.92 [19.7] 0.0112\n8  11.56   4.64 [15.1]\n\nRow and column labels: cyl\nUpper triangle: P values   adjust = \"tukey\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later\n\nAnother advantage of ‚Äúemmeans‚Äù over ‚Äúsummary‚Äù function is that we can easily plot our estimates with their 95% confidence intervals by using a ‚Äúplot‚Äù command. (The 95% confidence intervals are the default, but you can change them with ‚Äúlevel‚Äù argument, if you want to.) But the advantages of using ‚Äúemmeans‚Äù over ‚Äúsummary‚Äù doesn‚Äôt stop there. The benefits are even more pronounced if we analyze numeric predictors or covariates.\n\n\nemmeans(m, pairwise ~ cyl)$emmeans %>% plot()\n\n\nemmeans(m, pairwise ~ cyl, level = 0.5)$emmeans %>% plot()\n\n\n\nSingle numeric predictor + altering the reference grid\nNamely, instead of just looking at the average 20 miles per gallon for a boring average car (as represented by the mean ‚Äúhorsepower‚Äù of 147), we can estimate ‚Äúmileages per gallon‚Äù for weak (imagine a baby car) and muscle cars (imagine a muscle car and a engine noise)? To do this, we‚Äôll reduce our covariate to only the range of ‚Äúhorsepower‚Äù, from 52 to 335, which gives us a much more informative and less boring look at fuel efficiency. Particularly, weak cars drive further and strong cars drive much less than the average 20 miles per gallon.\n\n\nm <- lm(mpg ~ hp, d)\nemmeans(m, ~ hp)\n\n  hp emmean    SE df lower.CL upper.CL\n 147   20.1 0.683 30     18.7     21.5\n\nConfidence level used: 0.95 \n\nemmeans(m, ~ hp, cov.reduce = range)\n\n  hp emmean   SE df lower.CL upper.CL\n  52  26.55 1.18 30    24.15     29.0\n 335   7.24 2.02 30     3.11     11.4\n\nConfidence level used: 0.95 \n\nMoreover, we can specify any particular values of a numeric predictor, which might be useful when we have a non-linear relationship, but still need to figure out the efficiency of cars at different values of horsepower.\n\n\nm1 <- lm(mpg ~ poly(hp, 2), d) \n\nlibrary(sjPlot) # I made a video on this üì¶\nplot_model(m1, type = \"pred\", show.data = T)\n\n$hp\n\n\nLet‚Äôs use 100, 200, and 300 ‚Äúhorsepowers‚Äù and not only plot the estimates, but also compare them statistically by using the ‚Äúcomparisons = TRUE‚Äù argument. The blue bars represent the 95% confidence intervals for the EMMs, and the red arrows show statistical comparisons among them. If an arrow from one mean overlaps with an arrow from another mean, the difference is not significant.\n\n\nemmeans(m1, ~ hp, at = list(hp = c(100, 200, 300))) %>% \n  plot(comparisons = TRUE)\n\n\n\nSpeaking of significance: if we want to see the exact p-values for pairwise comparisons among the different ‚Äúhorsepowers,‚Äù we again can add the ‚Äúpairwise‚Äù argument in front of the tilde (~) and see that after crossing the 200 horsepower threshold, the mileage doesn‚Äôt significant change anymore, but stays at around 14.5 miles.\n\n\nemmeans(m1, pairwise ~ hp, at = list(hp = c(100, 200, 300)))\n\n$emmeans\n  hp emmean    SE df lower.CL upper.CL\n 100   23.3 0.662 29     21.9     24.6\n 200   14.6 0.820 29     12.9     16.3\n 300   14.3 1.735 29     10.7     17.8\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast      estimate    SE df t.ratio p.value\n hp100 - hp200     8.71 0.919 29   9.477  <.0001\n hp100 - hp300     9.00 1.945 29   4.626  0.0002\n hp200 - hp300     0.29 1.728 29   0.168  0.9846\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\nNow, having learned what ‚Äúemmeans‚Äù does with one categorical and one numeric predictors, let‚Äôs figure out what happens if we have both in a multiple model.\nOne categorical + One numeric predictors\nIn models that include covariates, EMMs are often referred to as adjusted means. For example, consider a scenario where we want to understand the effect of profession on salary. We might include age as a covariate in our model because salary certainly changes over the lifetime. By holding age constant at its average, we are controlling for the influence of age and are able to better understand the unique effect of profession on salary and make more accurate predictions.\nIf you wonder whether we really need any covariate at all, you can compare the Akaike‚Äôs Information Criterion (AIC) of a model with, to the model without this covariate. The lower AIC of the model with age indicates that the covariate improves the model and thus makes predictions of salary by profession indeed more realistic.\n\n\nlibrary(ISLR)\n\nset.seed(10)  # for reproducibility\nsalary <- Wage %>% \n    group_by(jobclass) %>% \n    sample_n(50)\n\nm_no_age <- lm(wage ~ jobclass, salary)\nm <- lm(wage ~ jobclass + age, salary)\nAIC(m, m_no_age)\n\n         df      AIC\nm         4 1014.413\nm_no_age  3 1020.342\n\nThe average age of 41.39 years shows that industrial workers earn slightly below 100K, while the IT crowd earns over 116K, which is already a first insight into our question. However, we can go one step further and ask ‚Äúemmeans‚Äù to provide salary estimates for different ages in order to see how strongly salary increases over a lifetime for different professions.\n\n\nref_grid(m)\n\n'emmGrid' object with variables:\n    jobclass = 1. Industrial, 2. Information\n    age = 41.39\n\nemmeans(m, ~ jobclass)\n\n jobclass       emmean   SE df lower.CL upper.CL\n 1. Industrial    99.9 5.33 97     89.4      111\n 2. Information  116.4 5.33 97    105.8      127\n\nConfidence level used: 0.95 \n\nFor instance, if people start working at 25 and finish working at 65 years old, we will see that IT professionals receive a salary of over 100K already at the beginning of their career at the age of 25, while factory workers only reach that 100K mark by the age of 42.\n\n\nemmeans(m, pairwise ~ age | jobclass, at = list(age = c(25, 42, 65)))$emmeans\n\njobclass = 1. Industrial:\n age emmean   SE df lower.CL upper.CL\n  25     85 7.33 97     70.4     99.5\n  42    100 5.34 97     89.9    111.1\n  65    121 9.49 97    102.6    140.3\n\njobclass = 2. Information:\n age emmean   SE df lower.CL upper.CL\n  25    101 7.67 97     86.2    116.6\n  42    117 5.32 97    106.3    127.5\n  65    138 9.09 97    119.8    155.9\n\nConfidence level used: 0.95 \n\nTwo categorical predictors\n\n\nm <- lm(mpg ~ am + cyl, d)\nref_grid(m)\n\n'emmGrid' object with variables:\n    am = 0, 1\n    cyl = 4, 6, 8\n\nemmeans(m, pairwise ~ cyl)\n\n$emmeans\n cyl emmean    SE df lower.CL upper.CL\n 4     26.1 0.972 28     24.1     28.1\n 6     19.9 1.165 28     17.5     22.3\n 8     16.0 0.943 28     14.1     17.9\n\nResults are averaged over the levels of: am \nConfidence level used: 0.95 \n\n$contrasts\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6     6.16 1.54 28   4.009  0.0012\n cyl4 - cyl8    10.07 1.45 28   6.933  <.0001\n cyl6 - cyl8     3.91 1.47 28   2.660  0.0331\n\nResults are averaged over the levels of: am \nP value adjustment: tukey method for comparing a family of 3 estimates \n\nThe model with two categorical predictors works a little differently. If we examine the estimated marginal means (EMMs) of cylinders while adjusting for automatic transmission, the ‚Äúemmeans‚Äù command indicates that the results are somehow averaged over the levels of the variable ‚Äúam‚Äù. I was initially unsure of what this meant, so I calculated the means of cylinders separately for transmission 0 and 1 using ‚Äúby‚Äù argument and averaged them to get the same EMMs we obtained initially.\n(24.8 + 27.4) / 2 = 26.1\n(18.6 + 21.2) / 2 = 19.9\n(14.7 + 17.3) / 2 = 16\nOh, by the way, if you want to display the sample size of every level, use calc = c(n = \".wgt.\") argument ;)\n\n\nemmeans(m, ~ cyl, by = \"am\", calc = c(n = \".wgt.\"))\n\nam = 0:\n cyl emmean    SE df  n lower.CL upper.CL\n 4     24.8 1.323 28  3     22.1     27.5\n 6     18.6 1.288 28  4     16.0     21.3\n 8     14.7 0.842 28 12     13.0     16.5\n\nam = 1:\n cyl emmean    SE df  n lower.CL upper.CL\n 4     27.4 0.992 28  8     25.3     29.4\n 6     21.2 1.378 28  3     18.4     24.0\n 8     17.3 1.383 28  2     14.5     20.1\n\nConfidence level used: 0.95 \n\nSo, in the case of a numeric covariate, ‚Äúemmeans‚Äù are estimated for the mean of the covariate. However, for a categorical covariate, ‚Äúemmeans‚Äù calculates the averages for each category and then takes the average of the estimated marginal means over the categories.\nBut what happens if we have two numeric predictors?\nTwo numeric predictors\nTo answer this question, let‚Äôs consider the horsepower and weight of cars in the same model. The reference grid tells us that the mileage will be calculated only for the averages of predictors, which is somewhat boring. However, using the ‚Äúcov.reduce‚Äù argument we learned about today, we can tell a more interesting story that is already part of the model but would have gone untold if we had only used the ‚Äúsummary‚Äù function. In particular, weak (52 horsepower) and light (1.52 weight) cars are the most efficient, as they can drive the most distance (nearly 30 miles per gallon of fuel). If a weak car is heavy (5.42 weight), it can only manage to drive 14.5 miles, which is less than a light but sporty muscle car (335 horsepower) with a mileage of 20 miles per gallon. The most inefficient cars, however, are strong and heavy, which can only manage 5.5 miles per gallon, so you‚Äôll never pass a gas station.\n\n\nm <- lm(mpg ~ hp + wt, d)\nref_grid(m)\n\n'emmGrid' object with variables:\n    hp = 146.69\n    wt = 3.2172\n\nemmeans(m, ~ wt | hp, cov.reduce = range)\n\nhp =  52:\n   wt emmean    SE df lower.CL upper.CL\n 1.51  29.71 0.943 29    27.78    31.64\n 5.42  14.54 2.113 29    10.22    18.86\n\nhp = 335:\n   wt emmean    SE df lower.CL upper.CL\n 1.51  20.72 2.585 29    15.43    26.00\n 5.42   5.55 1.387 29     2.71     8.39\n\nConfidence level used: 0.95 \n\nPlotting the results supports our conclusion that increasing the strength and weight of cars decreases their efficiency. The only thing we would need to be absolutely sure of our finding is the p-values. However, if we use the ‚Äúpairwise‚Äù argument in front of the tilde (~) to get contrasts between values, we oddly get identical results for different horsepower levels. This happens because in a multiple model without interactions we can only adjust the effect of one predictor, such as weight, while the other predictors are held constant and do not change or vary.\n\n\nplot_model(m, type = \"pred\", terms = c(\"wt\", \"hp [52, 335]\"))\n\n\nlm(mpg ~ hp + wt, d) %>% \n  emmeans(pairwise ~ wt | hp, cov.reduce = range)\n\n$emmeans\nhp =  52:\n   wt emmean    SE df lower.CL upper.CL\n 1.51  29.71 0.943 29    27.78    31.64\n 5.42  14.54 2.113 29    10.22    18.86\n\nhp = 335:\n   wt emmean    SE df lower.CL upper.CL\n 1.51  20.72 2.585 29    15.43    26.00\n 5.42   5.55 1.387 29     2.71     8.39\n\nConfidence level used: 0.95 \n\n$contrasts\nhp =  52:\n contrast          estimate   SE df t.ratio p.value\n wt1.513 - wt5.424     15.2 2.47 29   6.129  <.0001\n\nhp = 335:\n contrast          estimate   SE df t.ratio p.value\n wt1.513 - wt5.424     15.2 2.47 29   6.129  <.0001\n\nHowever, when we introduce interactions, things dramatically improve, because we can calculate EMMs for every level of one predictor within every level of the other predictor, and we can easily obtain contrasts between levels with unique p-values. This, however, is a completely different story, because analyzing interactions can be very challenging, but at the same time, it is extremely rewarding because it allows us to extract even more valuable knowledge from models. Thus, if you want to step up your data science game and unleash the real power of {emmeans}, you should definitely watch this video!\n\n\nlm(mpg ~ hp * wt, d) %>% \n  emmeans(pairwise ~ wt | hp, cov.reduce = range)\n\n$emmeans\nhp =  52:\n   wt emmean   SE df lower.CL upper.CL\n 1.51  33.32 1.24 28    30.78     35.9\n 5.42   6.85 2.70 28     1.33     12.4\n\nhp = 335:\n   wt emmean   SE df lower.CL upper.CL\n 1.51  11.26 3.31 28     4.48     18.0\n 5.42  15.61 2.92 28     9.63     21.6\n\nConfidence level used: 0.95 \n\n$contrasts\nhp =  52:\n contrast          estimate   SE df t.ratio p.value\n wt1.513 - wt5.424    26.47 3.65 28   7.261  <.0001\n\nhp = 335:\n contrast          estimate   SE df t.ratio p.value\n wt1.513 - wt5.424    -4.35 5.59 28  -0.778  0.4430\n\nWhat‚Äôs next? ‚Ä¶ Interactions!\n\n\n\nSome bonus content\n‚ÄúCohen‚Äôs d‚Äù Effect sizes\nyou must specify the ‚ÄúemmGrid‚Äù object with the means to be compared, the estimated population SD sigma of the model, and its degrees of freedom ‚Äúedf‚Äù:\n\n\neff_size(emmeans(m, ~ cyl), sigma = sigma(m), edf = 29)\n\n contrast    effect.size    SE df lower.CL upper.CL\n cyl4 - cyl6        1.86 0.566 26    0.698     3.02\n cyl4 - cyl8        3.38 0.677 26    1.993     4.78\n cyl6 - cyl8        1.52 0.576 26    0.340     2.71\n\nResults are averaged over the levels of: am \nsigma used for effect sizes: 3.032 \nConfidence level used: 0.95 \n\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-11-29-emmeans/thumbnail_emmeans_1.png",
    "last_modified": "2023-01-31T17:43:04+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-12-29-emmeans2interactions/",
    "title": "Don't Ignore Interactions - Unleash the Full Power of Models with {emmeans} R-package",
    "description": "Analysing interactions is both (1) very challenging, that's why it's rarely executed, and (2) very rewording if done well, that's why it's still sometimes attempted. {emmeans} is one of the few packages which demistify interactions and extract the most knowledge out of statistical models!",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2023-01-31",
    "categories": [
      "videos",
      "statistics",
      "models"
    ],
    "contents": "\n\nContents\nThis post as a video\nWhy do we need interactions?\nInteraction between two categorical predictors\nInteractions with covariates\nLinear relationship\nNon-Linear relationship\n\nTwo numeric predictors\nHigher order interactions\nMatrix results\nGraphical comparisons and plot p-values\nConclusion\nBonus (questionable ;) content\nEffect size: Cohen effect sizes are close cousins of pairwise differences\nOne interaction + more predictors (think twice before you do that)\nInteractions with covariates + other predictors (think twice before doing this)\nSeveral interactions (thing three times before doing this and then run away without doing this! or try to consult a VERY professional statistitian!)\n\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 12 minutes long.\n\n\n\n\n\n\n\n\n\nlibrary(ISLR)            # for Wage data\nlibrary(emmeans)         # unleash the power of your results!\nlibrary(tidyverse)       # for everything good in R\ntheme_set(theme_test())  # beautifies plots \n\nset.seed(1)              # for reproducibility\nsalary <- Wage %>% \n  mutate(age_cat = case_when(\n    age < 40 ~ \"1. young\",\n    TRUE     ~ \"2. old\"\n  )) %>% \n  group_by(education) %>% \n  sample_n(40)\n\n\nWhy do we need interactions?\nInteractions in a statistical model allow for a much deeper understanding of the relationships between variables. Interaction effects can be viewed as an ‚Äúit depends‚Äù effect.\nFor instance, if someone asks you, ‚ÄúDo you prefer to wear a raincoat or a sweater on a cold day?‚Äù Your response would probably be, ‚ÄúIt depends on whether it‚Äôs raining or not!‚Äù In this case, the cloths is one variable, while the precipitation is the other variable in the interaction term, and it affects your preference for wearing a raincoat or a sweater. This shows the ‚Äúit depends‚Äù nature of an interaction effect, where you can‚Äôt answer the question without considering the other variable in the interaction term.\nInteraction between two categorical predictors\nLet‚Äôs compare a model with, to a model without interaction to better understand the advantages of using interactions. Both models explore, whether we‚Äôll earn more money as we age and see how a profession choice affects our salary.\nAnd the model without interaction clearly states, that as we age, our income increases by the average of 15K, and when we choose the IT job over the Industrial job, we‚Äôll earn 21 thousand dollars more. So far so good.\n\n\nm  <- lm(wage ~ age_cat * jobclass, salary)\nm1 <- lm(wage ~ age_cat + jobclass, salary)\n\nlibrary(gtsummary) # I made a video on this üì¶\ntbl_regression(m1)\n\n\nCharacteristic\n      Beta\n      95% CI1\n      p-value\n    age_cat\n\n\n¬†¬†¬†¬†1. young\n‚Äî\n‚Äî\n¬†¬†¬†¬†2. old\n15\n1.8, 27\n0.025jobclass\n\n\n¬†¬†¬†¬†1. Industrial\n‚Äî\n‚Äî\n¬†¬†¬†¬†2. Information\n21\n8.9, 34\n1 CI = Confidence Interval\n    \n\nHowever, the model has a problem: salary increases with age regardless of profession, resulting in identical differences of 15k between young and old in both job classes. Similarly, IT professionals earn 21.3K more than factory workers at any age, young or old. This seems unrealistic! Right?\n\n\nemmeans(m1, pairwise ~ age_cat | jobclass)$contrasts\n\njobclass = 1. Industrial:\n contrast          estimate   SE  df t.ratio p.value\n 1. young - 2. old    -14.6 6.47 197  -2.255  0.0252\n\njobclass = 2. Information:\n contrast          estimate   SE  df t.ratio p.value\n 1. young - 2. old    -14.6 6.47 197  -2.255  0.0252\n\nemmeans(m1, pairwise ~ jobclass| age_cat)$contrasts\n\nage_cat = 1. young:\n contrast                       estimate  SE  df t.ratio p.value\n 1. Industrial - 2. Information    -21.3 6.3 197  -3.381  0.0009\n\nage_cat = 2. old:\n contrast                       estimate  SE  df t.ratio p.value\n 1. Industrial - 2. Information    -21.3 6.3 197  -3.381  0.0009\n\nThis occurs because models without interactions examine the influence of each predictor while holding all other predictors constant. While this IS fine, it does not offer a deep understanding of how our income depends on both (1) job type and (2) age at the same time. In contrast, a model with interaction looks at the effect of one predictor at each level of the other predictor, which provides many more insights.\n\n\na <- emmip(m1, age_cat ~ jobclass, CIs = TRUE)+\n  theme(legend.position = \"top\")+\n  ggtitle(\"NO INTERACTION\")+\n  ylab(\"SALARY [1000$]\")+ \n  labs(color = \"AGE\")+\n  scale_color_manual(values=c('blue','black'))\n\nb <- emmip(m, age_cat ~ jobclass, CIs = TRUE)+\n  theme(legend.position = \"top\")+\n  ggtitle(\"INTERACTION\")+\n  ylab(\"SALARY [1000$]\")+ \n  labs(color = \"AGE\")+\n  scale_color_manual(values=c('blue','black'))\n\nlibrary(patchwork)\na + b\n\n\n# save your fancy image ;)\nggsave(\"interaction.png\", plot = last_plot(), width = 5, height = 3)\n\n\nFor instance, for the young, a choice of profession is of great importance, as they can earn more in IT than in an industrial job. While as people age, the difference in income between the two types of jobs becomes less significant. Similarly, an Industrial jobs tend to have a noticeable increase in salary with age, while IT jobs do not. This may seem disappointing for IT crowd, but we must remember that IT folks earn a lot from the very beginning of their careers.\n\n\nemmeans(m, pairwise ~ jobclass | age_cat)$contrasts\n\nage_cat = 1. young:\n contrast                       estimate    SE  df t.ratio p.value\n 1. Industrial - 2. Information    -37.9 10.06 196  -3.771  0.0002\n\nage_cat = 2. old:\n contrast                       estimate    SE  df t.ratio p.value\n 1. Industrial - 2. Information    -10.9  7.97 196  -1.363  0.1745\n\nemmeans(m, pairwise ~ age_cat | jobclass)$contrasts\n\njobclass = 1. Industrial:\n contrast          estimate   SE  df t.ratio p.value\n 1. young - 2. old  -28.047 9.05 196  -3.099  0.0022\n\njobclass = 2. Information:\n contrast          estimate   SE  df t.ratio p.value\n 1. young - 2. old   -0.994 9.10 196  -0.109  0.9131\n\nIf you are still uncertain about the usefulness of interactions in models, consider this: which model would you trust to design your professional life? The one with interactions, or the one without? The choice is yours. The consequences, lasting. ‚Ä¶ No pressure ;)\n\nNOTE: Buy the way, if you stumbled upon this video and some words or code are unclear, I highly recommend watching the introductory video on {emmeans} first.\n\nFor now, let‚Äôs return to our model (very quick picture of a female model, like in fight club) and point out 3 important nuances.\nFirst, if we ask for EMMs in only one of predictors, let‚Äôs say age, we‚Äôll get a red warning that our results may be misleading due to involvement in interactions ‚Ä¶ which seems like a problem (disappointing sound), but it is actually not! The effect of age is now studied for different jobclasses, and the warning helps us to remember that. (supportive sound) Therefore, when interaction is involved, do not interpret the main effects separately.\n\n\nemmeans(m, pairwise ~ age_cat)\n\n$emmeans\n age_cat  emmean   SE  df lower.CL upper.CL\n 1. young    110 5.03 196     99.9      120\n 2. old      124 3.99 196    116.4      132\n\nResults are averaged over the levels of: jobclass \nConfidence level used: 0.95 \n\n$contrasts\n contrast          estimate   SE  df t.ratio p.value\n 1. young - 2. old    -14.5 6.42 196  -2.263  0.0247\n\nResults are averaged over the levels of: jobclass \n\nSecondly, the difference between the young and old is negative because the income of older workers was subtracted from that of younger workers. If this seems counterintuitive, we can simply reverse the order within the pairs to obtain a positive number.\n\n\nemmeans(m, ~ age_cat | jobclass) %>%\n          pairs(reverse = TRUE)\n\njobclass = 1. Industrial:\n contrast          estimate   SE  df t.ratio p.value\n 2. old - 1. young   28.047 9.05 196   3.099  0.0022\n\njobclass = 2. Information:\n contrast          estimate   SE  df t.ratio p.value\n 2. old - 1. young    0.994 9.10 196   0.109  0.9131\n\nAnd finally, did you realize that emmeans have 95% confidence intervals (CIs), while contrasts do not? Don‚Äôt worry, I have you covered ;) We can display them using the infer = TRUE argument.\n\n\nemmeans(m, pairwise ~ age_cat | jobclass, infer = TRUE)$contrasts\n\njobclass = 1. Industrial:\n contrast          estimate   SE  df lower.CL upper.CL t.ratio\n 1. young - 2. old  -28.047 9.05 196    -45.9    -10.2  -3.099\n p.value\n  0.0022\n\njobclass = 2. Information:\n contrast          estimate   SE  df lower.CL upper.CL t.ratio\n 1. young - 2. old   -0.994 9.10 196    -18.9     16.9  -0.109\n p.value\n  0.9131\n\nConfidence level used: 0.95 \n\nInteractions with covariates\nLinear relationship\n\n\nset.seed(1)              # for reproducibility\nsalary <- Wage %>% \n  group_by(education) %>% \n  sample_n(100)\n\nm <- lm(wage ~ health * age, salary)\n\nlibrary(sjPlot) # I made a video on this üì¶\nplot_model(m, type = \"pred\", terms = c(\"age\", \"health\"))\n\n\n\nAn interaction between a categorical predictor and a numeric predictor (covariate) is even more intriguing, as it produces even more results. Specifically, we can estimate and compare the slopes for salary changes over a lifetime for individuals with varying levels of health using the emtrends() function.\n\n\nemtrends(m, pairwise ~ health, var = \"age\", infer = T)\n\n$emtrends\n health         age.trend    SE  df lower.CL upper.CL t.ratio p.value\n 1. <=Good          0.448 0.332 496   -0.204     1.10   1.350  0.1775\n 2. >=Very Good     1.253 0.202 496    0.856     1.65   6.191  <.0001\n\nConfidence level used: 0.95 \n\n$contrasts\n contrast                   estimate    SE  df lower.CL upper.CL\n 1. <=Good - 2. >=Very Good   -0.805 0.389 496    -1.57  -0.0415\n t.ratio p.value\n  -2.071  0.0388\n\nConfidence level used: 0.95 \n\nHere, the salary of individuals with merely good health hardly changes over a lifetime, as indicated by the non-significant slope. On the other hand, the salary increase for those with excellent health is significant. The contrast between the two slopes also reveals a significant difference. To put it in statistical terms: good health has a significant marginal disadvantage in terms of income, amounting to 0.8 relative to the income of those with excellent health.\nHowever, the problem with this difference in slopes of 0.8 is that we still don‚Äôt know, how much more people on the blue line would earn. Moreover, the plot shows that this difference changes with age and can not be expressed as a single number. The cov.reduce = range argument solves this problem, by measuring the difference between blue and red lines at minimum and maximum ages. This reveals, that young people earn roughly the same amount regardless of their health, while older healthier individuals will earn over 53K per year more at the end of their career, which IS significant.\n\n\nemmeans(m, pairwise ~ health|age, cov.reduce = range)\n\n$emmeans\nage = 18:\n health         emmean    SE  df lower.CL upper.CL\n 1. <=Good        87.8  9.50 496     69.2      106\n 2. >=Very Good   91.3  5.25 496     81.0      102\n\nage = 80:\n health         emmean    SE  df lower.CL upper.CL\n 1. <=Good       115.6 12.29 496     91.4      140\n 2. >=Very Good  169.0  8.13 496    153.0      185\n\nConfidence level used: 0.95 \n\n$contrasts\nage = 18:\n contrast                   estimate   SE  df t.ratio p.value\n 1. <=Good - 2. >=Very Good    -3.49 10.9 496  -0.322  0.7477\n\nage = 80:\n contrast                   estimate   SE  df t.ratio p.value\n 1. <=Good - 2. >=Very Good   -53.41 14.7 496  -3.625  0.0003\n\nNon-Linear relationship\nWell, being healthy and rich at 80 might sound like a dream come true, but let‚Äôs face it, it‚Äôs probably not gonna happen. Most older folks are retired and can‚Äôt make that kind of money. So, to spice things up a bit, let‚Äôs make the numeric variable non-linear, since that‚Äôs often how it goes in the real world. But the problem with this more realistic approach is that non-linear relationships can‚Äôt be summed up as slopes. Fortunately, we can use specific values of age using the ‚Äúemmip‚Äù function.\n\n\nm <- lm(wage ~ health * poly(age, 2), salary)\n\nemmip(m, health ~ age, CIs = TRUE, \n      cov.reduce = FALSE, ylab = \"Salary  [1000 $]\")\n\n\n\nCheck this out! If we use the ‚Äúcov.reduce = FALSE‚Äù argument with ‚Äúemmip‚Äù function, we will be able to see all possible values of age. This gives us a complete overview of our data and enables us to choose fewer, more meaningful values of age to compare. The ‚Äúat = list()‚Äù argument allows us to specify exact ages. Let‚Äôs for example take 25, 45, and 65 years, representing the beginning, middle, and the end of a career.\n\n\nemmip(m, health ~ age, CIs = TRUE, \n      at = list(age = c(25, 45, 65)), \n      dodge = 5, ylab = \"Salary  [1000 $]\")\n\n\n\nAnd all of a sudden, it looks like we‚Äôre comparing two categorical variables, with age having 3 categories and health having 2. As a result, we can again create two different types of contrasts. The first is between specific values of the age covariate, at the beginning, middle, and end of a career, within each level of health. And the second type of contrast can be examined between health levels at any particular value of age.\n\n\n# get contrasts\nemmeans(m, pairwise ~ age|health,  at = list(age = c(25, 45, 65)))\n\n$emmeans\nhealth = 1. <=Good:\n age emmean    SE  df lower.CL upper.CL\n  25   78.5  9.92 494     59.0     98.0\n  45  105.1  4.47 494     96.3    113.8\n  65   95.8 10.34 494     75.5    116.1\n\nhealth = 2. >=Very Good:\n age emmean    SE  df lower.CL upper.CL\n  25   85.6  4.84 494     76.0     95.1\n  45  133.6  2.84 494    128.0    139.1\n  65  128.4  6.69 494    115.3    141.6\n\nConfidence level used: 0.95 \n\n$contrasts\nhealth = 1. <=Good:\n contrast      estimate    SE  df t.ratio p.value\n age25 - age45   -26.54 11.61 494  -2.287  0.0585\n age25 - age65   -17.31 12.93 494  -1.339  0.3743\n age45 - age65     9.24 11.89 494   0.777  0.7173\n\nhealth = 2. >=Very Good:\n contrast      estimate    SE  df t.ratio p.value\n age25 - age45   -48.02  5.99 494  -8.013  <.0001\n age25 - age65   -42.87  8.01 494  -5.351  <.0001\n age45 - age65     5.15  7.13 494   0.722  0.7507\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\nemmeans(m, pairwise ~ health|age,  at = list(age = c(25, 45, 65)))\n\n$emmeans\nage = 25:\n health         emmean    SE  df lower.CL upper.CL\n 1. <=Good        78.5  9.92 494     59.0     98.0\n 2. >=Very Good   85.6  4.84 494     76.0     95.1\n\nage = 45:\n health         emmean    SE  df lower.CL upper.CL\n 1. <=Good       105.1  4.47 494     96.3    113.8\n 2. >=Very Good  133.6  2.84 494    128.0    139.1\n\nage = 65:\n health         emmean    SE  df lower.CL upper.CL\n 1. <=Good        95.8 10.34 494     75.5    116.1\n 2. >=Very Good  128.4  6.69 494    115.3    141.6\n\nConfidence level used: 0.95 \n\n$contrasts\nage = 25:\n contrast                   estimate    SE  df t.ratio p.value\n 1. <=Good - 2. >=Very Good    -7.04 11.04 494  -0.638  0.5237\n\nage = 45:\n contrast                   estimate    SE  df t.ratio p.value\n 1. <=Good - 2. >=Very Good   -28.51  5.29 494  -5.385  <.0001\n\nage = 65:\n contrast                   estimate    SE  df t.ratio p.value\n 1. <=Good - 2. >=Very Good   -32.60 12.31 494  -2.648  0.0084\n\nTherefore, if someone asks you who earns more, very healthy or less healthy people, you would have to respond, ‚ÄúIt depends on their age. Young people earn the same regardless of their health, while older individuals with very good health definitely earn more, probably because they are more capable of delivering. You can‚Äôt certainly answer the question about salaries in different health levels without knowing the age, because‚ÄùIT DEPENDS‚Äù on age ‚Ä¶ and probably some other important things. So, since the relationships between things in real life always depend on something, models with interactions tend to be more realistic. Then how does an interaction between two numeric predictors work?\nTwo numeric predictors\nPretty easy actually! Having the option to select specific values of numeric predictors gives us a valuable tool for easily modeling and interpreting interactions between two numeric predictors. It‚Äôs crucial to choose only a few key values, as plotting all the numeric values of both covariates could be overwhelming and chaotic, although it does also provide a complete overview and reveals the story within our data.\nTake for instance, slow and weak cars - they may not be flashy but they can go the farthest on a single gallon of fuel, making them the most sustainable. On the other hand, slow and strong cars would go the least distance on a gallon of fuel, likely because of their weight. Similarly, quick and strong cars have lower mileage compared to quick and weak cars, as the latter type is not able to burn as much fuel and therefore can go further.\n\n\nm <- lm(mpg ~ poly(hp, 2) * poly(qsec, 2), mtcars)\n\nemmip(m, hp ~ qsec, cov.reduce = FALSE, \n      ylab = \"Miles per gallon\")\n\n\n\nHaving the full picture gives us an advantage as it allows us to choose the most meaningful values for both numeric variables. From there, we can treat those specific values as categories and easily obtain EMMs and the pairwise contrasts between them. I think it‚Äôs quite remarkable that we can interpret a model with two interacting numeric predictors that have a non-linear relationship with the response and with each other in such an understandable way. Let me know in the comments below what you think, and whether you know a better way to interpret similar models.\n\n\nemmip(m, hp ~ qsec, CIs = TRUE, dodge = 1,\n      at = list(hp   = c(50, 150, 250), \n                qsec = c(14, 17, 20)),\n      ylab = \"Miles per gallon\")\n\n\nemmeans(m, pairwise ~ qsec | hp,  \n        at = list(hp   = c(50, 150, 250), \n                  qsec = c(14, 17, 20)))\n\n$emmeans\nhp =  50:\n qsec emmean    SE df lower.CL upper.CL\n   14  38.53 10.16 23    17.51     59.6\n   17  30.87  3.47 23    23.69     38.0\n   20  33.69  2.84 23    27.81     39.6\n\nhp = 150:\n qsec emmean    SE df lower.CL upper.CL\n   14  22.62  4.40 23    13.52     31.7\n   17  19.11  1.03 23    16.98     21.2\n   20  11.77  3.35 23     4.84     18.7\n\nhp = 250:\n qsec emmean    SE df lower.CL upper.CL\n   14  15.30  5.21 23     4.53     26.1\n   17  12.46  2.51 23     7.27     17.7\n   20   2.08 17.66 23   -34.46     38.6\n\nConfidence level used: 0.95 \n\n$contrasts\nhp =  50:\n contrast        estimate    SE df t.ratio p.value\n qsec14 - qsec17     7.67  8.69 23   0.882  0.6566\n qsec14 - qsec20     4.85 10.49 23   0.462  0.8894\n qsec17 - qsec20    -2.82  5.10 23  -0.553  0.8460\n\nhp = 150:\n contrast        estimate    SE df t.ratio p.value\n qsec14 - qsec17     3.50  4.61 23   0.760  0.7307\n qsec14 - qsec20    10.85  4.41 23   2.461  0.0548\n qsec17 - qsec20     7.34  3.85 23   1.905  0.1599\n\nhp = 250:\n contrast        estimate    SE df t.ratio p.value\n qsec14 - qsec17     2.84  6.77 23   0.419  0.9082\n qsec14 - qsec20    13.22 14.46 23   0.914  0.6370\n qsec17 - qsec20    10.38 18.22 23   0.570  0.8373\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\nHigher order interactions\nTreating interactions as ‚Äúit depends‚Äù effects enables us to model higher-order interactions, like a three-way interaction which involves three variables in the term, for example age * jobclass * health, where the relationship between wage and any of the predictors is entirely dependent on the other two predictors\n\n\nm  <- lm(wage ~ poly(age, 2) * jobclass * health, Wage)\n\nemmip(m, health ~ age|jobclass, CIs = TRUE, \n      at = list(age = c(25, 45, 65)), \n      dodge = 5, ylab = \"Salary  [1000 $]\")\n\n\n\nFor example, the relationship between money and health depends on jobclass and age. As we can see from the reference grid of our model, there are many combinations you can get contrasts for. You can essentially compare every error-bar on the plot to any other error-bar. Here are just a couple of contrast examples, but feel free to explore any of them.\n\n\nref_grid(m, at = list(age = c(25, 45, 65))) @grid \n\n   age       jobclass         health .wgt.\n1   25  1. Industrial      1. <=Good   487\n2   45  1. Industrial      1. <=Good   487\n3   65  1. Industrial      1. <=Good   487\n4   25 2. Information      1. <=Good   371\n5   45 2. Information      1. <=Good   371\n6   65 2. Information      1. <=Good   371\n7   25  1. Industrial 2. >=Very Good  1057\n8   45  1. Industrial 2. >=Very Good  1057\n9   65  1. Industrial 2. >=Very Good  1057\n10  25 2. Information 2. >=Very Good  1085\n11  45 2. Information 2. >=Very Good  1085\n12  65 2. Information 2. >=Very Good  1085\n\nemmeans(m, pairwise ~ health|jobclass,  by = \"age\",\n        at = list(age = c(25, 45, 65)))$contrasts\n\njobclass = 1. Industrial, age = 25:\n contrast                   estimate   SE   df t.ratio p.value\n 1. <=Good - 2. >=Very Good    -6.34 4.34 2988  -1.461  0.1440\n\njobclass = 2. Information, age = 25:\n contrast                   estimate   SE   df t.ratio p.value\n 1. <=Good - 2. >=Very Good    -5.37 6.44 2988  -0.835  0.4041\n\njobclass = 1. Industrial, age = 45:\n contrast                   estimate   SE   df t.ratio p.value\n 1. <=Good - 2. >=Very Good   -13.97 2.76 2988  -5.061  <.0001\n\njobclass = 2. Information, age = 45:\n contrast                   estimate   SE   df t.ratio p.value\n 1. <=Good - 2. >=Very Good   -21.52 2.99 2988  -7.198  <.0001\n\njobclass = 1. Industrial, age = 65:\n contrast                   estimate   SE   df t.ratio p.value\n 1. <=Good - 2. >=Very Good   -12.34 6.32 2988  -1.951  0.0512\n\njobclass = 2. Information, age = 65:\n contrast                   estimate   SE   df t.ratio p.value\n 1. <=Good - 2. >=Very Good   -12.26 5.80 2988  -2.116  0.0345\n\nemmeans(m, pairwise ~ age|jobclass,  by = \"health\",\n        at = list(age = c(25, 45, 65)))$contrasts\n\njobclass = 1. Industrial, health = 1. <=Good:\n contrast      estimate   SE   df t.ratio p.value\n age25 - age45   -24.32 4.35 2988  -5.587  <.0001\n age25 - age65   -16.37 5.76 2988  -2.842  0.0126\n age45 - age65     7.95 5.12 2988   1.553  0.2665\n\njobclass = 2. Information, health = 1. <=Good:\n contrast      estimate   SE   df t.ratio p.value\n age25 - age45   -19.92 6.49 2988  -3.068  0.0062\n age25 - age65   -18.12 7.05 2988  -2.571  0.0275\n age45 - age65     1.80 5.12 2988   0.352  0.9340\n\njobclass = 1. Industrial, health = 2. >=Very Good:\n contrast      estimate   SE   df t.ratio p.value\n age25 - age45   -31.96 2.92 2988 -10.940  <.0001\n age25 - age65   -22.37 4.80 2988  -4.656  <.0001\n age45 - age65     9.58 4.77 2988   2.011  0.1097\n\njobclass = 2. Information, health = 2. >=Very Good:\n contrast      estimate   SE   df t.ratio p.value\n age25 - age45   -36.07 3.41 2988 -10.586  <.0001\n age25 - age65   -25.01 4.45 2988  -5.626  <.0001\n age45 - age65    11.06 4.25 2988   2.600  0.0254\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\nBut weight! You may suddenly realize that you now have a different problem. While models without interactions have limited results, models with interactions provide too many results, even with only two or three categories per predictor. Imagine the results if one of the predictors had five categories. Fortunately, there is a solution that allows for presenting these results in a more compact manner.\nMatrix results\nFor that, and in order to be more realistic, let‚Äôs actually consider the education predictor with five categories and have it interact with jobclass. Even with only two predictors, we‚Äôll get so many EMMs and contrasts that they won‚Äôt fit on a single computer page. So, as the number of results increases, it becomes necessary to have a more compact way of presenting them.\nOne of the most effective ways way I have found is to put all the results into a matrix. This matrix displays the EMMs along the diagonal, P values in the upper triangle, and differences in the lower triangle. For example, the EMMs for individuals who did not finish high school are 85 for Industrial jobs and 88 for IT jobs, which is exactly what we see in the first square brackets. The salary difference between them and their peers who did finish high school is 16K per year and is not significant for the Industrial job class because the p-value is 0.16. You get the idea.\n\n\nm  <- lm(wage ~ education * jobclass, salary)\n\nm_emmeans <- emmeans(m, pairwise ~ education | jobclass, adjust = \"bonferroni\")\nm_emmeans\n\n$emmeans\njobclass = 1. Industrial:\n education          emmean   SE  df lower.CL upper.CL\n 1. < HS Grad         85.0 4.72 490     75.8     94.3\n 2. HS Grad          101.5 4.98 490     91.7    111.3\n 3. Some College     108.4 5.24 490     98.1    118.7\n 4. College Grad     122.7 6.07 490    110.8    134.6\n 5. Advanced Degree  148.6 8.70 490    131.5    165.7\n\njobclass = 2. Information:\n education          emmean   SE  df lower.CL upper.CL\n 1. < HS Grad         88.6 6.87 490     75.1    102.1\n 2. HS Grad           99.3 6.23 490     87.0    111.5\n 3. Some College     105.3 5.80 490     93.9    116.7\n 4. College Grad     127.6 5.06 490    117.6    137.5\n 5. Advanced Degree  155.3 4.35 490    146.7    163.8\n\nConfidence level used: 0.95 \n\n$contrasts\njobclass = 1. Industrial:\n contrast                             estimate    SE  df t.ratio\n 1. < HS Grad - 2. HS Grad              -16.48  6.86 490  -2.403\n 1. < HS Grad - 3. Some College         -23.40  7.05 490  -3.319\n 1. < HS Grad - 4. College Grad         -37.68  7.69 490  -4.900\n 1. < HS Grad - 5. Advanced Degree      -63.58  9.89 490  -6.427\n 2. HS Grad - 3. Some College            -6.93  7.23 490  -0.958\n 2. HS Grad - 4. College Grad           -21.20  7.85 490  -2.700\n 2. HS Grad - 5. Advanced Degree        -47.10 10.02 490  -4.701\n 3. Some College - 4. College Grad      -14.28  8.02 490  -1.779\n 3. Some College - 5. Advanced Degree   -40.18 10.15 490  -3.957\n 4. College Grad - 5. Advanced Degree   -25.90 10.61 490  -2.442\n p.value\n  0.1664\n  0.0097\n  <.0001\n  <.0001\n  1.0000\n  0.0718\n  <.0001\n  0.7582\n  0.0009\n  0.1496\n\njobclass = 2. Information:\n contrast                             estimate    SE  df t.ratio\n 1. < HS Grad - 2. HS Grad              -10.62  9.28 490  -1.145\n 1. < HS Grad - 3. Some College         -16.70  8.99 490  -1.857\n 1. < HS Grad - 4. College Grad         -38.94  8.54 490  -4.561\n 1. < HS Grad - 5. Advanced Degree      -66.62  8.13 490  -8.190\n 2. HS Grad - 3. Some College            -6.08  8.51 490  -0.714\n 2. HS Grad - 4. College Grad           -28.32  8.03 490  -3.528\n 2. HS Grad - 5. Advanced Degree        -56.00  7.59 490  -7.373\n 3. Some College - 4. College Grad      -22.24  7.70 490  -2.889\n 3. Some College - 5. Advanced Degree   -49.92  7.25 490  -6.889\n 4. College Grad - 5. Advanced Degree   -27.68  6.67 490  -4.148\n p.value\n  1.0000\n  0.6392\n  0.0001\n  <.0001\n  1.0000\n  0.0046\n  <.0001\n  0.0403\n  <.0001\n  0.0004\n\nP value adjustment: bonferroni method for 10 tests \n\npwpm(m_emmeans[[1]], adjust = \"bonferroni\")\n\n\njobclass = 1. Industrial\n                   1. < HS Grad 2. HS Grad 3. Some College\n1. < HS Grad              [ 85]     0.1664          0.0097\n2. HS Grad               -16.48      [102]          1.0000\n3. Some College          -23.40      -6.93           [108]\n4. College Grad          -37.68     -21.20          -14.28\n5. Advanced Degree       -63.58     -47.10          -40.18\n                   4. College Grad 5. Advanced Degree\n1. < HS Grad                <.0001             <.0001\n2. HS Grad                  0.0718             <.0001\n3. Some College             0.7582             0.0009\n4. College Grad              [123]             0.1496\n5. Advanced Degree          -25.90              [149]\n\njobclass = 2. Information\n                   1. < HS Grad 2. HS Grad 3. Some College\n1. < HS Grad            [ 88.6]     1.0000          0.6392\n2. HS Grad               -10.62    [ 99.3]          1.0000\n3. Some College          -16.70      -6.08         [105.3]\n4. College Grad          -38.94     -28.32          -22.24\n5. Advanced Degree       -66.62     -56.00          -49.92\n                   4. College Grad 5. Advanced Degree\n1. < HS Grad                <.0001             <.0001\n2. HS Grad                  0.0046             <.0001\n3. Some College             0.0403             <.0001\n4. College Grad            [127.6]             0.0004\n5. Advanced Degree          -27.68            [155.3]\n\nRow and column labels: education\nUpper triangle: P values   adjust = \"bonferroni\"\nDiagonal: [Estimates] (emmean) \nLower triangle: Comparisons (estimate)   earlier vs. later\n\nGraphical comparisons and plot p-values\nAnd since one picture is worth a thousands words, the second effective way to present a lot of results is to actually visualize them, where we:\nfirst visualize the estimates with contrasts using a simple plot command and the argument comparisons = TRUE and\nthen plot all the p-values using the Pairwise P-value plot (via pwpp command)\n\n\nplot(m_emmeans, comparisons = TRUE)\n\n\n\n\n\npwpp(m_emmeans[[1]])+     # by = \"health\"\n  geom_vline(xintercept = 0.05, linetype = 2) \n\n\n\nNow, that we know how useful and informative interactions can be, the ONLY thing left to learn is how to find the best model, which includes only meaningful interactions. Fortunately, {glmulti} package allows you to do this with a few lines of code and if you wanna find out how, check out this video.\nConclusion\nIncluding and interpreting interactions make things more complicated, but also more rewarding. However, interpreting three-way type of effect can be difficult and is prone to mistakes. That is why they are rarely used in inferential statistics, despite the fact that they can improve predictive power of models. Further difficult to interpret cases would be including one interaction and several non-interacting predictors, or including several interactions into the same model. Thus, I‚Äôd only use two predictors and two-way interactions.\nBonus (questionable ;) content\nEffect size: Cohen effect sizes are close cousins of pairwise differences\n\n\neff_size(\n  m_emmeans, \n  sigma = sqrt(mean(sigma(m)^2)), \n  edf   = df.residual(m) )\n\njobclass = 1. Industrial:\n contrast                               effect.size    SE  df\n (1. < HS Grad - 2. HS Grad)                 -0.424 0.177 490\n (1. < HS Grad - 3. Some College)            -0.602 0.182 490\n (1. < HS Grad - 4. College Grad)            -0.969 0.200 490\n (1. < HS Grad - 5. Advanced Degree)         -1.635 0.260 490\n (2. HS Grad - 3. Some College)              -0.178 0.186 490\n (2. HS Grad - 4. College Grad)              -0.545 0.203 490\n (2. HS Grad - 5. Advanced Degree)           -1.211 0.261 490\n (3. Some College - 4. College Grad)         -0.367 0.207 490\n (3. Some College - 5. Advanced Degree)      -1.033 0.263 490\n (4. College Grad - 5. Advanced Degree)      -0.666 0.274 490\n lower.CL upper.CL\n   -0.771  -0.0762\n   -0.960  -0.2435\n   -1.362  -0.5757\n   -2.145  -1.1247\n   -0.544   0.1874\n   -0.943  -0.1469\n   -1.723  -0.6993\n   -0.773   0.0389\n   -1.550  -0.5160\n   -1.204  -0.1285\n\njobclass = 2. Information:\n contrast                               effect.size    SE  df\n (1. < HS Grad - 2. HS Grad)                 -0.273 0.239 490\n (1. < HS Grad - 3. Some College)            -0.429 0.232 490\n (1. < HS Grad - 4. College Grad)            -1.001 0.222 490\n (1. < HS Grad - 5. Advanced Degree)         -1.713 0.216 490\n (2. HS Grad - 3. Some College)              -0.156 0.219 490\n (2. HS Grad - 4. College Grad)              -0.728 0.208 490\n (2. HS Grad - 5. Advanced Degree)           -1.440 0.201 490\n (3. Some College - 4. College Grad)         -0.572 0.199 490\n (3. Some College - 5. Advanced Degree)      -1.284 0.191 490\n (4. College Grad - 5. Advanced Degree)      -0.712 0.173 490\n lower.CL upper.CL\n   -0.742   0.1958\n   -0.885   0.0257\n   -1.437  -0.5653\n   -2.138  -1.2883\n   -0.586   0.2737\n   -1.136  -0.3201\n   -1.834  -1.0457\n   -0.962  -0.1813\n   -1.659  -0.9088\n   -1.052  -0.3717\n\nsigma used for effect sizes: 38.89 \nConfidence level used: 0.95 \n\nOne interaction + more predictors (think twice before you do that)\n\n\nd <- mtcars %>% \n  mutate(cyl = factor(cyl),\n         am  = factor(am),\n         gear= factor(gear))\n\nm <- lm(mpg ~ am * cyl + hp + gear + disp, d)\nref_grid(m)\n\n'emmGrid' object with variables:\n    am = 0, 1\n    cyl = 4, 6, 8\n    hp = 146.69\n    gear = 3, 4, 5\n    disp = 230.72\n\n# cooler determine different values then reg_grid using \"at\"\nemmeans(m, pairwise ~ cyl | am, \n        at = list(am = \"0\", hp = 146.6875, disp = 230.7219)\n)$emmeans\n\nam = 0:\n cyl emmean   SE df lower.CL upper.CL\n 4     19.1 2.10 22     14.8     23.5\n 6     17.8 1.61 22     14.4     21.1\n 8     19.5 2.29 22     14.7     24.2\n\nResults are averaged over the levels of: gear \nConfidence level used: 0.95 \n\nemmeans(m, pairwise ~ cyl | am, \n        at = list(am = \"1\", hp = 146.6875, disp = 230.7219)\n)$emmeans\n\nam = 1:\n cyl emmean   SE df lower.CL upper.CL\n 4     23.2 2.02 22     19.1     27.4\n 6     18.9 1.88 22     14.9     22.8\n 8     23.0 3.52 22     15.7     30.3\n\nResults are averaged over the levels of: gear \nConfidence level used: 0.95 \n\n# contrasts here stay the same because of the linear numeric predictors\n\n# determine different values then reg_grid using \"at\"\n\nemmeans(m, pairwise ~ cyl | am, \n        at = list(am = \"1\", hp = 300, disp = c(100, 200, 300), gear = \"5\")\n)$emmeans %>% plot()\n\n\nmtcars.rg <- ref_grid(m,\n                      at = list(hp = 300, \n                                disp = c(100, 200, 300)))\nmtcars.rg\n\n'emmGrid' object with variables:\n    am = 0, 1\n    cyl = 4, 6, 8\n    hp = 300\n    gear = 3, 4, 5\n    disp = 100, 200, 300\n\n# plotting \nplot(mtcars.rg, by = \"gear\")\n\n\n\nInteractions with covariates + other predictors (think twice before doing this)\n\n\nlibrary(ISLR)\nm <- lm(wage ~ age*education + jobclass, data = Wage)\n\nref_grid(m)\n\n'emmGrid' object with variables:\n    age = 42.415\n    education = 1. < HS Grad, 2. HS Grad, 3. Some College, 4. College Grad, 5. Advanced Degree\n    jobclass = 1. Industrial, 2. Information\n\nemtrends(m, pairwise ~ education, var = \"age\")$emtrends\n\n education          age.trend    SE   df lower.CL upper.CL\n 1. < HS Grad           0.359 0.174 2989   0.0173    0.700\n 2. HS Grad             0.460 0.096 2989   0.2722    0.649\n 3. Some College        0.931 0.122 2989   0.6912    1.170\n 4. College Grad        0.464 0.126 2989   0.2172    0.710\n 5. Advanced Degree     0.485 0.169 2989   0.1526    0.817\n\nResults are averaged over the levels of: jobclass \nConfidence level used: 0.95 \n\nemmip(m, education ~ age, CIs = T, \n      cov.reduce = range)\n\n\nemtrends(m, pairwise ~ education, var = \"age\", \n        at = list(jobclass = \"1. Industrial\")\n)$emtrends\n\n education          age.trend    SE   df lower.CL upper.CL\n 1. < HS Grad           0.359 0.174 2989   0.0173    0.700\n 2. HS Grad             0.460 0.096 2989   0.2722    0.649\n 3. Some College        0.931 0.122 2989   0.6912    1.170\n 4. College Grad        0.464 0.126 2989   0.2172    0.710\n 5. Advanced Degree     0.485 0.169 2989   0.1526    0.817\n\nConfidence level used: 0.95 \n\nSeveral interactions (thing three times before doing this and then run away without doing this! or try to consult a VERY professional statistitian!)\n\n\nm <- lm(mpg ~ am * cyl + hp * gear, d)\nref_grid(m)\n\n'emmGrid' object with variables:\n    am = 0, 1\n    cyl = 4, 6, 8\n    hp = 146.69\n    gear = 3, 4, 5\n\nemmeans(m, pairwise ~ cyl | am)\n\n$emmeans\nam = 0:\n cyl emmean   SE df lower.CL upper.CL\n 4     18.3 2.03 21    14.10     22.6\n 6     18.5 1.53 21    15.28     21.6\n 8     16.9 2.48 21    11.78     22.1\n\nam = 1:\n cyl emmean   SE df lower.CL upper.CL\n 4     22.1 2.05 21    17.85     26.4\n 6     18.8 1.76 21    15.13     22.5\n 8     17.4 6.51 21     3.88     31.0\n\nResults are averaged over the levels of: gear \nConfidence level used: 0.95 \n\n$contrasts\nam = 0:\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6   -0.128 2.32 21  -0.055  0.9983\n cyl4 - cyl8    1.388 3.18 21   0.436  0.9009\n cyl6 - cyl8    1.516 2.79 21   0.543  0.8509\n\nam = 1:\n contrast    estimate   SE df t.ratio p.value\n cyl4 - cyl6    3.305 2.30 21   1.435  0.3418\n cyl4 - cyl8    4.685 7.27 21   0.644  0.7977\n cyl6 - cyl8    1.379 6.37 21   0.217  0.9745\n\nResults are averaged over the levels of: gear \nP value adjustment: tukey method for comparing a family of 3 estimates \n\nemmip(m, am ~ cyl, CIs = T)\n\n\nemtrends(m, ~ gear, var = \"hp\", cov.reduce = range)\n\n gear hp.trend     SE df lower.CL upper.CL\n 3     -0.0423 0.0236 21  -0.0913  0.00679\n 4     -0.1390 0.0419 21  -0.2260 -0.05191\n 5     -0.0369 0.0374 21  -0.1146  0.04088\n\nResults are averaged over the levels of: am, cyl, hp \nConfidence level used: 0.95 \n\nemmip(m, gear ~ hp, CIs = T, cov.reduce = range)\n\n\n\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-12-29-emmeans2interactions/thumbnail_emmeans_2.png",
    "last_modified": "2023-01-31T17:47:52+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-10-31-gtsummary/",
    "title": "R package reviews {gtsummary} Publication-Ready Tables of Data, Stat-Tests and Models!",
    "description": "{gtsummary} package helps to easily produce publication-ready & beautifully formatted summary tables of Data, Statistical Tests and Models! It calculates tons of statistics and has a beautiful design by default, but you can customize every aspect of your table and export it as a picture or MS Word format.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-11-29",
    "categories": [
      "videos",
      "statistics",
      "models",
      "machine learning"
    ],
    "contents": "\n\nContents\nThis post as a video\nWhy do we need {gtsummary}\n1. Summarize data\ntbl_summary()\n‚Äúby‚Äù argument\nStratified {gtsummary} tables with tbl_strata()\n\n2. Summarize Statistical tests with p-values\n1) pimp your table with add_*() functions\nSeveral different tests in one table\nChange all tests\nSpecify Independent Tests\nSpecify Dependent Tests\n\n2) pimp your table by tbl_summary() arguments\n\n3. Summarize regression models\nReady regression results with tbl_regression()\nSeveral univariate regression at once with tbl_uvregression()\nRegression model where the predictor remains the same, and the outcome changes\nSide-by-side Regression Models with tbl_merge()\nDifferent models with same predictors\nUni- Multi-variate models with same predictors + Descpriptive stats\n\ntbl_stack()\n3) Pimp your regression tables with helper functions modify_* (), bold_* () / italicize_* ()\n\n4. Saving these beautiful tables as a picture or as MS Word document\n4) Pimp your tables withadd {gt} arguments\n\n5. Report statistics inline\ninline_text()\n\n6. themes()\n7. Survey Data\n8. Some potentially useful things\nCross Tables with tbl_cross()\nCustom functions\nSummarize a continuous variable by one or more categorical variables\nPlot estimate (experimental, the forest plot is much better for now)\ntbl_survfit - Creates table of survival probabilities\nMedian survival\n\n9. Further readings and references\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 11 minutes long.\n\n\n\n\n\n\n\nWhy do we need {gtsummary}\nThe tables of data, different statistical tests and model results, we see in published scientific paper are beautifully formatted. But the road from data to such tables is often bumpy, has numerous steps and is prone to mistakes. Well, with {gtsummary} package you‚Äôll get these results in a few lines of code. So, let‚Äôs get into it and start with data!\n1. Summarize data\ntbl_summary()\nThe tbl_summary() function:\nautomatically recognizes continuous, categorical, and dichotomous (/da…™Ààk…ëÀêt.…ô.m…ôs/) variables in your data,\ncalculates appropriate descriptive statistics, like counts and percentages for categorical, and Median + IQR for numeric variables\nincludes the amount of missing values in each variable and\nautomatically creates footnotes with explanations and abbreviations.\n\n\n# install.packages(\"gtsummary\")\nlibrary(gtsummary)\n\nd <- trial %>% select(trt, age, grade, response)\n\ntbl_summary(d)\n\n\nCharacteristic\n      N = 2001\n    Chemotherapy Treatment\n¬†¬†¬†¬†Drug A\n98 (49%)¬†¬†¬†¬†Drug B\n102 (51%)Age\n47 (38, 57)¬†¬†¬†¬†Unknown\n11Grade\n¬†¬†¬†¬†I\n68 (34%)¬†¬†¬†¬†II\n68 (34%)¬†¬†¬†¬†III\n64 (32%)Tumor Response\n61 (32%)¬†¬†¬†¬†Unknown\n71 n (%); Median (IQR)\n    \n\n‚Äúby‚Äù argument\nHowever, most of the time we need to get descriptive stats for some groups, for example for Drug A and Drug B from the Treatment variable, which can be easily done by using ‚Äúby‚Äù argument.\n\n\ntbl_summary(d, by = trt)\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n    Age\n46 (37, 59)\n48 (39, 56)¬†¬†¬†¬†Unknown\n7\n4Grade\n\n¬†¬†¬†¬†I\n35 (36%)\n33 (32%)¬†¬†¬†¬†II\n32 (33%)\n36 (35%)¬†¬†¬†¬†III\n31 (32%)\n33 (32%)Tumor Response\n28 (29%)\n33 (34%)¬†¬†¬†¬†Unknown\n3\n41 Median (IQR); n (%)\n    \n\nStratified {gtsummary} tables with tbl_strata()\nAnd if we need to go even further and calculate descriptive stats for Different Drugs inside of different Grade Levels, we can create a stratified table by using tbl_summary() function inside of tbl_strata() function, where we also specify ‚Äústrata‚Äù argument.\n\n\n# minimalist example\nd %>%\n  tbl_strata(\n    strata = grade, \n    ~.x %>% \n      tbl_summary(by = trt))\n\n\nCharacteristic\n      \n        I\n      \n      \n        II\n      \n      \n        III\n      \n    Drug A, N = 351\n      Drug B, N = 331\n      Drug A, N = 321\n      Drug B, N = 361\n      Drug A, N = 311\n      Drug B, N = 331\n    Age\n46 (36, 60)\n48 (42, 55)\n44 (31, 54)\n50 (43, 57)\n52 (42, 60)\n45 (36, 52)¬†¬†¬†¬†Unknown\n2\n0\n2\n4\n3\n0Tumor Response\n8 (23%)\n13 (41%)\n7 (23%)\n12 (36%)\n13 (43%)\n8 (24%)¬†¬†¬†¬†Unknown\n0\n1\n2\n3\n1\n01 Median (IQR); n (%)\n    \n\n\n\n# advanced example\nfancy_data_table <- trial %>%\n  select(trt, grade, age, stage) %>%\n  mutate(grade = paste(\"Grade\", grade)) %>%\n  tbl_strata(\n    strata = grade, \n    ~.x %>%\n      tbl_summary(by = trt, missing = \"no\") %>%\n      modify_header(all_stat_cols() ~ \"**{level}**\")\n  )\n\nlibrary(flextable)\nfancy_data_table %>%\n  as_flex_table() %>% \n  save_as_image(path = \"fancy_data_table.png\")\n\n[1] \"/Users/zablotski/Library/Mobile Documents/com~apple~CloudDocs/7. Hobby/programming and stats/yuzaR-Blog/_posts/2022-10-31-gtsummary/fancy_data_table.png\"\n\n2. Summarize Statistical tests with p-values\n1) pimp your table with add_*() functions\nBut while descriptive stats is fine, we often would rather compare some groups and see whether they significantly differ. And here is where the real magic of {gtsummary} starts. Namely, the add_p() function:\nautomatically detects a variable type,\napplies correct statistical tests to check the difference between Treatment Drugs\nand displays p-values and names of Statistical Tests in the table\n\n\nd %>% \n  tbl_summary(by = trt) %>% \n  add_p()\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    Age\n46 (37, 59)\n48 (39, 56)\n0.7¬†¬†¬†¬†Unknown\n7\n4\nGrade\n\n\n0.9¬†¬†¬†¬†I\n35 (36%)\n33 (32%)\n¬†¬†¬†¬†II\n32 (33%)\n36 (35%)\n¬†¬†¬†¬†III\n31 (32%)\n33 (32%)\nTumor Response\n28 (29%)\n33 (34%)\n0.5¬†¬†¬†¬†Unknown\n3\n4\n1 Median (IQR); n (%)\n    2 Wilcoxon rank sum test; Pearson's Chi-squared test\n    \n\nNice, right? But wait, there is more! There is a whole family of add_* functions which enable you to pimp your table with more useful information, namely:\nadd_q() - add a column of p-values corrected for multiple testing in order to reduce False Discovery Rate\nadd_overall() - add overall summary statistics\nadd_n() - add number of observations\nadd_ci() - add confidence intervals for medians proportions, etc., and even\nadd_stat_label() - add labels of descriptive statistics to each variable.\n\n\ntbl_summary(d, by = trt) %>% \n  add_p() %>% \n  add_q() %>% \n  add_overall() %>% \n  add_n() %>% \n  add_ci() %>% \n  add_stat_label()\n\n\nCharacteristic\n      N\n      Overall, N = 200\n      95% CI1\n      Drug A, N = 98\n      95% CI1\n      Drug B, N = 102\n      95% CI1\n      p-value2\n      q-value3\n    Age, Median (IQR)\n189\n47 (38, 57)\n\n46 (37, 59)\n44, 50\n48 (39, 56)\n45, 50\n0.7\n0.9¬†¬†¬†¬†Unknown\n\n11\n\n7\n\n4\n\n\nGrade, n (%)\n200\n\n\n\n\n\n\n0.9\n0.9¬†¬†¬†¬†I\n\n68 (34%)\n28%, 41%\n35 (36%)\n26%, 46%\n33 (32%)\n24%, 42%\n\n¬†¬†¬†¬†II\n\n68 (34%)\n28%, 41%\n32 (33%)\n24%, 43%\n36 (35%)\n26%, 45%\n\n¬†¬†¬†¬†III\n\n64 (32%)\n26%, 39%\n31 (32%)\n23%, 42%\n33 (32%)\n24%, 42%\n\nTumor Response, n (%)\n193\n61 (32%)\n25%, 39%\n28 (29%)\n21%, 40%\n33 (34%)\n25%, 44%\n0.5\n0.9¬†¬†¬†¬†Unknown\n\n7\n\n3\n\n4\n\n\n1 CI = Confidence Interval\n    2 Wilcoxon rank sum test; Pearson's Chi-squared test\n    3 False discovery rate correction for multiple testing\n    \n\nThe add_* helpers are actually just one of the four ways to improve your table and I‚Äôll show you the next three very soon, but for now, have a look at the last function from add_* family, namely:\nadd_difference() which can be used instead of add_p(). It is useful, because ‚Äúa difference‚Äù between groups is actually what we want most of the time. Here, you‚Äôll get different statistical tests by default, namely Welch t-test for the difference in means, and Two sample test for equality in proportions. How amazing is that?\n\n\ntbl_summary(d, by = trt) %>% \n  add_difference() %>% \n  add_q() %>% \n  add_overall() %>% \n  add_n() %>% \n  add_ci() %>% \n  add_stat_label() \n\n\nCharacteristic\n      N\n      Overall, N = 200\n      95% CI1\n      Drug A, N = 98\n      95% CI1\n      Drug B, N = 102\n      95% CI1\n      Difference2\n      95% CI2,1\n      p-value2\n      q-value3\n    Age, Median (IQR)\n189\n47 (38, 57)\n\n46 (37, 59)\n44, 50\n48 (39, 56)\n45, 50\n-0.44\n-4.6, 3.7\n0.8\n0.8¬†¬†¬†¬†Unknown\n\n11\n\n7\n\n4\n\n\n\n\nGrade, n (%)\n200\n\n\n\n\n\n\n0.07\n-0.20, 0.35\n\n¬†¬†¬†¬†I\n\n68 (34%)\n28%, 41%\n35 (36%)\n26%, 46%\n33 (32%)\n24%, 42%\n\n\n\n¬†¬†¬†¬†II\n\n68 (34%)\n28%, 41%\n32 (33%)\n24%, 43%\n36 (35%)\n26%, 45%\n\n\n\n¬†¬†¬†¬†III\n\n64 (32%)\n26%, 39%\n31 (32%)\n23%, 42%\n33 (32%)\n24%, 42%\n\n\n\nTumor Response, n (%)\n193\n61 (32%)\n25%, 39%\n28 (29%)\n21%, 40%\n33 (34%)\n25%, 44%\n-4.2%\n-18%, 9.9%\n0.6\n0.8¬†¬†¬†¬†Unknown\n\n7\n\n3\n\n4\n\n\n\n\n1 CI = Confidence Interval\n    2 Welch Two Sample t-test; Standardized Mean Difference; Two sample test for equality of proportions\n    3 False discovery rate correction for multiple testing\n    \n\nSeveral different tests in one table\nBut while default statistical tests save you tons of time and nerves, what if we want to apply specific tests to specific variables? Well, you can of coarse easily do that and here is how!\nChange all tests\nFirst of all, you can change all tests at once. For example, if all of our your data is normally distributed, ‚Ä¶ which would probably never happen ‚Ä¶ by anyway, if all of our your data is normally distributed, we can apply ‚Äút.tests‚Äù to all_continuous() variables via ‚Äútest‚Äù argument inside of add_p() function. And since it makes sense to use ‚ÄúMean & SD‚Äù for ‚Äút.tests‚Äù instead of the default ‚ÄúMedian & IQR‚Äù, we can specify the descriptive statistics of our choice by using ‚Äústatistic‚Äù argument inside of tbl_summary() function. Similarly, we can force add_p() to apply ‚Äúfisher.tests‚Äù to all_categorical() variables.\n\n\ntrial %>%\n  tbl_summary(\n    by        = trt,\n    statistic = gtsummary::all_continuous()  ~ \"{mean} ({sd})\") %>%\n  add_p(test  = list(\n      gtsummary::all_continuous()  ~ \"t.test\", \n      gtsummary::all_categorical() ~ \"fisher.test\"))\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    Age\n47 (15)\n47 (14)\n0.8¬†¬†¬†¬†Unknown\n7\n4\nMarker Level (ng/mL)\n1.02 (0.89)\n0.82 (0.83)\n0.12¬†¬†¬†¬†Unknown\n6\n4\nT Stage\n\n\n0.9¬†¬†¬†¬†T1\n28 (29%)\n25 (25%)\n¬†¬†¬†¬†T2\n25 (26%)\n29 (28%)\n¬†¬†¬†¬†T3\n22 (22%)\n21 (21%)\n¬†¬†¬†¬†T4\n23 (23%)\n27 (26%)\nGrade\n\n\n>0.9¬†¬†¬†¬†I\n35 (36%)\n33 (32%)\n¬†¬†¬†¬†II\n32 (33%)\n36 (35%)\n¬†¬†¬†¬†III\n31 (32%)\n33 (32%)\nTumor Response\n28 (29%)\n33 (34%)\n0.5¬†¬†¬†¬†Unknown\n3\n4\nPatient Died\n52 (53%)\n60 (59%)\n0.5Months to Death/Censor\n20.2 (5.0)\n19.0 (5.5)\n0.111 Mean (SD); n (%)\n    2 Welch Two Sample t-test; Fisher's exact test\n    \n\nSpecify Independent Tests\nMoreover, we can specify ANY descriptive statistics and ANY test we wish for ANY particular variable. The variables which were not explicitly specified, for example ‚ÄúGrade‚Äù, will be analysed with default tests, and the footnote will be automatically extended in order to include all the necessary information, so that you have less things to worry about.\n\n\ntrial %>% \n  tbl_summary(\n    by = trt,\n    statistic = list(\n      age    ~ \"{mean} ({sd})\",\n      marker ~ \"{mean} ({min}, {p25}, {p75}, {max})\",\n      stage  ~ \"{n} / {N} ({p}%)\")) %>% \n  add_p(test = list(\n      age    ~ \"t.test\",\n      marker ~ \"wilcox.test\",\n      stage  ~ \"fisher.test\")) %>% \n  separate_p_footnotes()\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value\n    Age\n47 (15)\n47 (14)\n0.82¬†¬†¬†¬†Unknown\n7\n4\nMarker Level (ng/mL)\n1.02 (0.00, 0.24, 1.57, 3.87)\n0.82 (0.01, 0.19, 1.20, 3.64)\n0.0853¬†¬†¬†¬†Unknown\n6\n4\nT Stage\n\n\n0.94¬†¬†¬†¬†T1\n28 / 98 (29%)\n25 / 102 (25%)\n¬†¬†¬†¬†T2\n25 / 98 (26%)\n29 / 102 (28%)\n¬†¬†¬†¬†T3\n22 / 98 (22%)\n21 / 102 (21%)\n¬†¬†¬†¬†T4\n23 / 98 (23%)\n27 / 102 (26%)\nGrade\n\n\n0.95¬†¬†¬†¬†I\n35 (36%)\n33 (32%)\n¬†¬†¬†¬†II\n32 (33%)\n36 (35%)\n¬†¬†¬†¬†III\n31 (32%)\n33 (32%)\nTumor Response\n28 (29%)\n33 (34%)\n0.55¬†¬†¬†¬†Unknown\n3\n4\nPatient Died\n52 (53%)\n60 (59%)\n0.45Months to Death/Censor\n23.5 (17.4, 24.0)\n21.2 (14.6, 24.0)\n0.1431 Mean (SD); Mean (Minimum, IQR, Maximum); n / N (%); n (%); Median (IQR)\n    2 Welch Two Sample t-test\n    3 Wilcoxon rank sum test\n    4 Fisher's exact test\n    5 Pearson's Chi-squared test\n    \n\nSpecify Dependent Tests\nYou can conduct almost any statistical test you can imagine. Here is a quick example of ‚Äúpaired.t.test‚Äù for continuous and paired ‚Äúmcnemar.test‚Äù for categorical variables. You just need to make sure, you have an ‚Äúid‚Äù column and have complete pairs in the data:\n\n\ntrial_paired <-\n  trial %>%\n  select(trt, marker, response) %>%\n  group_by(trt) %>%\n  mutate(id = row_number()) %>%\n  ungroup()\n\ntrial_paired %>%\n  filter(complete.cases(.)) %>%\n  group_by(id) %>%\n  filter(n() == 2) %>%\n  ungroup() %>%\n  tbl_summary(by = trt, include = -id) %>%\n  add_p(test = list(\n    marker   ~ \"paired.t.test\",\n    response ~ \"mcnemar.test\"), \n    group    = id)\n\n\nCharacteristic\n      Drug A, N = 831\n      Drug B, N = 831\n      p-value2\n    Marker Level (ng/mL)\n0.82 (0.22, 1.63)\n0.53 (0.18, 1.26)\n0.2Tumor Response\n21 (25%)\n28 (34%)\n0.31 Median (IQR); n (%)\n    2 Paired t-test; McNemar's Chi-squared test with continuity correction\n    \n\n2) pimp your table by tbl_summary() arguments\nTests are amazing, but the real power of {gtsummary} is that it can easily summarize regression results. But before we summarize regressions, I have to give you the second useful kind of pimping your table, namely by using tbl_summary() arguments.\n\nSince you are already familiar with ‚Äúby‚Äù and ‚Äústatistic‚Äù arguments, you can imagine that there are more, right? Here are some of the most useful ones:\nlabel - changes variable names\ndigits - changes the number of rounded decimal places\nmissing - determines whether missing observations are reported\nmissing_text - changes the name of the missing data\ntype - changes variable type for specified variables, affecting which summary statistics are displayed\nsort - changes the type of sorting for categorical variables, where ‚Äúalphanumeric‚Äù is a default, but when we change it to ‚Äúfrequency‚Äù, ‚ÄúTumor response‚Äù and ‚ÄúPatient Died‚Äù variables will be sorted differently\npercent - determines how percentage statistics are calculated and displayed. While the default is ‚Äúcolumn‚Äù, you can choose ‚Äúrow‚Äù, or ‚Äúcell‚Äù\ninclude - allows to either choose or remove particular predictors. For example, let‚Äôs remove ‚ÄúMonth to Death‚Äù for now, but make a separate survival study on that in a moment\nAnd of coarse, every function we use has it‚Äôs own arguments. For instance, if you want to determine the number of decimal places in only p-values, not in the rest of the table, use pvalue_fun argument inside of add_p() function\n\n\ntrial %>%\n  tbl_summary(\n    by           = trt,\n    statistic    = age   ~ \"{mean} ({sd})\", \n    label        = grade ~ \"New Name - Tumor Grade\",\n    digits       = all_continuous() ~ 1,\n    # missing    = \"no\", \n    missing_text = \"Missing values\",\n    type         = list(response ~ \"categorical\",\n                        death    ~ \"categorical\"),\n    sort         = everything()  ~ \"frequency\", \n    percent      = \"cell\", \n    include      = - ttdeath\n    ) %>%\n  add_p(pvalue_fun = ~style_pvalue(.x, digits = 3))\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    Age\n47.0 (14.7)\n47.4 (14.0)\n0.718¬†¬†¬†¬†Missing values\n7\n4\nMarker Level (ng/mL)\n0.8 (0.2, 1.6)\n0.5 (0.2, 1.2)\n0.085¬†¬†¬†¬†Missing values\n6\n4\nT Stage\n\n\n0.866¬†¬†¬†¬†T2\n25 (12%)\n29 (14%)\n¬†¬†¬†¬†T1\n28 (14%)\n25 (12%)\n¬†¬†¬†¬†T4\n23 (12%)\n27 (14%)\n¬†¬†¬†¬†T3\n22 (11%)\n21 (10%)\nNew Name - Tumor Grade\n\n\n0.871¬†¬†¬†¬†I\n35 (18%)\n33 (16%)\n¬†¬†¬†¬†II\n32 (16%)\n36 (18%)\n¬†¬†¬†¬†III\n31 (16%)\n33 (16%)\nTumor Response\n\n\n0.530¬†¬†¬†¬†0\n67 (35%)\n65 (34%)\n¬†¬†¬†¬†1\n28 (15%)\n33 (17%)\n¬†¬†¬†¬†Missing values\n3\n4\nPatient Died\n\n\n0.412¬†¬†¬†¬†1\n52 (26%)\n60 (30%)\n¬†¬†¬†¬†0\n46 (23%)\n42 (21%)\n1 Mean (SD); Median (IQR); n (%)\n    2 Wilcoxon rank sum test; Pearson's Chi-squared test\n    \n\n3. Summarize regression models\nNow, we finally arrived at the best part of {gtsummary}, namely summarizing regression models. ‚Ä¶ Hallelujah‚Ä¶ {gtsummary} supports most of the classic models (status 14.11.2022), like generalized linear, survival or mixed effects models and many more and this support is steadily growing:\n\nReady regression results with tbl_regression()\nFor example the tbl_regression() function takes a multiple Bayesian logistic regression and returns a publication-ready and beautifully formatted table of model results. Here the ‚Äúexponentiate = TRUE‚Äù argument displays Odds Ratios instead of default but less intuitive Log-Odds Ratios.\n\n\nbm <- arm::bayesglm(response ~ trt + age + grade, trial, family = binomial)\n\nbm_table <- tbl_regression(bm, exponentiate = TRUE) \n\nbm_table\n\n\nCharacteristic\n      OR1\n      95% CI1\n      p-value\n    Chemotherapy Treatment\n\n\n¬†¬†¬†¬†Drug A\n‚Äî\n‚Äî\n¬†¬†¬†¬†Drug B\n1.13\n0.60, 2.13\n0.7Age\n1.02\n1.00, 1.04\n0.10Grade\n\n\n¬†¬†¬†¬†I\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n0.86\n0.39, 1.85\n0.7¬†¬†¬†¬†III\n1.01\n0.47, 2.15\n>0.91 OR = Odds Ratio, CI = Confidence Interval\n    \n\nNow, remember we removed ‚Äútime to death‚Äù predictor from the table for a separate analysis? Well, let‚Äôs use ‚Äútime to death‚Äù in a Cox Proportional Hazard regression and see a beautiful output tbl_regression() function produces:\n\n\nlibrary(survival)\ncm <- coxph(Surv(ttdeath, death) ~ trt + age + grade, data = trial)\n\ncm_table <- tbl_regression(cm, exponentiate = TRUE) \n\ncm_table\n\n\nCharacteristic\n      HR1\n      95% CI1\n      p-value\n    Chemotherapy Treatment\n\n\n¬†¬†¬†¬†Drug A\n‚Äî\n‚Äî\n¬†¬†¬†¬†Drug B\n1.30\n0.88, 1.92\n0.2Age\n1.01\n0.99, 1.02\n0.3Grade\n\n\n¬†¬†¬†¬†I\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n1.21\n0.73, 1.99\n0.5¬†¬†¬†¬†III\n1.79\n1.12, 2.86\n0.0141 HR = Hazard Ratio, CI = Confidence Interval\n    \n\nSeveral univariate regression at once with tbl_uvregression()\nBut tbl_regression() function is just a beginning. tbl_uvregression() conducts univariate model with every variable in our data set. We only need to\ndefine the method we want to use for modelling,\ndetermine the ‚Äúresponse‚Äù variable, which in our case is called ‚Ä¶ response :)\nand specify modelling family in the ‚Äúmethod arguments‚Äù ‚Ä¶ argument :)\n‚Ä¶ and viola, we have three separate logistic regression beautifully combined into one table.\n\n\nuvlm_table <- trial %>%\n  select(response, trt, age, grade) %>%\n  tbl_uvregression(\n    method       = glm,\n    y            = response,\n    method.args  = list(family = binomial),\n    exponentiate = TRUE\n  ) \n\nuvlm_table\n\n\nCharacteristic\n      N\n      OR1\n      95% CI1\n      p-value\n    Chemotherapy Treatment\n193\n\n\n¬†¬†¬†¬†Drug A\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†Drug B\n\n1.21\n0.66, 2.24\n0.5Age\n183\n1.02\n1.00, 1.04\n0.10Grade\n193\n\n\n¬†¬†¬†¬†I\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n\n0.95\n0.45, 2.00\n0.9¬†¬†¬†¬†III\n\n1.10\n0.52, 2.29\n0.81 OR = Odds Ratio, CI = Confidence Interval\n    \n\nConducting several univariate Cox regressions is even more simple. I quickly learned to love tbl_uvregression(), because the same few lines of code would easily conduct 30 or 300 univariate models if my data set if huge. So, it saves time! But what saves time even more is combining different tables with each other. Have a look at it!\n\n\nuvcm_table <- tbl_uvregression(\n  trial %>% \n    select(ttdeath, death, trt, age, grade), # 300 predictors\n  method       = coxph,\n  y            = Surv(ttdeath, death),\n  exponentiate = TRUE\n  ) \n\nuvcm_table\n\n\nCharacteristic\n      N\n      HR1\n      95% CI1\n      p-value\n    Chemotherapy Treatment\n200\n\n\n¬†¬†¬†¬†Drug A\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†Drug B\n\n1.25\n0.86, 1.81\n0.2Age\n189\n1.01\n0.99, 1.02\n0.3Grade\n200\n\n\n¬†¬†¬†¬†I\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n\n1.28\n0.80, 2.05\n0.3¬†¬†¬†¬†III\n\n1.69\n1.07, 2.66\n0.0241 HR = Hazard Ratio, CI = Confidence Interval\n    \n\nRegression model where the predictor remains the same, and the outcome changes\n\n\ntrial %>%\n  select(age, marker, ttdeath, trt) %>%\n  tbl_uvregression(\n    method = lm,\n    x = trt,\n    show_single_row = \"trt\",\n    hide_n = TRUE\n  ) %>%\n  modify_header(list(\n    label ~\"**Model Outcome**\",\n    estimate ~ \"**Treatment Coef.**\"\n  )) %>%\n  modify_footnote(estimate ~ \"Values larger than 0 indicate larger values in the Drug B group.\")\n\n\nModel Outcome\n      Treatment Coef.1\n      95% CI2\n      p-value\n    Age\n0.44\n-3.7, 4.6\n0.8Marker Level (ng/mL)\n-0.20\n-0.44, 0.05\n0.12Months to Death/Censor\n-1.2\n-2.7, 0.27\n0.111 Values larger than 0 indicate larger values in the Drug B group.\n    2 CI = Confidence Interval\n    \n\nSide-by-side Regression Models with tbl_merge()\nUsing only one function tbl_merge() we can put two tables we just created side-by-side and see the influence of the same predictors on two completely different response variables. We can even name those tables, for example ‚ÄúTumor Response‚Äù for univariate logistic regressions and ‚ÄúTime to Death‚Äù for univariate Cox regressions we just conducted. The footnote is again, automatically displaying appropriate units for each model type.\nDifferent models with same predictors\n\n\nfancy_table <-\n  tbl_merge(\n    tbls        = list(bm_table, cm_table),\n    tab_spanner = c(\"Tumor Response\", \"Time to Death\")\n  )\n\nfancy_table\n\n\nCharacteristic\n      \n        Tumor Response\n      \n      \n        Time to Death\n      \n    OR1\n      95% CI1\n      p-value\n      HR1\n      95% CI1\n      p-value\n    Chemotherapy Treatment\n\n\n\n\n\n¬†¬†¬†¬†Drug A\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†Drug B\n1.13\n0.60, 2.13\n0.7\n1.30\n0.88, 1.92\n0.2Age\n1.02\n1.00, 1.04\n0.10\n1.01\n0.99, 1.02\n0.3Grade\n\n\n\n\n\n¬†¬†¬†¬†I\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n0.86\n0.39, 1.85\n0.7\n1.21\n0.73, 1.99\n0.5¬†¬†¬†¬†III\n1.01\n0.47, 2.15\n>0.9\n1.79\n1.12, 2.86\n0.0141 OR = Odds Ratio, CI = Confidence Interval, HR = Hazard Ratio\n    \n\nUni- Multi-variate models with same predictors + Descpriptive stats\nBut even more useful is a meta-table where descriptive statistics is combined with the results ob both univariate and multivariate models.\n\n\nuni_multi <- tbl_merge(\n    tbls        = list(tbl_summary(d), uvlm_table, bm_table),\n    tab_spanner = c(\"**Describe**\", \"**Univariate Models**\", \"**Multivariate Model**\")\n  )\n\nuni_multi\n\n\nCharacteristic\n      \n        Describe\n      \n      \n        Univariate Models\n      \n      \n        Multivariate Model\n      \n    N = 2001\n      N\n      OR2\n      95% CI2\n      p-value\n      OR2\n      95% CI2\n      p-value\n    Chemotherapy Treatment\n\n193\n\n\n\n\n\n¬†¬†¬†¬†Drug A\n98 (49%)\n\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†Drug B\n102 (51%)\n\n1.21\n0.66, 2.24\n0.5\n1.13\n0.60, 2.13\n0.7Age\n47 (38, 57)\n183\n1.02\n1.00, 1.04\n0.10\n1.02\n1.00, 1.04\n0.10¬†¬†¬†¬†Unknown\n11\n\n\n\n\n\n\nGrade\n\n193\n\n\n\n\n\n¬†¬†¬†¬†I\n68 (34%)\n\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n68 (34%)\n\n0.95\n0.45, 2.00\n0.9\n0.86\n0.39, 1.85\n0.7¬†¬†¬†¬†III\n64 (32%)\n\n1.10\n0.52, 2.29\n0.8\n1.01\n0.47, 2.15\n>0.9Tumor Response\n61 (32%)\n\n\n\n\n\n\n¬†¬†¬†¬†Unknown\n7\n\n\n\n\n\n\n1 n (%); Median (IQR)\n    2 OR = Odds Ratio, CI = Confidence Interval\n    \n\ntbl_stack()\n\n\n# stacking two tbl_regression objects\nt1 <-\n    glm(response ~ trt, trial, family = binomial) %>%\n    tbl_regression(\n        exponentiate = TRUE,\n        label = list(trt ~ \"Treatment (unadjusted)\")\n    )\n\nt2 <-\n    glm(response ~ trt + grade + stage + marker, trial, family = binomial) %>%\n    tbl_regression(\n        include = \"trt\",\n        exponentiate = TRUE,\n        label = list(trt ~ \"Treatment (adjusted)\")\n    )\n\ntbl_stack_ex1 <- tbl_stack(list(t1, t2))\n\n# Example 2 ----------------------------------\n# stacking two tbl_merge objects\nlibrary(survival)\nt3 <-\n    coxph(Surv(ttdeath, death) ~ trt, trial) %>%\n    tbl_regression(\n        exponentiate = TRUE,\n        label = list(trt ~ \"Treatment (unadjusted)\")\n    )\n\nt4 <-\n    coxph(Surv(ttdeath, death) ~ trt + grade + stage + marker, trial) %>%\n    tbl_regression(\n        include = \"trt\",\n        exponentiate = TRUE,\n        label = list(trt ~ \"Treatment (adjusted)\")\n    )\n\n\n# first merging, then stacking\nrow1 <- tbl_merge(list(t1, t3), tab_spanner = c(\"Tumor Response\", \"Death\"))\nrow2 <- tbl_merge(list(t2, t4))\ntbl_stack_ex2 <-\n    tbl_stack(list(row1, row2), group_header = c(\"Unadjusted Analysis\", \"Adjusted Analysis\"))\n\ntbl_stack_ex2\n\n\nCharacteristic\n      \n        Tumor Response\n      \n      \n        Death\n      \n    OR1\n      95% CI1\n      p-value\n      HR1\n      95% CI1\n      p-value\n    Unadjusted Analysis\n    Treatment (unadjusted)\n\n\n\n\n\n¬†¬†¬†¬†Drug A\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†Drug B\n1.21\n0.66, 2.24\n0.5\n1.25\n0.86, 1.81\n0.2Adjusted Analysis\n    Treatment (adjusted)\n\n\n\n\n\n¬†¬†¬†¬†Drug A\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†Drug B\n1.48\n0.78, 2.86\n0.2\n1.30\n0.88, 1.92\n0.21 OR = Odds Ratio, CI = Confidence Interval, HR = Hazard Ratio\n    \n\n3) Pimp your regression tables with helper functions modify_* (), bold_* () / italicize_* ()\nAnd similarly to summary tables of statistical tests, we can easily customize our regression tables in 3 different ways.\nFirst, we can add more useful information to our regression table. For instance:\nadd_n(location = ‚Äúlevel‚Äù) - adds a N¬∞ of observations in each level of categorical variables\nadd_nevent(location = ‚Äúlevel‚Äù) - adds a N¬∞ of positive events of the outcome\nadd_global_p() - adds a global p-value for every variable, which is useful if we want to preselect potentially most influential variables for the further multivariate analysis with, let‚Äôs say, p-value of under 0.2\nadd_q() - adjusts p-values with False Discovery Rate correction for multiple testing as a default method, which can easily be changed to Bonferroni or any other correction method\nadd_significance_stars() - ‚Ä¶ adds significant stars and by default removes p-values and confidence intervals, but we can choose to keep them\nadd_vif() - adds the variance inflation factor (VIF) or generalized VIF (GVIF) to the regression table. Function uses car::vif() to calculate the VIF.\nSecondly, we can modify the appearance of our table by modifying headers, captions or footnotes or by sorting variables by significance\nmodify_header()\nmodify_caption()\nmodify_footnote\nsort_p()\nAnd finally, we have some aesthetics helpers, like bold_ * or italic_ * which helps to beautify our labels and levels even more. I personally found the bold_p() function to be the most interesting, because I can specify the threshold, under which the p-values will be displayed bold\nbold_p(t = 0.10, q = TRUE)\nbold_labels()\nbold_levels()\nitalicize_labels()\nitalicize_levels()\n\n\nglm(response ~ trt + age + ttdeath + grade, trial, family = binomial) %>% \n  tbl_regression(\n    #pvalue_fun   = ~style_pvalue(.x, digits = 3),\n    exponentiate = TRUE\n  ) %>% \n  \n  # add_* helpers\n  add_n(location = \"level\") %>%\n  add_nevent(location = \"level\") %>%  \n  add_global_p() %>%   \n  add_q() %>%        \n  add_significance_stars(hide_p = F, hide_se = T, hide_ci = F) %>% \n  add_vif() %>% \n  \n  # modify_* helpers\n  modify_header(label = \"**Predictor**\") %>% \n  modify_caption(\"**Table 1. Really cool looking table!**\") %>% \n  modify_footnote(\n    ci = \"CI = Credible Intervals are incredible ;)\", abbreviation = TRUE) %>%\n  sort_p() %>% \n  \n  # aesthetics helpers\n  bold_p(t = 0.10, q = TRUE) %>% \n  bold_labels() %>% \n  #bold_levels() %>% \n  #italicize_labels() %>% \n  italicize_levels()\n\n\nTable 1: Table 1. Really cool looking table!\n  \n  Predictor\n      N\n      Event N\n      OR1,2\n      95% CI2\n      p-value\n      q-value3\n      GVIF2\n      Adjusted GVIF4,2\n    Months to Death/Censor\n183\n58\n1.12**\n1.04, 1.22\n0.001\n0.006\n1.1\n1.0Age\n183\n58\n1.02\n1.00, 1.05\n0.052\n0.10\n1.0\n1.0Chemotherapy Treatment\n\n\nNA\n\n0.4\n0.5\n1.0\n1.0¬†¬†¬†¬†Drug A\n89\n27\n‚Äî\n‚Äî\n\n\n\n¬†¬†¬†¬†Drug B\n94\n31\n1.32\n0.69, 2.55\n\n\n\nGrade\n\n\nNA\n\n0.7\n0.7\n1.0\n1.0¬†¬†¬†¬†I\n65\n21\n‚Äî\n‚Äî\n\n\n\n¬†¬†¬†¬†II\n58\n17\n0.98\n0.44, 2.19\n\n\n\n¬†¬†¬†¬†III\n60\n20\n1.31\n0.59, 2.89\n\n\n\n1 *p<0.05; **p<0.01; ***p<0.001\n    2 OR = Odds Ratio, CI = Credible Intervals are incredible ;), GVIF = Generalized Variance Inflation Factor\n    3 False discovery rate correction for multiple testing\n    4 GVIF^[1/(2*df)]\n    \n\nWell, {gtsummary} has many more useful features that I can‚Äôt cover in this blog-post without completely overwhelming you. But some of the honorable mentions are:\ncross tables,\npossibility to handle survey data,\nuse predefined designs of tables with themes,\nreport statistical result inside of body of text with inline_text() function, and\ncustomize the table even further using a massive functionality of {gt} package,\n‚Ä¶ so, feel free to explore it by yourself. But there is ONE THING left which you absolutely need to know how to do, namely ‚Ä¶\n4. Saving these beautiful tables as a picture or as MS Word document\nFor that we‚Äôll use a {flextable} package and first convert our summary table into the flex-table-object, which we then save as an image, or as a publication ready ‚Äúdoc‚Äù file. Now, knowing how to visualize perfect tables, you absolutely need to learn how to perfectly visualize data and model results. Fortunately, you can also do it with only a few lines of code, which you can learn all about in less then 10 minutes from this video.\n\n\n# install.packages(\"flextable\")\nlibrary(flextable)\n\nfancy_table %>%\n  as_flex_table() %>% \n  save_as_image(path = \"fancy_table.png\")\n\n[1] \"/Users/zablotski/Library/Mobile Documents/com~apple~CloudDocs/7. Hobby/programming and stats/yuzaR-Blog/_posts/2022-10-31-gtsummary/fancy_table.png\"\n\nfancy_table %>%\n  as_flex_table() %>%\n  save_as_docx(path = \"fancy_table.docx\") \n\n\n4) Pimp your tables withadd {gt} arguments\nIt does not matter what kind of appearence you wish to add to your table, using {gt} package - it‚Äôs possible. By the way, that‚Äôs where {gtsummary} got it‚Äôs name ( ;) with sound ), where ‚Äúgt‚Äù means ‚Äúgrammar of tables‚Äù.\n\n\nlibrary(gt)\ntrial %>%\n  # create a gtsummary table\n  tbl_summary(by = trt) %>%\n  # convert from gtsummary object to gt object\n  as_gt() %>%\n  # modify with gt functions\n  tab_header(\"Table 1: Baseline Characteristics\") %>% \n  tab_spanner(\n    label = \"Randomization Group\",  \n    columns = starts_with(\"stat_\")\n  ) %>% \n  tab_options(\n    table.font.size = \"small\",\n    data_row.padding = px(1)) \n\n\nTable 1: Baseline Characteristics\n    Characteristic\n      \n        Randomization Group\n      \n    Drug A, N = 981\n      Drug B, N = 1021\n    Age\n46 (37, 59)\n48 (39, 56)¬†¬†¬†¬†Unknown\n7\n4Marker Level (ng/mL)\n0.84 (0.24, 1.57)\n0.52 (0.19, 1.20)¬†¬†¬†¬†Unknown\n6\n4T Stage\n\n¬†¬†¬†¬†T1\n28 (29%)\n25 (25%)¬†¬†¬†¬†T2\n25 (26%)\n29 (28%)¬†¬†¬†¬†T3\n22 (22%)\n21 (21%)¬†¬†¬†¬†T4\n23 (23%)\n27 (26%)Grade\n\n¬†¬†¬†¬†I\n35 (36%)\n33 (32%)¬†¬†¬†¬†II\n32 (33%)\n36 (35%)¬†¬†¬†¬†III\n31 (32%)\n33 (32%)Tumor Response\n28 (29%)\n33 (34%)¬†¬†¬†¬†Unknown\n3\n4Patient Died\n52 (53%)\n60 (59%)Months to Death/Censor\n23.5 (17.4, 24.0)\n21.2 (14.6, 24.0)1 Median (IQR); n (%)\n    \n\n5. Report statistics inline\ninline_text()\nThe Drug B was significantly different from Drug A inline_text(t1, variable = trt, level = ‚ÄúDrug B‚Äù) ‚Ä¶\nWe often need to report the results from a table in the text of an R markdown report\n\n\ntrial2 <-\n  trial %>%\n  select(trt, marker, stage)\n\ntab1 <- tbl_summary(trial2, by = trt)\ntab1\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n    Marker Level (ng/mL)\n0.84 (0.24, 1.57)\n0.52 (0.19, 1.20)¬†¬†¬†¬†Unknown\n6\n4T Stage\n\n¬†¬†¬†¬†T1\n28 (29%)\n25 (25%)¬†¬†¬†¬†T2\n25 (26%)\n29 (28%)¬†¬†¬†¬†T3\n22 (22%)\n21 (21%)¬†¬†¬†¬†T4\n23 (23%)\n27 (26%)1 Median (IQR); n (%)\n    \n\nThe median (IQR) marker level in the Drug A and Drug B groups are 0.84 (0.24, 1.57) and 0.52 (0.19, 1.20), respectively.\n\n\nm1 <- glm(response ~ age + stage, trial, family = binomial(link = \"logit\"))\n\ntbl_m1 <- tbl_regression(m1, exponentiate = TRUE)\ntbl_m1\n\n\nCharacteristic\n      OR1\n      95% CI1\n      p-value\n    Age\n1.02\n1.00, 1.04\n0.091T Stage\n\n\n¬†¬†¬†¬†T1\n‚Äî\n‚Äî\n¬†¬†¬†¬†T2\n0.58\n0.24, 1.37\n0.2¬†¬†¬†¬†T3\n0.94\n0.39, 2.28\n0.9¬†¬†¬†¬†T4\n0.79\n0.33, 1.90\n0.61 OR = Odds Ratio, CI = Confidence Interval\n    \n\n1.02 (95% CI 1.00, 1.04; p=0.091)\nAge was not significantly associated with tumor response (OR 1.02; 95% CI 1.00 - 1.04; p=0.091)\nThe inline_text function has arguments for rounding the p-value (pvalue_fun) and the coefficients and confidence interval (estimate_fun). These default to the same rounding performed in the table, but can be modified when reporting inline.\nHowever, while {gtsummary} is a King in producing amazing tables, reporting statistical results has it‚Äôs own king, namely {report} package.\n6. themes()\nJAMA Theme\n\n\ntheme_gtsummary_journal(journal = \"jama\")\n#> Setting theme `JAMA`\ntheme_gtsummary_compact()\n\nlibrary(ISLR)    # for Auto dataset\nm <- lm(mpg ~ origin * horsepower, data = Auto)\n\ntbl_regression(m) %>% \n  modify_caption(\"Compact theme\")\n\n\nTable 2: Compact theme\n  \n  Characteristic\n      Beta (95% CI)1\n      p-value\n    origin\n7.9 (5.6 to 10)\nhorsepower\n-0.06 (-0.09 to -0.03)\norigin * horsepower\n-0.06 (-0.09 to -0.04)\n1 CI = Confidence Interval\n    \n\nreset_gtsummary_theme()\n\n\n7. Survey Data\nThe {gtsummary} package also supports survey data (objects created with the {survey} package) via the tbl_svysummary() function. The syntax for tbl_svysummary() and tbl_summary() are nearly identical, and the examples above apply to survey summaries as well.\n\n\n# loading the api data set\ndata(api, package = \"survey\")\n\nsvy_apiclus1 <- \n  survey::svydesign(\n    id = ~dnum, \n    weights = ~pw, \n    data = apiclus1, \n    fpc = ~fpc\n  ) \n\nsvy_apiclus1 %>%\n  tbl_svysummary(\n    # stratify summary statistics by the \"both\" column\n    by = both, \n    # summarize a subset of the columns\n    include = c(api00, api99, both),\n    # adding labels to table\n    label = list(api00 ~ \"API in 2000\",\n                 api99 ~ \"API in 1999\")\n  ) %>%\n  add_p() %>%   # comparing values by \"both\" column\n  add_overall() %>%\n  # adding spanning header\n  modify_spanning_header(c(\"stat_1\", \"stat_2\") ~ \"**Met Both Targets**\")\n\n\nCharacteristic\n      Overall, N = 6,1941\n      \n        Met Both Targets\n      \n      p-value2\n    No, N = 1,6921\n      Yes, N = 4,5021\n    API in 2000\n652 (552, 718)\n631 (556, 710)\n654 (551, 722)\n0.4API in 1999\n615 (512, 691)\n632 (548, 698)\n611 (497, 686)\n0.21 Median (IQR)\n    2 Wilcoxon rank-sum test for complex survey samples\n    \n\ntbl_svysummary() can also handle weighted survey data where each row represents several individuals:\n\n\nTitanic %>%\n  as_tibble() %>%\n  survey::svydesign(data = ., ids = ~ 1, weights = ~ n) %>%\n  tbl_svysummary(include = c(Age, Survived))\n\n\nCharacteristic\n      N = 2,2011\n    Age\n¬†¬†¬†¬†Adult\n2,092 (95%)¬†¬†¬†¬†Child\n109 (5.0%)Survived\n711 (32%)1 n (%)\n    \n\n8. Some potentially useful things\nCross Tables with tbl_cross()\nAutomatically adds a spanning header to your table with the name or label of your comparison variable.\nUses percent = ‚Äúcell‚Äù by default.\nAdds row and column margin totals (customizable through the margin argument).\nDisplays missing data in both row and column variables (customizable through the missing argument).\n\n\ntrial %>%\n  tbl_cross(\n    row = stage,\n    col = trt,\n    percent = \"cell\", # or column, row, none\n    missing = \"ifany\", \n    #margin = NULL # or column or row, to supress totals\n  ) %>%\n  add_p()\n\n\n\n      \n        Chemotherapy Treatment\n      \n      Total\n      p-value1\n    Drug A\n      Drug B\n    T Stage\n\n\n\n0.9¬†¬†¬†¬†T1\n28 (14%)\n25 (12%)\n53 (26%)\n¬†¬†¬†¬†T2\n25 (12%)\n29 (14%)\n54 (27%)\n¬†¬†¬†¬†T3\n22 (11%)\n21 (10%)\n43 (22%)\n¬†¬†¬†¬†T4\n23 (12%)\n27 (14%)\n50 (25%)\nTotal\n98 (49%)\n102 (51%)\n200 (100%)\n1 Pearson's Chi-squared test\n    \n\nCustom functions\n\n\nrmanova_common_variance <- function(data, variable, by, ...) {\n  data <- data[c(variable, by)] %>% dplyr::filter(complete.cases(.))\n  t.test(data[[variable]] ~ factor(data[[by]]), var.equal = TRUE) %>%\n  broom::tidy()\n}\n\ntrial[c(\"age\", \"trt\")] %>%\n  tbl_summary(by = trt) %>%\n  add_p(test = age ~ \"rmanova_common_variance\")\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    Age\n46 (37, 59)\n48 (39, 56)\n0.8¬†¬†¬†¬†Unknown\n7\n4\n1 Median (IQR)\n    2 Two Sample t-test\n    \n\n\n\nfriedman_common_variance <- function(data, variable, by, random, ...) {\n  data <- data[c(variable, by)] %>% dplyr::filter(complete.cases(.))\n  friedman.test(variable ~ factor(data[[by]]) | random, data = data)\n\n  #t.test(data[[variable]] ~ factor(data[[by]]), var.equal = TRUE) %>%\n  broom::tidy()\n}\n\nwb %>%\n  tbl_summary(by = w, include = -t) %>%\n  add_p(test = x ~ \"friedman_common_variance\", random = t)\n\n\nSummarize a continuous variable by one or more categorical variables\n\n\ntbl_continuous(\n    data = trial,\n    variable = age,\n    by = trt,\n    include = grade\n)\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n    Grade\n\n¬†¬†¬†¬†I\n46 (36, 60)\n48 (42, 55)¬†¬†¬†¬†II\n44 (31, 54)\n50 (43, 57)¬†¬†¬†¬†III\n52 (42, 60)\n45 (36, 52)1 Age: Median (IQR)\n    \n\n\n\ntrial %>%\n    tbl_custom_summary(\n        include = c(\"stage\", \"grade\"),\n        by = \"trt\",\n        stat_fns = ~ gtsummary::continuous_summary(\"age\"),\n        statistic = ~ \"{median} [{p25}-{p75}]\",\n        overall_row = TRUE,\n        overall_row_label = \"All stages & grades\"\n    ) %>%\n    modify_footnote(\n        update = all_stat_cols() ~ \"Median age (IQR)\"\n    )\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n    All stages & grades\n46 [37-59]\n48 [39-56]T Stage\n\n¬†¬†¬†¬†T1\n43 [31-53]\n47 [43-57]¬†¬†¬†¬†T2\n48 [42-62]\n49 [42-53]¬†¬†¬†¬†T3\n48 [38-60]\n53 [40-59]¬†¬†¬†¬†T4\n46 [36-60]\n45 [38-54]Grade\n\n¬†¬†¬†¬†I\n46 [36-60]\n48 [42-55]¬†¬†¬†¬†II\n44 [31-54]\n50 [43-57]¬†¬†¬†¬†III\n52 [42-60]\n45 [36-52]1 Median age (IQR)\n    \n\nPlot estimate (experimental, the forest plot is much better for now)\n\n\nglm(response ~ marker + grade, trial, family = binomial) %>%\n  tbl_regression(\n    add_estimate_to_reference_rows = TRUE,\n    exponentiate = TRUE\n  ) %>%\n  plot()\n\n\n\ntbl_survfit - Creates table of survival probabilities\n\n\nlibrary(survival)\nt1 <- tbl_survfit(\n  list(\n    survfit(Surv(ttdeath, death) ~ 1, trial),\n    survfit(Surv(ttdeath, death) ~ trt + grade, trial)),\n    times = c(12, 24),\n    label_header = \"**{time} Month**\") \n\nt2 <- tbl_survfit(\n  trial, y = Surv(ttdeath, death),\n  include = c(trt, grade),\n  probs = 0.5,\n  label_header = \"**Median Survival**\"\n  ) %>% \n  add_p()\n\ntbl_merge(list(t1,t2), tab_spanner = FALSE)\n\n\nCharacteristic\n      12 Month\n      24 Month\n      Median Survival\n      p-value1\n    Overall\n88% (84%, 93%)\n44% (38%, 51%)\n\nChemotherapy Treatment\n\n\n\n0.2¬†¬†¬†¬†Drug A, grade=I  \n97% (92%, 100%)\n54% (40%, 74%)\n\n¬†¬†¬†¬†Drug A, grade=II \n88% (77%, 100%)\n50% (35%, 71%)\n\n¬†¬†¬†¬†Drug A, grade=III\n87% (76%, 100%)\n35% (22%, 57%)\n\n¬†¬†¬†¬†Drug B, grade=I  \n97% (91%, 100%)\n48% (34%, 69%)\n\n¬†¬†¬†¬†Drug B, grade=II \n78% (65%, 93%)\n44% (31%, 64%)\n\n¬†¬†¬†¬†Drug B, grade=III\n85% (73%, 98%)\n30% (18%, 51%)\n\n¬†¬†¬†¬†Drug A\n\n\n24 (21, ‚Äî)\n¬†¬†¬†¬†Drug B\n\n\n21 (18, ‚Äî)\nGrade\n\n\n\n0.072¬†¬†¬†¬†I\n\n\n‚Äî (22, ‚Äî)\n¬†¬†¬†¬†II\n\n\n22 (18, ‚Äî)\n¬†¬†¬†¬†III\n\n\n20 (18, 23)\n1 Log-rank test\n    \n\nMedian survival\n\n\nlibrary(survival)\ntbl_survfit(\n  trial, y = Surv(ttdeath, death),\n  include = c(trt, grade),\n  probs = 0.5,\n  label_header = \"**Median Survival**\"\n  ) %>% \n  add_p()\n\n\nCharacteristic\n      Median Survival\n      p-value1\n    Chemotherapy Treatment\n\n0.2¬†¬†¬†¬†Drug A\n24 (21, ‚Äî)\n¬†¬†¬†¬†Drug B\n21 (18, ‚Äî)\nGrade\n\n0.072¬†¬†¬†¬†I\n‚Äî (22, ‚Äî)\n¬†¬†¬†¬†II\n22 (18, ‚Äî)\n¬†¬†¬†¬†III\n20 (18, 23)\n1 Log-rank test\n    \n\n9. Further readings and references\nMain page of a package with most of the info you found here: https://www.danieldsjoberg.com/gtsummary/index.html\nCheat sheet: file:///Users/zablotski/Downloads/gtsummary.pdf\nThe R Journal Article\nHere is a very informative video of {gtsummary} creator - Daniel Sjoberg: https://www.youtube.com/watch?v=tANo9E1SYJE&t=2s&ab_channel=DanielSjoberg\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-10-31-gtsummary/thumbnail_gtsummary.png",
    "last_modified": "2022-11-29T18:48:50+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-11-25-gtsummary2/",
    "title": "Publication-Ready Tables of Particular Statistical Tests and Models with {gtsummary}",
    "description": "Find a review of incredibly useful {gtsummary} package in a separate blog-post. Here I'll just collect all the possible Statsitcal Tests and Models, {gtsummary} can help with.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-11-29",
    "categories": [
      "videos",
      "statistics",
      "models",
      "machine learning"
    ],
    "contents": "\n\nContents\nQuick review of the package as a video\nIndividual Statistical tests with p-values\nWilcoxon rank sum test for independent samples\nStudent‚Äôs Two Sample t-test for independent samples with equal variances\nWelch Two Sample t-test for independent samples with unequal variances\nPaired t-test\nPaired Wilcoxon test\nKruskal-wallis test\nFisher‚Äôs ANOVA\nWelsh‚Äôs ANOVA (not implemented yet: Nov.¬†2022)\nRepeated Measures ANOVA (not implemented yet: Nov.¬†2022)\nFriedman test (not implemented yet: Nov.¬†2022)\nPearson‚Äôs Chi-squared test\nFisher‚Äôs test\nMcNemar‚Äôs Chi-squared test with continuity correction\nshort table format\nlong table format\n\nMcNemar‚Äôs Chi-squared test without continuity correction\nProportion test: 2-sample test for equality of proportions with continuity correction\n\nIndividual models\nLinear regression\nLogistic regression\nCox Proportional Hazards Regression\nUnivariate Cox models\n\nMixed effects models\nMultinomial models\nemmeans - or sort of - simply pairwise tests\n\nFurther readings and references\n\nQuick review of the package as a video\n\n\n\n\n\n\n\nIndividual Statistical tests with p-values\nWilcoxon rank sum test for independent samples\n\n\nlibrary(gtsummary)\ntrial %>% \n  select(trt, age) %>% \n  tbl_summary(by = trt) %>% \n  add_p()\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    Age\n46 (37, 59)\n48 (39, 56)\n0.7¬†¬†¬†¬†Unknown\n7\n4\n1 Median (IQR)\n    2 Wilcoxon rank sum test\n    \n\nStudent‚Äôs Two Sample t-test for independent samples with equal variances\n\n\ntrial %>% \n  select(trt, age) %>% \n  tbl_summary(\n    by = trt, \n    statistic = age ~ \"{mean} ({sd})\",\n    missing = \"no\") %>% \n  add_n() %>% \n  add_p(age ~ \"t.test\", test.args = age ~ list(var.equal = TRUE))\n\n\nCharacteristic\n      N\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    Age\n189\n47 (15)\n47 (14)\n0.81 Mean (SD)\n    2 Two Sample t-test\n    \n\nWelch Two Sample t-test for independent samples with unequal variances\n\n\ntrial %>% \n  select(trt, age) %>% \n    tbl_summary(\n    by = trt, \n    statistic = age ~ \"{mean} ({sd})\") %>%\n  add_p(age ~ \"t.test\")\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    Age\n47 (15)\n47 (14)\n0.8¬†¬†¬†¬†Unknown\n7\n4\n1 Mean (SD)\n    2 Welch Two Sample t-test\n    \n\nPaired t-test\n\n\n# install.packages(\"BSDA\")       # for Fitness data\nlibrary(BSDA)\n\nFitness %>%\n  tbl_summary(\n    by = test, \n    include = -subject,\n    statistic = number ~ \"{mean} ({sd})\") %>%\n  add_p(test = number ~ \"paired.t.test\", group = subject)\n\n\nCharacteristic\n      After, N = 91\n      Before, N = 91\n      p-value2\n    number\n25 (6)\n23 (7)\n0.0251 Mean (SD)\n    2 Paired t-test\n    \n\n# prove\n# make wide format\nd <- Fitness %>% \n  pivot_wider(\n    id_cols     = subject, \n    names_from  = test, \n    values_from = number) %>% \n  mutate(difference =  After - Before)\n\nt.test(d$Before, d$After, paired = T)\n\n\n    Paired t-test\n\ndata:  d$Before and d$After\nt = -2.753, df = 8, p-value = 0.02494\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -3.6752732 -0.3247268\nsample estimates:\nmean difference \n             -2 \n\nggwithinstats(\n  data = Fitness,\n  x    = test, \n  y    = number, \n  type = \"parametric\"\n)\n\n\n\nPaired Wilcoxon test\n\n\n# install.packages(\"BSDA\")       # for Fitness data\nlibrary(BSDA)\n\nFitness %>%\n  tbl_summary(\n    by = test, \n    include = -subject) %>%\n    add_p(test = number ~ \"paired.wilcox.test\", group = subject, test.args = number ~ list(exact = F))\n\n\nCharacteristic\n      After, N = 91\n      Before, N = 91\n      p-value2\n    number\n26 (19, 30)\n24 (18, 28)\n0.0471 Median (IQR)\n    2 Wilcoxon signed rank test with continuity correction\n    \n\n# prove\nwilcox.test(d$Before, d$After, paired = T, exact = F)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  d$Before and d$After\nV = 3.5, p-value = 0.04688\nalternative hypothesis: true location shift is not equal to 0\n\nggwithinstats(\n  data = Fitness,\n  x    = test, \n  y    = number, \n  type = \"np\"\n)\n\n\n\nKruskal-wallis test\n\n\ntrial %>% \n  select(age, stage) %>% \n  tbl_summary(by = stage) %>% \n  add_p()\n\n\nCharacteristic\n      T1, N = 531\n      T2, N = 541\n      T3, N = 431\n      T4, N = 501\n      p-value2\n    Age\n45 (36, 56)\n48 (42, 55)\n50 (39, 60)\n46 (37, 55)\n0.6¬†¬†¬†¬†Unknown\n2\n1\n2\n6\n1 Median (IQR)\n    2 Kruskal-Wallis rank sum test\n    \n\nggbetweenstats(trial, stage, age, type = \"np\")\n\n\n\nFisher‚Äôs ANOVA\n\n\ntrial %>% \n  select(age, stage) %>% \n  tbl_summary(\n    by = stage, \n    statistic = age ~ \"mean ({sd})\") %>% \n  add_p(\n    age ~ \"aov\", \n    pvalue_fun = ~ style_pvalue(.x, digits = 2),\n    test.args = age ~ list(var.equal = T))\n\n\nCharacteristic\n      T1, N = 531\n      T2, N = 541\n      T3, N = 431\n      T4, N = 501\n      p-value2\n    Age\nmean (15)\nmean (13)\nmean (14)\nmean (16)\n0.48¬†¬†¬†¬†Unknown\n2\n1\n2\n6\n1 mean (SD)\n    2 One-way ANOVA\n    \n\nggbetweenstats(trial, stage, age, var.equal = T)\n\n\n\nWelsh‚Äôs ANOVA (not implemented yet: Nov.¬†2022)\n\n\ntrial %>% \n  select(age, stage) %>% \n  tbl_summary(\n    by = stage, \n    statistic = age ~ \"mean ({sd})\") %>% \n  add_p(\n    age ~ \"aov\", \n    pvalue_fun = ~ style_pvalue(.x, digits = 2),\n    test.args = age ~ list(var.equal = F))\n\n\nCharacteristic\n      T1, N = 531\n      T2, N = 541\n      T3, N = 431\n      T4, N = 501\n      p-value2\n    Age\nmean (15)\nmean (13)\nmean (14)\nmean (16)\n0.48¬†¬†¬†¬†Unknown\n2\n1\n2\n6\n1 mean (SD)\n    2 One-way ANOVA\n    \n\nggbetweenstats(trial, stage, age, var.equal = F)\n\n\n\nRepeated Measures ANOVA (not implemented yet: Nov.¬†2022)\nFriedman test (not implemented yet: Nov.¬†2022)\nPearson‚Äôs Chi-squared test\n\n\ntrial %>% \n  select(trt, stage) %>% \n  tbl_summary(by = stage) %>% \n  add_p(pvalue_fun = ~ style_pvalue(.x, digits = 3))\n\n\nCharacteristic\n      T1, N = 531\n      T2, N = 541\n      T3, N = 431\n      T4, N = 501\n      p-value2\n    Chemotherapy Treatment\n\n\n\n\n0.866¬†¬†¬†¬†Drug A\n28 (53%)\n25 (46%)\n22 (51%)\n23 (46%)\n¬†¬†¬†¬†Drug B\n25 (47%)\n29 (54%)\n21 (49%)\n27 (54%)\n1 n (%)\n    2 Pearson's Chi-squared test\n    \n\nggbarstats(trial, trt, stage, label = \"both\")\n\n\n\nFisher‚Äôs test\nIf number of observations in the categorical variable are <5, it takes Fisher‚Äôs test automatically, which is kind of genius.\n\n\nmtcars %>% \n  select(am, cyl) %>% \n  tbl_summary(by = am) %>% \n  add_p()\n\n\nCharacteristic\n      0, N = 191\n      1, N = 131\n      p-value2\n    cyl\n\n\n0.009¬†¬†¬†¬†4\n3 (16%)\n8 (62%)\n¬†¬†¬†¬†6\n4 (21%)\n3 (23%)\n¬†¬†¬†¬†8\n12 (63%)\n2 (15%)\n1 n (%)\n    2 Fisher's exact test\n    \n\nBut we can force the function calculate Fisher‚Äôs test with >5 numbers too.\n\n\ntrial %>% \n  select(trt, stage) %>%\n  tbl_summary(by = trt) %>% \n  add_p(\n    stage ~ \"fisher.test\", \n    pvalue_fun = ~ style_pvalue(.x, digits = 2))\n\n\nCharacteristic\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    T Stage\n\n\n0.87¬†¬†¬†¬†T1\n28 (29%)\n25 (25%)\n¬†¬†¬†¬†T2\n25 (26%)\n29 (28%)\n¬†¬†¬†¬†T3\n22 (22%)\n21 (21%)\n¬†¬†¬†¬†T4\n23 (23%)\n27 (26%)\n1 n (%)\n    2 Fisher's exact test\n    \n\nMcNemar‚Äôs Chi-squared test with continuity correction\nshort table format\n\n\nset.seed(9) # for reproducibility \n\ndata2 <- data.frame(\n  id     = c(1:20),\n  before = sample(c(\"Yes\", \"No\", \"Yes\", \"Yes\"), 20, replace = TRUE),\n  after  = sample(c(\"No\", \"Yes\", \"No\", \"No\"), 20, replace = TRUE))\n\n\ndata2 %>% \n  select(-id) %>% \n  tbl_cross(row = before, col = after) %>% \n  add_p(\"mcnemar.test.wide\")\n\n\n\n      \n        after\n      \n      Total\n      p-value1\n    No\n      Yes\n    before\n\n\n\n0.046¬†¬†¬†¬†No\n4\n1\n5\n¬†¬†¬†¬†Yes\n8\n7\n15\nTotal\n12\n8\n20\n1 McNemar's Chi-squared test with continuity correction\n    \n\nmcnemar.test(x = data2$before, y = data2$after, correct = T)\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  data2$before and data2$after\nMcNemar's chi-squared = 4, df = 1, p-value = 0.0455\n\nlong table format\n\n\ndata <- data2 %>% \n  pivot_longer(cols = 2:3, names_to = \"time\", values_to = \"result\")\n\ndata %>%\n  tbl_summary(\n    by = time, \n    include = result,\n    type = list(result ~ \"categorical\")) %>%\n  add_p(\n    everything() ~ \"mcnemar.test\", \n    group = id,\n    pvalue_fun = ~style_pvalue(., digits = 3)\n  ) \n\n\nCharacteristic\n      after, N = 201\n      before, N = 201\n      p-value2\n    result\n\n\n0.046¬†¬†¬†¬†No\n12 (60%)\n5 (25%)\n¬†¬†¬†¬†Yes\n8 (40%)\n15 (75%)\n1 n (%)\n    2 McNemar's Chi-squared test with continuity correction\n    \n\nMcNemar‚Äôs Chi-squared test without continuity correction\n\n\ndata2 %>% \n  select(-id) %>% \n  tbl_cross(row = before, col = after) %>% \n  add_p(\"mcnemar.test.wide\", test.args=list(correct = F))\n\n\n\n      \n        after\n      \n      Total\n      p-value1\n    No\n      Yes\n    before\n\n\n\n0.020¬†¬†¬†¬†No\n4\n1\n5\n¬†¬†¬†¬†Yes\n8\n7\n15\nTotal\n12\n8\n20\n1 McNemar's Chi-squared test\n    \n\nmcnemar.test(x = data2$before, y = data2$after, correct = F)\n\n\n    McNemar's Chi-squared test\n\ndata:  data2$before and data2$after\nMcNemar's chi-squared = 5.4444, df = 1, p-value = 0.01963\n\nggbarstats(\n  data = data2, \n  x = before,\n  y = after, \n  paired = T\n)\n\n\n\nProportion test: 2-sample test for equality of proportions with continuity correction\n\n\ntrial %>%\n  select(response, death, trt) %>%\n  tbl_summary(by = trt, missing = \"no\") %>%\n  add_p(\n    test = everything() ~ \"prop.test\", \n    pvalue_fun = ~style_pvalue(.x, digits = 3)) %>%\n  add_n() \n\n\nCharacteristic\n      N\n      Drug A, N = 981\n      Drug B, N = 1021\n      p-value2\n    Tumor Response\n193\n28 (29%)\n33 (34%)\n0.637Patient Died\n200\n52 (53%)\n60 (59%)\n0.4981 n (%)\n    2 Two sample test for equality of proportions\n    \n\nstats::prop.test(table(trial$trt, trial$response))\n\n\n    2-sample test for equality of proportions with continuity\n    correction\n\ndata:  table(trial$trt, trial$response)\nX-squared = 0.22329, df = 1, p-value = 0.6365\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.0993642  0.1833599\nsample estimates:\n   prop 1    prop 2 \n0.7052632 0.6632653 \n\nstats::prop.test(table(trial$trt, trial$death))\n\n\n    2-sample test for equality of proportions with continuity\n    correction\n\ndata:  table(trial$trt, trial$death)\nX-squared = 0.45996, df = 1, p-value = 0.4976\nalternative hypothesis: two.sided\n95 percent confidence interval:\n -0.08980379  0.20504989\nsample estimates:\n   prop 1    prop 2 \n0.4693878 0.4117647 \n\nIndividual models\nLinear regression\n\n\nm <- glm(age ~ trt + grade, trial, family = gaussian)\n\nt1 <- tbl_regression(m) %>%\n  add_glance_table()\n\nt1\n\n\nCharacteristic\n      Beta\n      95% CI1\n      p-value\n    Chemotherapy Treatment\n\n\n¬†¬†¬†¬†Drug A\n‚Äî\n‚Äî\n¬†¬†¬†¬†Drug B\n0.39\n-3.7, 4.5\n0.9Grade\n\n\n¬†¬†¬†¬†I\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n1.4\n-3.6, 6.4\n0.6¬†¬†¬†¬†III\n1.9\n-3.1, 7.0\n0.4Null deviance\n38,508\n\nNull df\n188\n\nLog-likelihood\n-770\n\nAIC\n1,551\n\nBIC\n1,567\n\nDeviance\n38,371\n\nResidual df\n185\n\nNo. Obs.\n189\n\n1 CI = Confidence Interval\n    \n\nLogistic regression\n\n\nm <- arm::bayesglm(response ~ trt + age + grade, trial, family = binomial)\n\nt2 <- tbl_regression(m, exponentiate = TRUE) %>% \n  add_vif()\n\nt2\n\n\nCharacteristic\n      OR1\n      95% CI1\n      p-value\n      GVIF1\n      Adjusted GVIF2,1\n    Chemotherapy Treatment\n\n\n\n1.0\n1.0¬†¬†¬†¬†Drug A\n‚Äî\n‚Äî\n\n\n¬†¬†¬†¬†Drug B\n1.13\n0.60, 2.13\n0.7\n\nAge\n1.02\n1.00, 1.04\n0.10\n1.0\n1.0Grade\n\n\n\n1.0\n1.0¬†¬†¬†¬†I\n‚Äî\n‚Äî\n\n\n¬†¬†¬†¬†II\n0.86\n0.39, 1.85\n0.7\n\n¬†¬†¬†¬†III\n1.01\n0.47, 2.15\n>0.9\n\n1 OR = Odds Ratio, CI = Confidence Interval, GVIF = Generalized Variance Inflation Factor\n    2 GVIF^[1/(2*df)]\n    \n\nCox Proportional Hazards Regression\n\n\nlibrary(survival)\ncox_m <- coxph(Surv(ttdeath, death) ~ age + marker + grade, data = trial)\ntbl_regression(cox_m, exponentiate = TRUE)\n\n\nCharacteristic\n      HR1\n      95% CI1\n      p-value\n    Age\n1.01\n0.99, 1.02\n0.4Marker Level (ng/mL)\n0.95\n0.75, 1.21\n0.7Grade\n\n\n¬†¬†¬†¬†I\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n1.11\n0.66, 1.87\n0.7¬†¬†¬†¬†III\n1.69\n1.04, 2.73\n0.0321 HR = Hazard Ratio, CI = Confidence Interval\n    \n\nNote the sensible defaults with this basic usage (that can be customized later):\nThe model was recognized as logistic regression with coefficients exponentiated, so the header displayed ‚ÄúOR‚Äù for odds ratio.\nVariable types are automatically detected and reference rows are added for categorical variables.\nModel estimates and confidence intervals are rounded and formatted.\nBecause the variables in the data set were labelled, the labels were carried through into the {gtsummary} output table. Had the data not been labelled, the default is to display the variable name.\nVariable levels are indented and footnotes added.\nUnivariate Cox models\n\n\ncox_uv_m <- trial %>%\n  select(ttdeath, death, age, marker, grade) %>%\n  tbl_uvregression(\n    method = coxph,\n    y = Surv(ttdeath, death), \n    exponentiate = TRUE,\n    hide_n = TRUE\n  ) %>%\n  add_nevent(location = \"level\")\n\ncox_uv_m\n\n\nCharacteristic\n      Event N\n      HR1\n      95% CI1\n      p-value\n    Age\n103\n1.01\n0.99, 1.02\n0.3Marker Level (ng/mL)\n104\n0.91\n0.72, 1.15\n0.4Grade\n\n\n\n¬†¬†¬†¬†I\n33\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n36\n1.28\n0.80, 2.05\n0.3¬†¬†¬†¬†III\n43\n1.69\n1.07, 2.66\n0.0241 HR = Hazard Ratio, CI = Confidence Interval\n    \n\n\n\nuni_multi <- tbl_merge(\n    tbls        = list(\n      tbl_summary(\n        trial %>%\n          select(age, marker, grade)), \n      cox_uv_m, \n      tbl_regression(cox_m, exponentiate = TRUE)),\n    tab_spanner = c(\"**Describe**\", \"**Univariate Models**\", \"**Multivariate Model**\")\n  )\n\nuni_multi\n\n\nCharacteristic\n      \n        Describe\n      \n      \n        Univariate Models\n      \n      \n        Multivariate Model\n      \n    N = 2001\n      Event N\n      HR2\n      95% CI2\n      p-value\n      HR2\n      95% CI2\n      p-value\n    Age\n47 (38, 57)\n103\n1.01\n0.99, 1.02\n0.3\n1.01\n0.99, 1.02\n0.4¬†¬†¬†¬†Unknown\n11\n\n\n\n\n\n\nMarker Level (ng/mL)\n0.64 (0.22, 1.39)\n104\n0.91\n0.72, 1.15\n0.4\n0.95\n0.75, 1.21\n0.7¬†¬†¬†¬†Unknown\n10\n\n\n\n\n\n\nGrade\n\n\n\n\n\n\n\n¬†¬†¬†¬†I\n68 (34%)\n33\n‚Äî\n‚Äî\n\n‚Äî\n‚Äî\n¬†¬†¬†¬†II\n68 (34%)\n36\n1.28\n0.80, 2.05\n0.3\n1.11\n0.66, 1.87\n0.7¬†¬†¬†¬†III\n64 (32%)\n43\n1.69\n1.07, 2.66\n0.024\n1.69\n1.04, 2.73\n0.0321 Median (IQR); n (%)\n    2 HR = Hazard Ratio, CI = Confidence Interval\n    \n\nMixed effects models\n\n\nlme4::lmer(age ~ marker + stage + (1|grade), trial) %>%\n  tbl_regression()\n\n\nCharacteristic\n      Beta\n      95% CI1\n    Marker Level (ng/mL)\n-0.20\n-2.7, 2.3T Stage\n\n¬†¬†¬†¬†T1\n‚Äî\n‚Äî¬†¬†¬†¬†T2\n1.3\n-4.5, 7.1¬†¬†¬†¬†T3\n2.9\n-3.4, 9.2¬†¬†¬†¬†T4\n-1.7\n-7.7, 4.31 CI = Confidence Interval\n    \n\nMultinomial models\n\n\nlibrary(nnet)\nm <- multinom(grade ~ age, trial)\n\n# weights:  9 (4 variable)\ninitial  value 207.637723 \nfinal  value 207.207693 \nconverged\n\ntbl_regression(m)\n\n\nCharacteristic\n      log(OR)1\n      95% CI1\n      p-value\n    II\n    Age\n0.01\n-0.02, 0.03\n0.6III\n    Age\n0.01\n-0.01, 0.03\n0.41 OR = Odds Ratio, CI = Confidence Interval\n    \n\nemmeans - or sort of - simply pairwise tests\n\n\n# table summarizing data with no p-values\nsmall_trial <- trial %>% select(grade, age, response)\nt0 <- small_trial %>%\n  tbl_summary(by = grade, missing = \"no\") %>%\n  modify_header(all_stat_cols() ~ \"**{level}**\")\n\nlibrary(gt)\n# table comparing grade I and II\nt1 <- small_trial %>%\n  filter(grade %in% c(\"I\", \"II\")) %>%\n  tbl_summary(by = grade, missing = \"no\") %>%\n  add_p() %>%\n  modify_header(p.value ~ md(\"**I vs. II**\")) %>%\n  # hide summary stat columns\n  modify_column_hide(all_stat_cols())\n\n# table comparing grade I and III\nt2 <- small_trial %>%\n  filter(grade %in% c(\"I\", \"III\")) %>%\n  tbl_summary(by = grade, missing = \"no\") %>%\n  add_p()  %>%\n  modify_header(p.value ~ md(\"**I vs. III**\")) %>%\n  # hide summary stat columns\n  modify_column_hide(all_stat_cols())\n\n# table comparing grade II and III\nt3 <- small_trial %>%\n  filter(grade %in% c(\"II\", \"III\")) %>%\n  tbl_summary(by = grade, missing = \"no\") %>%\n  add_p()  %>%\n  modify_header(p.value ~ md(\"**II vs. III**\")) %>%\n  # hide summary stat columns\n  modify_column_hide(all_stat_cols())\n\n# merging the 3 tables together, and adding additional gt formatting\ntbl_merge(list(t0, t1, t2, t3)) %>%\n  modify_spanning_header(\n    list(\n      all_stat_cols() ~ \"**Tumor Grade**\",\n      starts_with(\"p.value\") ~ \"**p-values**\"\n    )\n  )\n\n\nCharacteristic\n      \n        Tumor Grade\n      \n      \n        p-values\n      \n    I1\n      II1\n      III1\n      I vs. II2\n      I vs. III2\n      II vs. III2\n    Age\n47 (37, 56)\n48 (37, 57)\n47 (38, 58)\n0.7\n0.5\n0.8Tumor Response\n21 (31%)\n19 (30%)\n21 (33%)\n>0.9\n0.9\n0.81 Median (IQR); n (%)\n    2 Wilcoxon rank sum test; Fisher's exact test\n    \n\n\n\nlibrary(broom.helpers)\nmod <- glm(response ~ stage + grade, data = trial, family = binomial)\n\nmod %>%\n  tidy_plus_plus(\n    add_pairwise_contrasts = TRUE, \n    exponentiate = TRUE\n  ) %>%\n  knitr::kable()\n\n\nFurther readings and references\nMain page of a package with most of the info you found here: https://www.danieldsjoberg.com/gtsummary/index.html\nCheat sheet: file:///Users/zablotski/Downloads/gtsummary.pdf\nThe R Journal Article\nHere is a very informative video of {gtsummary} creator - Daniel Sjoberg: https://www.youtube.com/watch?v=tANo9E1SYJE&t=2s&ab_channel=DanielSjoberg\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-11-25-gtsummary2/thumbnail_gtsummary2.png",
    "last_modified": "2022-11-29T18:58:28+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-10-27-rsample/",
    "title": "R package reviews {rsample} Effective Resampling for Machine Learning!",
    "description": "Let's learn how to use three most important resampling techniques: train-test split, cross-validation and bootstrapping. We'll start with the question...",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-11-06",
    "categories": [
      "videos",
      "statistics",
      "models",
      "machine learning",
      "tidymodels",
      "R package reviews"
    ],
    "contents": "\n\nContents\nThis post as a video\nWhy do we actually need resampling?\nTraining and Testing sets\nRandom Resampling\nStratified Resampling\n‚Äúbreaks‚Äù argument\nHow to test the model\n\nValidation\nClassic-Validation\nV-Fold Cross-Validation\nvfold_cv() function\nworkflow() for regression model\nV-Fold Cross-Validation with ‚Äúrepeats‚Äù\nworkflow() for classification problem with random forest\nOther validation techniques\n\nBootstrap Resampling\nGrouped Resampling\nFinal thoughts\nFurther readings and references\n\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs only 14 minutes long.\n\n\n\n\n\n\n\nWhy do we actually need resampling?\nThe short answer is - resampling allows to create a useful model which helps to confidently answer any question with data. For example, if we want to know ‚ÄúWhich cars are more efficient, American, European or Japanese?‚Äù we could take all 392 cars from Auto dataset and model miles per gallon for all 3 origins. Plotting predictions reveals that American cars are the least efficient, while Japanese are the most efficient. However, we can‚Äôt fully trust those predictions yet, because predictions which are based on the whole dataset can only reflect what the model already knows ü§ì. While, if our model is somehow biased, overfitted or the model-assumptions are violated, our predictions might be just wrong. So, we need to somehow test the quality of those predictions.\n\n\nlibrary(tidymodels)\ntheme_set(theme_test())\nlibrary(ISLR)    # for Auto dataset\n\ndim(Auto)\n\n[1] 392   9\n\nauto <- Auto %>% \n  mutate(origin = case_when(\n    origin == 1 ~ \"American\", \n    origin == 2 ~ \"European\", \n    origin == 3 ~ \"Japanese\" ))\n\nm <- lm(mpg ~ origin, auto)\n\nlibrary(sjPlot)   # I made a video on {sjPlot} üì¶\n\nplot_model(m, type = \"pred\")\n\n$origin\n\n\nTraining and Testing sets\n\nRandom Resampling\nOne way to validate our model is to split our data into two distinct datasets, the training set and the test set. That would allow you to develop and optimize the model with training dataset, and use test dataset to determine the efficiency of our model. The function initial_split() from {rsample} package splits data for you. Your just give it the name of your dataset and the proportion you want to allocate into the training set. 80% is usually a good choice. The initial_split() function then uses random sampling to get 80% of your data and if you need those 80% to be reproducible, use set.seed() function with the number of your choise before splitting. After the initial_split(), the training() and testing() functions return the actual data sets.\n\n\nlibrary(rsample)\n\nset.seed(10)  # for reproducibility\nauto_split <- initial_split(auto, prop = 0.80)\n\nauto_train <- training(auto_split)\nauto_test  <- testing(auto_split)\nauto_test %>% glimpse()\n\nRows: 79\nColumns: 9\n$ mpg          <dbl> 18, 15, 18, 17, 14, 15, 22, 18, 25, 25, 16, 18,‚Ä¶\n$ cylinders    <dbl> 8, 8, 8, 8, 8, 8, 6, 6, 4, 4, 6, 6, 8, 6, 4, 4,‚Ä¶\n$ displacement <dbl> 307, 350, 318, 302, 455, 383, 198, 199, 110, 10‚Ä¶\n$ horsepower   <dbl> 130, 165, 150, 140, 225, 170, 95, 97, 87, 95, 1‚Ä¶\n$ weight       <dbl> 3504, 3693, 3436, 3449, 4425, 3563, 2833, 2774,‚Ä¶\n$ acceleration <dbl> 12.0, 11.5, 11.0, 10.5, 10.0, 10.0, 15.5, 15.5,‚Ä¶\n$ year         <dbl> 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 71, 71,‚Ä¶\n$ origin       <chr> \"American\", \"American\", \"American\", \"American\",‚Ä¶\n$ name         <fct> chevrolet chevelle malibu, buick skylark 320, p‚Ä¶\n\nStratified Resampling\nSuch simple random sampling is often good enough, but when some groups of a categorical variable are much more common than others, a simple random sample may accidentally misrepresent proportions of those groups in the training or test set. For example 17% of our cars are European, while 20% are Japanese. So, we have 3% more Japanese cars as compared with European. But when we look at the testing set, we‚Äôll see 5% less Japanese cars as compared with European cars, which is the opposite of what‚Äôs true.\n\n\nlibrary(janitor)       # I made a video on {janitor} üì¶\ntabyl(auto$origin)\n\n auto$origin   n   percent\n    American 245 0.6250000\n    European  68 0.1734694\n    Japanese  79 0.2015306\n\ntabyl(auto_test$origin)\n\n auto_test$origin  n   percent\n         American 46 0.5822785\n         European 19 0.2405063\n         Japanese 14 0.1772152\n\nTo avoid this, we can use strata =  argument to conduct stratified random sampling, where splitting into 80 and 20 percents is conducted separately within each group, and then these sub-samples are combined into the overall training and test set. In this way, the proportions inside of groups will be kept similar to the original data.\n\n\nset.seed(10)   # for reproducibility\nauto_split <- initial_split(auto, prop = 0.80, strata = origin)\n\nauto_train <- training(auto_split)\nauto_test  <- testing(auto_split)\n\ntabyl(auto_test$origin)\n\n auto_test$origin  n   percent\n         American 49 0.6202532\n         European 14 0.1772152\n         Japanese 16 0.2025316\n\nBut what if an important variable is not categorical, but numeric? In this case we need to keep it‚Äôs distributions similar between the training and test set. Because otherwise, if the distribution of an important variable is skewed, simple random resampling may accidentally skew your data even further. (mVertigo effect on the plot below with some strange sound). We don‚Äôt want that!\n\n\nggplot()+\n  geom_density(data = auto, aes(horsepower))\n\n\n\nFortunately, numeric data can also be easily stratified. The initial_split() function does it by artificially cutting (binning) numeric data into 4 quartiles and then uses stratified sampling for separate quartiles. For example, when we don‚Äôt stratify horsepower of cars (Funny Horse Picture with mVertigo Effect and Horse Whinny sound effect ;) ‚Ä¶ r√§uspern ‚Ä¶ sorry, the distribution of a training set (green) is similar to the original distribution (black), while the test set (red) differs in several locations.\n\n\nset.seed(10)   # for reproducibility\nauto_split <- initial_split(auto, prop = 0.80)\n\nauto_train <- training(auto_split)\nauto_test  <- testing(auto_split)\n\nggplot()+\n  geom_density(data = auto, aes(horsepower))+\n  geom_density(data = auto_train, aes(horsepower), color = \"green\")+\n  geom_density(data = auto_test, aes(horsepower), color = \"red\")\n\n\n\nBut if we stratify for horsepower, the distribution of a training set almost overlaps with the original set, and the distribution of a test set gets much more similar to the original data.\n\n\nset.seed(10)   # for reproducibility\nauto_split <- initial_split(auto, prop = 0.80, strata = horsepower)\n\nauto_train <- training(auto_split)\nauto_test  <- testing(auto_split)\n\nggplot()+\n  geom_density(data = auto, aes(horsepower))+\n  geom_density(data = auto_train, aes(horsepower), color = \"green\")+\n  geom_density(data = auto_test, aes(horsepower), color = \"red\")\n\n\n\n‚Äúbreaks‚Äù argument\nBy default, {rsample} will cut continuous variables into four parts, but we can increase the number of parts using the breaks argument in order to make distributions of training and test sets even more similar to the original data:\n\n\nset.seed(10)   # for reproducibility\nauto_split <- initial_split(auto, prop = 0.80, strata = horsepower, breaks = 10)\n\nauto_train <- training(auto_split)\nauto_test  <- testing(auto_split)\n\nggplot()+\n  geom_density(data = auto, aes(horsepower))+\n  geom_density(data = auto_train, aes(horsepower), color = \"green\")+\n  geom_density(data = auto_test, aes(horsepower), color = \"red\")\n\n\n\nAnd since there is no downside to using stratified sampling, I would always stratify sampling by the most important variable.\nHow to test the model\nHere is an example of the linear regression, which models miles-per-gallon of cars with their origin and horsepower. Important here is too see that we fit the model only with training data, and then use this model to predict the mpg in the test data. We‚Äôll take only Root mean squared error and coefficient of determination \\(R^2\\) as the prediction quality indicators. I say only, because there are many more, if you need other metrics. But for now the \\(R^2\\) of 0.68 means that our model explains 68% of variation in the test data. I think it‚Äôs pretty good, but it‚Äôs subjective. If this code seems confusing, don‚Äôt sweat it, it‚Äôs not important for this topic. It just helps to learn about resampling.\n\n\n# fit the model\nlm_fit <- \n  linear_reg() %>% \n  set_engine('lm') %>%\n  set_mode('regression') %>% \n  fit(mpg ~ origin * horsepower, data = auto_train)\n\n# test performance of the model\ntest_results <- \n  predict(lm_fit, new_data = auto_test) %>% \n  bind_cols(auto_test) \n\names_metrics <- metric_set(rsq) # options: \"mae\", \"ccc\" etc.\names_metrics(test_results, truth = mpg, estimate = .pred)\n\n# A tibble: 1 √ó 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.677\n\nWhat is important though, is to differentiate the Goodness-Of-Predictions (GoP) from the Goodness-Of-Fit (GoF). The Goodness-Of-Predictions (GoP), which is the \\(R^2\\) we just got after using a test data, is a realistic estimate of model performance, because it was tested on the data our model have never seen. While the Goodness-Of-Fit (GoF), which is the \\(R^2\\) we get, when we fit our model to the whole ‚Äúauto‚Äù dataset, will always result in an artificially optimistic, and therefore unrealistic, estimate of performance. The performance is unreaslistic, because some models are so good that they almost remember the data by hard, with all the noise and contaminations data contains. This phenomenon is called - overfitting and overfitted models are bad, because the noise our model have learned will result into great prediction on the training data and bad predictions on the test data.\nBy the way, here is a very useful free book about modelling, with code examples for both R and Python: https://ema.drwhy.ai/modelPerformance.html\n\n\nm <- lm(mpg ~ origin * horsepower, data = auto)\nlibrary(performance)   # I made a video on {performance} üì¶\nr2(m)\n\n# R2 for Linear Regression\n       R2: 0.683\n  adj. R2: 0.679\n\nValidation\nClassic-Validation\nSo, we need to split the data in order to be able to properly evaluate model performance. But this produces another problem: How do we know whether the model we just trained is good? If we log-transform the data or add another predictor, we might be able to improve the Goodness-of-Predictions. Therefore, we need to understand the performance of a model before using the test set.\n\nAnd since it‚Äôs critical to use the test-set only once, at the end of model building, we need a second small set of data in order to compare different models we train. This second small data set is similar to the test set, but in order to avoid confusion, it was called - validation set.\nHere is how it works: we can take our training set, which is 80% of the original data, and put let‚Äôs say 20% of it into a validation set. The 60% of the original data will be used for analysis. This validation set allows to stop the training, when the validation set error rate begins to rise. (make a plot in FCP where the curve of training error declines first and then increases) But I am not giving you any code example yet, because there is a much better way to use validation set.\n\nV-Fold Cross-Validation\nAnd this better way is - cross validation - which is one of the most popular resampling techniques. The working principle is very simple. Instead of cutting off 20% of the data from the training set as one validation set, we split the whole training set into, let‚Äôs say 4 validation sets (often called - folds) of 20% each.\nWe then use each validation set to evaluate the model analysed with the remaining 60% of the data. This produces 4 validations and when we take the average of them, we‚Äôll get a much better idea about the quality of our model, as compared to only one validation set. Such resampling uncovers how well our model works before using the test set.\n\nvfold_cv() function\nProgramming cross-validation in tidymodels is incredibly easy. We‚Äôll use the vfold_cv() function on the training set, and tell it how many folds do we want. Again, every validation set is similar to a test set, but was called a validation set to avoid confusion. And to avoid confusion between simple validation set and cross-validation, every fold which is used for validation - was called the assessment set, while remaining 3 folds were united to the analysis set. So, for every of the 4 folds, we‚Äôll build a model with the analysis set, and then apply this model on the assessment set to estimate model performance. We then calculate the average performance statistics of 4 models.\n\n\nset.seed(10)\nfolds <- vfold_cv(auto_train, v = 4) # 4-fold-cross-validation\nfolds$splits[[1]]\n\n<Analysis/Assess/Total>\n<232/78/310>\n\nZooming into a first fold will show that 232 observations will be used for the analysis, aka. creating the model, aka. training the model ‚Ä¶ Oh God it‚Äôs confusing! ‚Ä¶ while 78 observations will be used to assess aka. validate the performance of our model.\nworkflow() for regression model\nHere is how it works:\nwe first set up a model, namely linear regression\nwe then create a workflow, add the model and the formula to it,\nwe then use fit_resamples() function to fit our 4 models. By the way, fit_resamples() function is similar to fit() function we used before, but instead of having a data argument, where we put a training set, fit_resamples() has resamples argument, where we put the folds object which contains our 4 resamples,\nwe‚Äôll then use collect_metrics() function to get the average performance of our four fitted and validated models.\nDone! But if we want to see performance of all 4 separate models, we‚Äôll specify summarize = FALSE argument inside of collect_metrics()\n\n\n# set up a model\nlm_fit <- \n  linear_reg() %>% \n  set_engine('lm') %>%\n  set_mode('regression') \n\n# create a workflow\nmpg_wf <- \n  workflow() %>% \n  add_model(lm_fit) %>% \n  add_formula(mpg ~ origin * horsepower)\n\n# fit models to folds\ntrained_models <- fit_resamples(object    = mpg_wf, \n                                resamples = folds )\n\ntrained_models %>% \n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   4.47      4  0.246  Preprocessor1_Model1\n2 rsq     standard   0.675     4  0.0299 Preprocessor1_Model1\n\ntrained_models %>% \n  collect_metrics(summarize = FALSE)\n\n# A tibble: 8 √ó 5\n  id    .metric .estimator .estimate .config             \n  <chr> <chr>   <chr>          <dbl> <chr>               \n1 Fold1 rmse    standard       4.48  Preprocessor1_Model1\n2 Fold1 rsq     standard       0.681 Preprocessor1_Model1\n3 Fold2 rmse    standard       3.87  Preprocessor1_Model1\n4 Fold2 rsq     standard       0.743 Preprocessor1_Model1\n5 Fold3 rmse    standard       5.08  Preprocessor1_Model1\n6 Fold3 rsq     standard       0.597 Preprocessor1_Model1\n7 Fold4 rmse    standard       4.43  Preprocessor1_Model1\n8 Fold4 rsq     standard       0.677 Preprocessor1_Model1\n\nV-Fold Cross-Validation with ‚Äúrepeats‚Äù\nBut while 4 folds are great for explaining cross-validation, are 4 folds enough to get a reliable mean performance metrics? Well, interestingly the 10 fold cross-validation proved to deliver the best results, and thus, became the default in the vfold_cv() function, so that when you don‚Äôt specify argument ‚Äúv =‚Äù, your training set will automatically get 10 splits.\n\n\nvfold_cv(auto_train)\n\n#  10-fold cross-validation \n# A tibble: 10 √ó 2\n   splits           id    \n   <list>           <chr> \n 1 <split [279/31]> Fold01\n 2 <split [279/31]> Fold02\n 3 <split [279/31]> Fold03\n 4 <split [279/31]> Fold04\n 5 <split [279/31]> Fold05\n 6 <split [279/31]> Fold06\n 7 <split [279/31]> Fold07\n 8 <split [279/31]> Fold08\n 9 <split [279/31]> Fold09\n10 <split [279/31]> Fold10\n\nAnd while 10-fold cross-validation is cool enough, it has one little downside. Namely, it might produce slightly ‚Äúnoisy‚Äù, or high-variance, estimates. To reduce such ‚Äúnoisy‚Äù variance, we can perform repeated cross-validation, which simply means running the 10-fold cross-validation multiple times. For that, we can use the repeats argument inside of vfold_cv() function. Here, the default, and therefore not specified, 10-fold cross validation will be performed 10 times, which results in 100 models with 100 performance metrics, and therefore a less ‚Äúnoisy‚Äù average performance. This plot shows how quickly the standard error decreases with replicates:\nPicture source - Tidymodels book: https://www.tmwr.org/resampling.htmlworkflow() for classification problem with random forest\nAnd since we just used regression model, let‚Äôs try out repeated cross-validation on a classification model. For that we‚Äôll use a random forest algorithm with 1000 trees from the {ranger} package and classify car origins with the help of mpg and horsepower. (horse noise to wake up the viewers) The accuracy of 75% and the Area-Under-the-Curve (AUC_ROC) of 80% for such a small data set (of only 313 observations) is pretty good!\n\n\n# get repeated cross-validation object\nset.seed(10)\nfolds <- vfold_cv(auto_train, strata = origin, repeats = 10)\nfolds\n\n#  10-fold cross-validation repeated 10 times using stratification \n# A tibble: 100 √ó 3\n   splits           id       id2   \n   <list>           <chr>    <chr> \n 1 <split [278/32]> Repeat01 Fold01\n 2 <split [278/32]> Repeat01 Fold02\n 3 <split [278/32]> Repeat01 Fold03\n 4 <split [278/32]> Repeat01 Fold04\n 5 <split [279/31]> Repeat01 Fold05\n 6 <split [279/31]> Repeat01 Fold06\n 7 <split [279/31]> Repeat01 Fold07\n 8 <split [280/30]> Repeat01 Fold08\n 9 <split [280/30]> Repeat01 Fold09\n10 <split [281/29]> Repeat01 Fold10\n# ‚Ä¶ with 90 more rows\n\n# specify random forest model\nrf_spec <- \n  rand_forest(trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n# specify workflow\norigin_wf <- \n  workflow() %>%\n  add_model(rf_spec) %>%\n  add_formula(factor(origin) ~ mpg * horsepower)\n\n# fit models to folds\ntrained_models <- fit_resamples(object    = origin_wf, \n                                resamples = folds )\n\ntrained_models %>% \n  collect_metrics()\n\n# A tibble: 2 √ó 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.751   100 0.00646 Preprocessor1_Model1\n2 roc_auc  hand_till  0.802   100 0.00691 Preprocessor1_Model1\n\nOther validation techniques\nAnd while {rsample} package has tons of other useful techniques like, Leave-One-Out (LOO) cross-validation, Monte Carlo cross-validation, time dependent or grouped resampling and many more (scroll over this link), let‚Äôs go to our last primary resampling technique - Bootstrap Resampling.\n\n\nloo_cv(mtcars)\n\nmc_cv(auto_train, prop = 9/10, times = 10)\n\n\nNOTE: Leave-one-out (LOO) cross-validation perform worse than almost any other method with lots of data and is computationally excessive, but might be useful with pathologically small samples, like experimental data.\nBootstrap Resampling\n\nThe bootstrap resampling is like cross-validation on steroids. A ‚Äúbootstrap sample‚Äù is a sample, taken from the training set, and having the same size as the training set. The same size arises via replacements, where a particular observation might be sampled multiple times. Each observation has a 63% chance of being samples at least once, so that ca. 37% of the training set will not be selected for the analysis set and will therefore become the assessment set.\n\nBut, as cool as bootstrap resampling is, similarly to cross-validation, it also has a little downside. And while a little downside of cross-validation is - high variance, a little downside of bootstrap resampling is - low-variance, which might result in slightly pessimistic estimates of model performance. For example, if model accuracy with all data is 80%, the bootstrap resampling would show the accuracy to be below 80%.\nHowever, this decrease in accuracy is usually really small, and I personally prefer my estimates to be rather conservative and realistic, as opposed to overly optimistic, in order to avoid false positive results. So, I think, slightly conservative estimates can also be seen as small advantage.\n\nBut the most decisive advantage of bootstrapping compared to cross-validation, is that bootstrapping not only checks model performance, but can also deliver robust estimates of model parameters with 50%, 95% or any other confidence intervals. Moreover, bootstrapping is one of the best nonparametric modelling techniques, with 4 solid reasons of why a bootstrapped model is better then usual model. So, knowing bootstrapping will definitely make you a better data scientist and if you want to know how to bootstrapp models in R in the next 10 minutes, check out this video.\n\n\nset.seed(1)\nboot_samp <- bootstraps(auto_train, times = 10)\n\n# specify linear model\nlm_spec <- \n  rand_forest(trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n# workflow\nmpg_wf <- \n  workflow() %>%\n  add_model(lm_spec) %>%\n  add_formula(factor(origin) ~ mpg * horsepower)\n\ntrained_models <- \n  fit_resamples(object = mpg_wf, resamples = boot_samp)\n\ntrained_models %>% \n  collect_metrics(summarize = TRUE)\n\n# A tibble: 2 √ó 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.718    10 0.00806 Preprocessor1_Model1\n2 roc_auc  hand_till  0.791    10 0.00916 Preprocessor1_Model1\n\nGrouped Resampling\nIf we have related observation, for example repeated measures from the same person, or data collected at different cities, we should assign all related observations to either the analysis or the assessment set as a group, to avoid having assessment data that‚Äôs closely related to the data used to fit a model. Grouped Resampling is especially relevant for multilevel models, also known as mixed-effects models.\nLuckely for us, grouped resampling can be done with the same functions we just learned, but with the ‚Äúgroup_‚Äù prefix before, for example group_initial_split(). We also need to use the argument group inside of the function, to specify which column has related observations. You can think of the group argument as the opposite of the strata argument.\nAssigning the whole group to either analysis or assessment sets might screw-up proportions of those sets, especially if we have some small and some big groups. The functions, such as group_vfold_cv(), still try to balance the sets by default via randomly assigning groups and to balance the number of groups assigned to each fold. This means - we can still safely use repeated cross-validation, but if we want to balance the number of observations in each fold, instead of groups, you can use the argument balance = ‚Äúobservations‚Äù.\n\n\ntabyl(auto_train$year)\n\n auto_train$year  n    percent\n              70 21 0.06774194\n              71 21 0.06774194\n              72 23 0.07419355\n              73 32 0.10322581\n              74 21 0.06774194\n              75 27 0.08709677\n              76 25 0.08064516\n              77 24 0.07741935\n              78 27 0.08709677\n              79 23 0.07419355\n              80 22 0.07096774\n              81 21 0.06774194\n              82 23 0.07419355\n\nset.seed(10)  # for reproducibility\nauto_split <- group_initial_split(auto_train, prop = 0.8, group = year)\n\nauto_train <- training(auto_split)\nauto_test  <- testing(auto_split)\n\nset.seed(10)\nfolds <- group_vfold_cv(auto_train, v = 4, repeats = 4,  group = year, balance = \"observations\")\n\n# specify linear model\nlm_spec <- \n  rand_forest(trees = 1000) %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n# workflow\nmpg_wf <- \n  workflow() %>%\n  add_model(lm_spec) %>%\n  add_formula(factor(origin) ~ mpg * horsepower)\n\n# fit models to folds\ntrained_models <- fit_resamples(object    = mpg_wf, \n                                resamples = folds )\n\ntrained_models %>%\n  collect_metrics() \n\n# A tibble: 2 √ó 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.705    16 0.00983 Preprocessor1_Model1\n2 roc_auc  hand_till  0.764    16 0.0134  Preprocessor1_Model1\n\nGroup_bootstraps() is also easy to apply for related observations.\n\n\nset.seed(1)\nboot_samp <- group_bootstraps(auto_train, times = 2, group = year)\n\ntrained_models <- \n  fit_resamples(object = mpg_wf, resamples = boot_samp)\n\ntrained_models %>% \n  collect_metrics(summarize = TRUE)\n\n# A tibble: 2 √ó 6\n  .metric  .estimator  mean     n std_err .config             \n  <chr>    <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy multiclass 0.738     2  0.0158 Preprocessor1_Model1\n2 roc_auc  hand_till  0.767     2  0.0438 Preprocessor1_Model1\n\nFinal thoughts\nAnd while some statisticians avoid splitting the data, because they believe all of the data should be used for parameter estimation, it is a good modeling practice to have an unbiased set of observations as the final arbiter of model quality. A test set should be avoided only when the data are pathologically small. But, when training a final chosen model for production, after ascertaining the expected performance on new data, practitioners often use all available data for better parameter estimation.\nIt is critical to look at the test set only once; otherwise, it becomes part of the modeling process.\nFurther readings and references\nHere is the documentation of the {rsample} package with several articles: https://rsample.tidymodels.org/index.html\nHere are all the functions from {rsample} package: https://rsample.tidymodels.org/reference/index.html\nChapter from Tidymodels book: https://www.tmwr.org/resampling.html\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-10-27-rsample/thumbnail_rsample.png",
    "last_modified": "2022-11-06T16:20:13+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-08-31-bootstrappingregressions/",
    "title": "4 Reasons Non-Parametric Bootstrapped Regression (with tidymodels) is Better th–∞n Ordinary Regression",
    "description": "If the assumptions of parametric models can be satisfied, parametric models are the way to go. However, there are often many assumptions and to satisfy them all is rarely possible. Data transformation or using non-parametric methods are two solutions for that. In this post we'll learn the Non-Parametric Bootstrapped Regression as an alternative for the Ordinary Linear Regression in case when assumptions are violated.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-10-07",
    "categories": [
      "videos",
      "statistics",
      "visualization",
      "models"
    ],
    "contents": "\n\nContents\nThis post as a video (in making)\nThe problem with ordinary linear models\nWhy should we nonparametrically bootstrap regressions?\nWhat is - a bootstrap?\n4 steps to compute bootstrapped regression\n1. Bootstrap the data with resampling\n2. Produce 1000 models\n3. Plot estimates distribution of 1000 estimates\n4. Get and visualize predictions\n\nBootstrapping numeric predictors\nProblems\nFurther readings and references\n\nThis post as a video (in making)\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs less then ‚Ä¶ minutes long.\n\n\n\n\n\n\n\nThe problem with ordinary linear models\nIf we want to compare salaries of people with different education, we could create a simple linear model and plot the results. But we can not trust these results, without checking the assumptions of our model, because if assumptions are violated, the results might be wrong. However, the problem with assumptions is that there are too many, and most of the time we can‚Äôt satisfy them all.\n\n\nlibrary(tidymodels)   # for everything good in R ;)\nlibrary(ISLR)         # for \"Wage\" dataset\n\nset.seed(9999)        # makes sure we get the same 100 lines\nsalary <- Wage %>%\n  sample_n(100)\n\nm <- lm(wage ~ education, salary)\n\nlibrary(sjPlot)      # I have a video on \"sjPlot\" üì¶\ntheme_set(theme_test())\nplot_model(m, type = \"pred\", show.data = TRUE, jitter = TRUE)\n\n$education\n\n\nFor instance, our model has not-normally distributed residuals, the variances between groups differ and we have heteroskedasticity. So, we can‚Äôt trust our results, without fixing them, but when we start fixing assumptions, we might (1) either lose data (if, for example, we remove some influential points), (2) we might reduce interpretability (when we log-transform data) or screw up other assumptions even more, because data manipulation constantly changes the parameters of our - parametric model. And here is where non-parametric bootstraped regression comes into play.\n\n\nlibrary(performance) # I have a video on \"performance\" üì¶ \n\ncheck_normality(m)          # Shapiro-Wilk normality test\n\nWarning: Non-normality of residuals detected (p < .001).\n\ncheck_normality(m) %>% plot(type = \"qq\")\n\n\ncheck_homogeneity(m)        # Bartlett test \n\nWarning: Variances differ between groups (Bartlett Test, p = 0.000).\n\ncheck_homogeneity(m) %>% plot()\n\n\ncheck_heteroscedasticity(m) # Breusch-Pagan test\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p < .001).\n\ncheck_heteroscedasticity(m) %>% plot()\n\n\n\nWhy should we nonparametrically bootstrap regressions?\nThe 4 reasons a bootstrapped model is better then usual linear model, is that bootstrapping:\ndoes not have any distributional assumptions (such as normally distributed residuals or equal variances among groups)\nbootstrapped models provide more accurate inferences, e.g.¬†confidence intervals 1, which we‚Äôll soon prove on these two examples, one with categorical and one with numeric predictors\nbootstrapped models work better for small samples and finally\nit describes variation in the data much better than traditional linear models\nTherefore, having bootstrapped models in your tool-box will definitely make you a better data-scientist! So ‚Ä¶\nWhat is - a bootstrap?\nIn order to better understand how bootstrap models work, we need to understand what the bootstrap itself actually is. A bootstrapp is the little loop on the back of a boot to help you pool it on. And the phrase ‚Äúpulling oneself up by one‚Äôs bootstraps‚Äù means to succeed without any external help. For data it simply means - resampling.\n\n\nlibrary(ggstatsplot) # I made many videos on \"ggstatsplot\" üì¶\nggbetweenstats(data = salary, x = education, y = wage)\n\n\n\nSo, lets (1) take a very small dataset, (2) resample it 1000 times, (3) calculate bootstrapped coefficients & 95% CIs and (4) compare them to the ordinary regression in order to find out whether bootstrapping model is indeed better!\n4 steps to compute bootstrapped regression\nThere are only 4 steps to conduct and visualize bootstrapped regression.\n1. Bootstrap the data with resampling\nFirst, we bootstrap the data, which simply means we take 1000 samples from our 100 data rows. Every of those 1000 samples is of the same size as the original data set, namely 100 rows, but is made using replacements, which results into multiple replicates of some of the original rows of the data. Replacements are necessary, because we would otherwise simply reproduce the original sample. (The assessment set contains the rows of the original data that were not included in the bootstrap sample.) In order to distinguish bootstrapped samples from the original sample let‚Äôs call our bootstrapped samples - splits.\n\n\nset.seed(123)\n# 1000 samples are OK, but with 2000-10000 you are save!\nboot_data <- bootstraps(salary, times = 1000) \nboot_data\n\n# Bootstrap sampling \n# A tibble: 1,000 √ó 2\n   splits           id           \n   <list>           <chr>        \n 1 <split [100/41]> Bootstrap0001\n 2 <split [100/38]> Bootstrap0002\n 3 <split [100/42]> Bootstrap0003\n 4 <split [100/40]> Bootstrap0004\n 5 <split [100/30]> Bootstrap0005\n 6 <split [100/30]> Bootstrap0006\n 7 <split [100/36]> Bootstrap0007\n 8 <split [100/35]> Bootstrap0008\n 9 <split [100/32]> Bootstrap0009\n10 <split [100/36]> Bootstrap0010\n# ‚Ä¶ with 990 more rows\n\nboot_data$splits[[1]]\n\n<Analysis/Assess/Total>\n<100/41/100>\n\n2. Produce 1000 models\nThen we fit a linear model to every split using the first map() function and tidy up model coefficients with the second map() function. Let‚Äôs use ‚Äú0 +‚Äù in the model formula to remove the Intercept, because we are interested in estimates of the salary and not the slope of change from that Intercept.\n\n\nboot_models <- boot_data %>% \n  mutate(model = map(splits, ~lm(wage ~ 0 + education, data = .)), \n         coefs = map(model, tidy) ) # optional \"conf.int = TRUE\"\n\nboot_models\n\n# Bootstrap sampling \n# A tibble: 1,000 √ó 4\n   splits           id            model  coefs           \n   <list>           <chr>         <list> <list>          \n 1 <split [100/41]> Bootstrap0001 <lm>   <tibble [5 √ó 5]>\n 2 <split [100/38]> Bootstrap0002 <lm>   <tibble [5 √ó 5]>\n 3 <split [100/42]> Bootstrap0003 <lm>   <tibble [5 √ó 5]>\n 4 <split [100/40]> Bootstrap0004 <lm>   <tibble [5 √ó 5]>\n 5 <split [100/30]> Bootstrap0005 <lm>   <tibble [5 √ó 5]>\n 6 <split [100/30]> Bootstrap0006 <lm>   <tibble [5 √ó 5]>\n 7 <split [100/36]> Bootstrap0007 <lm>   <tibble [5 √ó 5]>\n 8 <split [100/35]> Bootstrap0008 <lm>   <tibble [5 √ó 5]>\n 9 <split [100/32]> Bootstrap0009 <lm>   <tibble [5 √ó 5]>\n10 <split [100/36]> Bootstrap0010 <lm>   <tibble [5 √ó 5]>\n# ‚Ä¶ with 990 more rows\n\n3. Plot estimates distribution of 1000 estimates\n\n\nboot_coefs <- boot_models %>%\n  unnest(coefs) %>% \n  select(term, estimate)\n\nboot_coefs\n\n# A tibble: 5,000 √ó 2\n   term                        estimate\n   <chr>                          <dbl>\n 1 education1. < HS Grad           85.5\n 2 education2. HS Grad             87.3\n 3 education3. Some College       107. \n 4 education4. College Grad       127. \n 5 education5. Advanced Degree    174. \n 6 education1. < HS Grad           85.0\n 7 education2. HS Grad             95.1\n 8 education3. Some College       109. \n 9 education4. College Grad       150. \n10 education5. Advanced Degree    149. \n# ‚Ä¶ with 4,990 more rows\n\nNow, we not only have 1000 models, which are nested in the column ‚Äúmodel‚Äù, but also the results of those models which are nested in the column ‚Äúcoefficients‚Äù. And if we unnest() the coefficients, we‚Äôll see 5 estimates for every of our 1000 models. It‚Äôs like we have done 1000 experiments, which we can immediately visualize as a distribution.\n\n\nboot_coefs %>%\n  ggplot(aes(x = estimate)) +\n  geom_histogram() +\n  facet_wrap(~term, scales = \"free\")\n\n\n\nFirst of all, the distributions seem normal, or bell-shaped, which is already amazing because we can use average estimates ;). But despite normal distribution, I still would prefer median estimate for every group instead of the mean. That makes our non-parametric estimates even more robust, since median is estimated by another wonderful robust technique - quantile-regression, which is also used when the assumptions of linear regression are not met. I find it really cool! But the coolest thing about this distribution and quantile-function is that we can easily get not only classic 95% Confidence Intervals, but any intervals we want. Which is kind of hard to get out of the ordinary model. Thus, we actually have more than 4 advantages ü•≥ of the bootstrapped model.\n\n\nboot_meds <- boot_coefs %>% \n  group_by(term) %>% \n  summarise(\n    med_est   = median(estimate),\n    quantile  = quantile(estimate, 0.5  ),\n    conf.low  = quantile(estimate, 0.025),\n    conf.high = quantile(estimate, 0.975),\n    conf.25   = quantile(estimate, 0.25 ),\n    conf.75   = quantile(estimate, 0.75 ))\n\nboot_meds\n\n# A tibble: 5 √ó 7\n  term                 med_est quant‚Ä¶¬π conf.‚Ä¶¬≤ conf.‚Ä¶¬≥ conf.25 conf.75\n  <chr>                  <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n1 education1. < HS Gr‚Ä¶    91.4    91.4    75.9    108.    85.8    97.3\n2 education2. HS Grad     94.4    94.4    86.9    103.    91.5    97.3\n3 education3. Some Co‚Ä¶   106.    106.     99.5    114.   104.    109. \n4 education4. College‚Ä¶   134.    134.    110.     165.   126.    143. \n5 education5. Advance‚Ä¶   153.    153.    119.     191.   140.    164. \n# ‚Ä¶ with abbreviated variable names ¬π‚Äãquantile, ¬≤‚Äãconf.low, ¬≥‚Äãconf.high\n\nAnd when we plot our estimates we can clearly see that 50% of people who did not finish a high school, will never reach a salary of 100 thousands, while 95% of folks with higher education will never earn below 100 thousands bugs. Waw! So, education matters!\n\n\nboot_coefs %>%\n  ggplot(aes(x = estimate)) +\n  geom_rect(aes(x = med_est, xmin = conf.low, xmax = conf.high, ymin = 0, ymax = Inf), data = boot_meds, alpha = 0.1, fill = \"green\") +\n  geom_rect(aes(x = med_est, xmin = conf.25, xmax = conf.75, ymin = 0, ymax = Inf), data = boot_meds, alpha = 0.3, fill = \"green\") +\n  geom_density() +\n  geom_vline(data = boot_meds, aes(xintercept = med_est))+\n  facet_wrap(~term, scales = \"free\")\n\n\n\nBut what blew my mind even more the first time I learned to bootstrap regression estimates, is that I can even get a distribution of p-values :). For that we just (1) remodel 1000 ‚Ä¶ ahhh, what a hell, let‚Äôs remodel 10.000 models with the Intercept, (2) unnest out coefficients again and (3) visualize the distribution of our 10.000 p.values. Now you see why it‚Äôs always better to take median instead of mean.\n\n\nboot_models <- bootstraps(salary, times = 10000) %>% \n  mutate(model = map(splits, ~lm(wage ~ education, data = .)), \n         coefs = map(model, tidy) ) # optional \"conf.int = TRUE\"\n\nboot_coefs <- boot_models %>%\n  unnest(coefs) %>% \n  select(term, p.value)\n\nboot_coefs %>%\n  ggplot(aes(x = p.value)) +\n  geom_histogram() +\n  facet_wrap(~term, scales = \"free\")\n\n\n\nAnd if we compare mean and median bootstrapped p.values to the p.values from the ordinary models, we‚Äôll see that median bootstrapped p.values are much closer to the the p.values of the ordinary model. And despite the fact that median p.values and ordinary p.values are only slightly different, I would still intuitively trust the bootstrapped p.values more, because they were produced in a statistically robust fashion, where no assumptions were violated.\n\n\nboot_coefs %>% \n  group_by(term) %>% \n  summarise(\n    mean_boot_p = mean(p.value),\n    med_boot_p  = median(p.value)\n    ) %>% \n  # remember? m <- lm(wage ~ education, salary)\n  left_join(tidy(m) %>% select(term, p.value)) %>% \n  mutate_if(is.numeric, ~round(., 4))\n\n# A tibble: 5 √ó 4\n  term                        mean_boot_p med_boot_p p.value\n  <chr>                             <dbl>      <dbl>   <dbl>\n1 (Intercept)                      0.0003     0       0     \n2 education2. HS Grad              0.652      0.686   0.862 \n3 education3. Some College         0.417      0.374   0.384 \n4 education4. College Grad         0.0632     0.0153  0.0145\n5 education5. Advanced Degree      0.0238     0.0017  0.0015\n\n4. Get and visualize predictions\nFinally, here is the moment we‚Äôve been waiting for:\nlet‚Äôs use the augment() command to produce predictions for every of our 10.000 models,\nthen calculate a median and 95% CIs from our predictions\ncalculate predictions made by the ordinary linear regression\ncombine two results into one dataset and\ncompare them visually by\nfirst plotting our original 100 observations and then\ndisplay predictions of both models to see which describes the data best\n\n\n\n# get predictions\nboot_aug <- boot_models %>% \n  mutate(augmented = map(model, augment)) %>% \n  unnest(augmented) %>% \n  select(-splits, -model)\n\n# get median estimates, median 95% CIs\nnonpar_med_boot_preds <- boot_aug %>% \n  group_by(education) %>% \n  summarise(\n    predicted = median(.fitted ),\n    conf.low  = quantile(.fitted, 0.025),\n    conf.high = quantile(.fitted, 0.975)) %>% \n  mutate(model = \"bootstrapped\") %>% \n  select(model, everything())\n\nnonpar_med_boot_preds\n\n# A tibble: 5 √ó 5\n  model        education          predicted conf.low conf.high\n  <chr>        <fct>                  <dbl>    <dbl>     <dbl>\n1 bootstrapped 1. < HS Grad            91.6     75.9      108.\n2 bootstrapped 2. HS Grad              94.4     86.1      103.\n3 bootstrapped 3. Some College        106.      99.4      114.\n4 bootstrapped 4. College Grad        134.     110.       163.\n5 bootstrapped 5. Advanced Degree     152.     122.       189.\n\n# get mean estimates from the classic model\nm <- lm(wage ~ education, salary)  # just a reminder\npar_avg_preds <- ggeffects::ggeffect(m, terms = \"education\") %>% \n  tibble() %>% \n  mutate(model = \"linear\") %>% \n  select(model, education = x, predicted, conf.low, conf.high)\n\npar_avg_preds\n\n# A tibble: 5 √ó 5\n  model  education          predicted conf.low conf.high\n  <chr>  <fct>                  <dbl>    <dbl>     <dbl>\n1 linear 1. < HS Grad            91.6     62.2      121.\n2 linear 2. HS Grad              94.5     79.8      109.\n3 linear 3. Some College        106.      89.8      123.\n4 linear 4. College Grad        135.     117.       152.\n5 linear 5. Advanced Degree     153.     130.       176.\n\n# combine two datasets\npreds <- rbind(nonpar_med_boot_preds, par_avg_preds)\n\n# plot row data and predicted CIs from both models\nggplot() +\n  geom_jitter(data = salary, aes(education, wage), \n              width = .2, alpha = 0.2, size = 2)+\n  geom_pointrange(data = preds, aes(x = education, y = predicted, \n                  ymin = conf.low, ymax = conf.high, color = model),\n                  position=position_dodge(width=.6), size = 1)\n\n\n\n\nInterestingly, the bootstrapped confidence intervals are slimmer where the variance is low and wider, where the variance is high. So, as mentioned in the beginning, bootstrapped models describe the variation in the data better and produce therefore more accurate and realistic inferences without violating any assumptions, especially for small samples.\nBootstrapping numeric predictors\nBootstrapping with numeric predictors works in the same way, so, let‚Äôs summarise all we have learned so far, namely:\nwe first create 1000 resamples of our data,\nthen map() over it to fit new 1000 models, with the numeric predictor ‚Äúage‚Äù\nwe then map() over every model and use augment() function to extract fitted data for every model in an new nested column, with 100 fitted values for every model, because every resample in boot_data has 100 observations.\n1000 models with 100 fitted values each result in 100.000 fitted values, which we see when we unnest them for plotting, and visualize as 1000 lines on the plot\n\n\n# produce 1000 resamples\nset.seed(123)\nboot_data <- bootstraps(salary, times = 1000) \n\n# bootstrap new regressions and unnest all fits\nboot_models <- boot_data %>% \n  mutate(model     = map(splits, ~lm(wage ~ age, data = .)), \n         augmented = map(model, augment) )\n\nboot_models\n\n# Bootstrap sampling \n# A tibble: 1,000 √ó 4\n   splits           id            model  augmented         \n   <list>           <chr>         <list> <list>            \n 1 <split [100/41]> Bootstrap0001 <lm>   <tibble [100 √ó 9]>\n 2 <split [100/38]> Bootstrap0002 <lm>   <tibble [100 √ó 9]>\n 3 <split [100/42]> Bootstrap0003 <lm>   <tibble [100 √ó 9]>\n 4 <split [100/40]> Bootstrap0004 <lm>   <tibble [100 √ó 9]>\n 5 <split [100/30]> Bootstrap0005 <lm>   <tibble [100 √ó 9]>\n 6 <split [100/30]> Bootstrap0006 <lm>   <tibble [100 √ó 9]>\n 7 <split [100/36]> Bootstrap0007 <lm>   <tibble [100 √ó 9]>\n 8 <split [100/35]> Bootstrap0008 <lm>   <tibble [100 √ó 9]>\n 9 <split [100/32]> Bootstrap0009 <lm>   <tibble [100 √ó 9]>\n10 <split [100/36]> Bootstrap0010 <lm>   <tibble [100 √ó 9]>\n# ‚Ä¶ with 990 more rows\n\nboot_aug <- boot_models %>% \n  unnest(augmented) %>% \n  select(-splits, -model)\n\nboot_aug\n\n# A tibble: 100,000 √ó 10\n   id         .rown‚Ä¶¬π  wage   age .fitted .resid   .hat .sigma .cooksd\n   <chr>      <chr>   <dbl> <int>   <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n 1 Bootstrap‚Ä¶ 31       61.2    25    95.7 -34.5  0.0256   47.7 7.09e-3\n 2 Bootstrap‚Ä¶ 79      112.     48   126.  -14.6  0.0149   47.8 7.20e-4\n 3 Bootstrap‚Ä¶ 51      110.     40   116.   -5.83 0.0100   47.8 7.66e-5\n 4 Bootstrap‚Ä¶ 14       79.9    52   132.  -51.8  0.0207   47.5 1.28e-2\n 5 Bootstrap‚Ä¶ 67       82.7    40   116.  -33.0  0.0100   47.7 2.45e-3\n 6 Bootstrap‚Ä¶ 42       95.2    30   102.   -7.12 0.0168   47.8 1.95e-4\n 7 Bootstrap‚Ä¶ 50       99.7    25    95.7   3.99 0.0256   47.8 9.47e-5\n 8 Bootstrap‚Ä¶ 43       70.5    24    94.4 -23.9  0.0278   47.8 3.70e-3\n 9 Bootstrap‚Ä¶ 14.1     79.9    52   132.  -51.8  0.0207   47.5 1.28e-2\n10 Bootstrap‚Ä¶ 25      121.     47   125.   -3.55 0.0138   47.8 3.94e-5\n# ‚Ä¶ with 99,990 more rows, 1 more variable: .std.resid <dbl>, and\n#   abbreviated variable name ¬π‚Äã.rownames\n\nggplot(data = boot_aug, aes(x = age, y = wage)) +\n  geom_line(aes(y = .fitted, group = id), col = \"grey\",  size = 0.25) \n\n\n\nwe then create new age data (picture of new age movement) ‚Ä¶ ups, sorry, not that\nnew age, just data of age from 20 to 75 years old, and call them new in order to evaluate our predictions\nuse map() function to predict salaries for the new age data by every model\nthen summarize all 1000 predictions by the median and calculate 95 and 50% confidence intervals (CIs),\nand finally plot them on top our our 1000 fitted models\n\n\n# get new data for predictions\nnew <- data.frame(age = seq(20, by = 3, 75))\nnew %>% t()\n\n    [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12]\nage   20   23   26   29   32   35   38   41   44    47    50    53\n    [,13] [,14] [,15] [,16] [,17] [,18] [,19]\nage    56    59    62    65    68    71    74\n\nfew_preds <- boot_models %>% \n  mutate(few_preds = map(model, predict, new)) %>% \n  unnest(few_preds) %>% \n  select(-splits, -model) %>% \n  mutate(age = rep( seq(20, by = 3, 75) , 1000)) %>% \n  group_by(age) %>% \n  summarise(wage   = median(few_preds),\n            LCL_50 = quantile(few_preds, 0.25),\n            UCL_50 = quantile(few_preds, 0.775),\n            LCL    = quantile(few_preds, 0.025),\n            UCL    = quantile(few_preds, 0.975),\n  )\n\nfew_preds\n\n# A tibble: 19 √ó 6\n     age  wage LCL_50 UCL_50   LCL   UCL\n   <dbl> <dbl>  <dbl>  <dbl> <dbl> <dbl>\n 1    20  93.4   89.9   97.5  83.4  104.\n 2    23  96.2   93.3   99.8  87.9  106.\n 3    26  99.1   96.6  102.   91.7  107.\n 4    29 102.    99.8  105.   95.5  109.\n 5    32 105.   103.   108.   98.8  112.\n 6    35 108.   106.   111.  101.   115.\n 7    38 111.   108.   114.  104.   119.\n 8    41 114.   110.   117.  106.   123.\n 9    44 117.   113.   121.  107.   128.\n10    47 120.   115.   124.  109.   132.\n11    50 122.   118.   128.  110.   137.\n12    53 125.   120.   131.  112.   142.\n13    56 128.   122.   134.  113.   147.\n14    59 131.   125.   138.  114.   152.\n15    62 134.   127.   142.  115.   156.\n16    65 137.   129.   145.  117.   161.\n17    68 139.   131.   149.  118.   166.\n18    71 142.   134.   153.  119.   171.\n19    74 145.   136.   157.  121.   176.\n\nggplot(data = boot_aug, aes(x = age, y = wage)) +\n  geom_line(aes(y = .fitted, group = id), col = \"grey\",  size = 0.25) +\n  geom_point(data = few_preds, aes(age, wage))+\n  geom_errorbar(data = few_preds, aes(ymin = LCL, ymax = UCL), width = 1)+\n  geom_errorbar(data = few_preds, aes(ymin = LCL_50, ymax = UCL_50))\n\n\n\nthen we‚Äôll plot the original 100 data points in green and\nthe results of an ordinary linear regression in blue with their 95% CIs in red\n\n\nggplot(data = boot_aug, aes(x = age, y = wage)) +\n  geom_point(data = salary, aes(age, wage), colour = \"green\", shape = 1) +\n  geom_line(aes(y = .fitted, group = id), col = \"grey\",  size = 0.25) +\n  geom_point(data = few_preds, aes(age, wage))+\n  geom_errorbar(data = few_preds, aes(ymin = LCL, ymax = UCL), width = 1)+\n  geom_errorbar(data = few_preds, aes(ymin = LCL_50, ymax = UCL_50))+\n  geom_smooth(aes(x = age, y = wage), data = salary, method = \"lm\",\n              colour = \"blue\", fill = \"red\", alpha = 0.25)\n\n\nggsave(\"num_example.jpeg\", plot = last_plot(), width = 7, height = 4)\n\n\nAnd as we can see again,\nlow variance in salaries in the beginning of professional life is described with narrower CIs by the bootstrapped predictions as compared to the ordinary linear regression, while\nlarger variance in salaries after age of 45 is described by the bootstrapped predictions with wider CIs as compared to the ordinary linear regression\nThus, similarly to the categorical predictor, we see that bootstrapped results better describe the variation in the data and produce therefore more accurate and realistic inferences.\nProblems\nSo, is bootstrap method perfect? Of coarse no! While bootstrap more accurately describes the variance of a sample, for example here I really want to include high salaries into my variance, to see what is actually possible to earn with this education, if you have real outliers or very influential observations in your data, they will be given more weight then needed. In this case you would use a robust regression, and if you wanna become a more complete data scientist in the next five minutes, watch this video.\n\n\nlibrary(robustbase)\nrm <- lmrob(wage ~ education, salary)\nrob_avg_preds <- ggeffects::ggeffect(rm, terms = \"education\") %>% \n  tibble() %>% \n  mutate(model = \"robust\") %>% \n  select(model, education = x, predicted, conf.low, conf.high)\n\n# combine two datasets\npreds <- rbind(nonpar_med_boot_preds, par_avg_preds, rob_avg_preds)\n\nggplot() +\n  geom_jitter(data = salary, aes(education, wage), width = .2, alpha = 0.2, size = 2)+\n  geom_pointrange(data = preds, aes(x=education, y=predicted, \n                                    ymin=conf.low, ymax=conf.high, color = model),  \n                  position=position_dodge(width=.6), size = 1)\n\n\nggsave(\"rob_example.jpeg\", plot = last_plot(), width = 7, height = 4)\n\n\nFurther readings and references\nhttps://www.datawim.com/post/bootstrapping-regression-coefficients-in-r/\nhttps://padpadpadpad.github.io/post/bootstrapping-non-linear-regressions-with-purrr/\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\nTim Hesterberg (2015), What Teachers Should Know about the Bootstrap: Resampling in the Undergraduate Statistics Curriculum, The American Statistician 69(4) 371-386, DOI: 10.1080/00031305.2015.1089789‚Ü©Ô∏é\n",
    "preview": "posts/2022-08-31-bootstrappingregressions/thumbnail_bootstrapped_regression.png",
    "last_modified": "2022-10-07T23:39:39+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-01-30-rmanova/",
    "title": "R demo | Repeated Measures ANOVA (One-Way) | How to Conduct, Visualise and Interpret",
    "description": "Can sport increase our selfesteem? Well, one experiment measured self-esteem of 10 people on three different time points and used Repeated Measures ANOVA to answer this question. So, let's learn how to produce this statistically rich plot using only one simple command and how to interpret all these results.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-10-05",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nGet the data\nCompute One-Way Repeated Measures ANOVA\nInterpret the result\nCustomise the result\nCheck Sphericity & Normality assumptions\nWhat‚Äôs next, or when not to use Repeated Measures ANOVA\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 7 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nPaired t-Test and Paired Wilcoxon test would help.\nGet the data\n\n\n# install.packages(\"tidyverse\")  # for everything ;)\nlibrary(tidyverse)\n\n# install.packages(\"datarium\")   # for selfesteem data\nlibrary(datarium)\n\n# View(selfesteem)\n\n\n\nidt1t2t314.015.187.1122.566.916.3133.244.449.7843.424.718.3552.873.916.4662.055.346.6573.535.586.8483.184.377.8293.514.408.47103.044.498.58\n\nFor that let‚Äôs take {selfesteem} data from {datarium} package, and gather all three time-points into one column, so that our timepoints become a variable for the x-axis of the plot, and our selfesteem scores become a variable for the y-axis of the plot. For repeated measures, the data needs to be sorted so that, the first observation of the first time point, pairs with the first observation of other time points. If our data is sorter, we are ready to compute the test.\n\n\n# make long format\nd <- selfesteem %>%\n  gather(key = \"time\", value = \"score\", t1, t2, t3) \n\n# View(d)\n\n\nCompute One-Way Repeated Measures ANOVA\nAnd the best way to compute Repeated Measures ANOVA (in my opinion) is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:\nfirst, our data, which is d, then\nx - as grouping variable - time,\ny - will be the scores of selfesteem and we choose\nthe parametric type of statistical approach, which tells {ggwithinstats} to conduct Repeated Measures ANOVA\nSuch simple command results in this statistically rich and publication ready plot! Now, let‚Äôs interpret the results.\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nset.seed(1)   # for Bayesian reproducibility\nggwithinstats(\n  data = d,\n  x    = time, \n  y    = score, \n  type = \"parametric\"\n)\n\n\nggsave(filename = \"rm_anova.jpg\", plot = last_plot(), width = 5.5, height = 5.5)\n\n\nInterpret the result\nF-statistics and degrees of freedom (DFs) were previously used to get p-values. But, since modern statistical software always report p-values, we can safely ignore them.\nthe p-value helps to test the hypothesis, where Null Hypothesis says that sample-means are similar, or, to be more exact, that the differences between pairwise samples are equal to Zero, while the Alternative Hypothesis says that sample-means differ, or, in other words, that differences between pairwise samples are not-equal to Zero.\n\n\ndiffs <- selfesteem %>%\n  mutate(\n    diff_t3_t1 = t3 - t1,\n    diff_t3_t2 = t3 - t2,\n    diff_t2_t1 = t2 - t1 )\n\n# View(diffs)\n\n\n\nidt1t2t3diff_t3_t1diff_t3_t2diff_t2_t114.015.187.113.101.931.1822.566.916.313.75-0.604.3533.244.449.786.535.331.2043.424.718.354.933.641.2952.873.916.463.592.551.0462.055.346.654.611.313.2973.535.586.843.311.262.0583.184.377.824.643.451.1993.514.408.474.964.070.89103.044.498.585.544.091.45\n\nour very low P-value (p = 0.00000216) shows a very strong evidence against the null hypothesis (H0), in favor of the alternative hypothesis (HAlt), that sample-means are different. That tells us that exercise significantly increases selfesteem over time. Which is good to know! But how strong is the effect of sports on selfesteem? P-value can not tell that. A P-value only tells you that there is an effect from training, but not how strong this effect is.\n\nFortunately, {ggwithinstats} provides partian omega squared with 95% Confidence Intervals as the measure of the Effect Size for Repeated Measures ANOVA, which shows that training effect of 0.81 is large.\n\nMoreover, {ggwithinstats} also provides a Bayesian Effect Size, namely the coefficient of determination - \\(R_2\\) with 95% Highest Density Intervals. \\(R_2\\) shows the explanatory power of our model and since \\(R_2\\) goes from 0 to 100%, the explanatory power of 82% in our model is huge, or, if we interpret \\(R_2\\) as the effect size, our effects is substantial.\n\nIf that‚Äôs not enough, we can have a look at the Bayes Factor, which is conceptually similar to the p-value. Our Bayes Factor of -20 indicates a Decisive evidence for the alternative hypothesis - that training does increase our selfesteem ‚Ä¶ which IS in line with the frequentists statistics on the top of the plot.\n\nnow, both, Bayes Factor and p-value tell us that difference among time-points exists, however, they don‚Äôt show between which time-points exactly. That‚Äôs why we need to compare every timepoint to every other timepoint pairwisely.\nby the way, our two global tests are often called with a strange name - omnibus test, while the pairwise tests between time-points, are sometimes described in a dead latin language as - post-hoc - which means - after the event - in plain English. I really think those unnecessary names make statistics more complicated then it is.\nAnyway, {ggwithinstats} automatically knows that we need Paired t-Tests for Repeated Measures ANOVA, automatically conducts those tests and displays p-values and even corrects p-values for multiple comparisons without any additional code. How cool is that!\nHere is a quick proof that {ggwithinstats} indeed uses paired t-test.\n\n\npairwise.t.test(d$score, d$time,\n                paired=T, \n                p.adjust.method = \"holm\")\n\n\n    Pairwise comparisons using paired t tests \n\ndata:  d$score and d$time \n\n   t1     t2    \nt2 0.0015 -     \nt3 1e-06  0.0015\n\nP value adjustment method: holm \n\nCustomise the result\nHowever, if we want to, we can easily customize our plot by using either additional arguments within the function, or arguments from {ggplot2} package outside of it. For example,\nif you found outliers in your data, you can display them on the plot and\nuse a robust ANOVA to minimize the effect of outliers,\nhere again, the function automatically uses correct pairwise tests for every omnibus test and corrects p-values for multiple comparisons with a Holm method,\nwhich you can easily change to a more famous Bonferroni correction for multiple comparisons ‚Ä¶ but I wouldn‚Äôt recommend it, because Bonferroni correction is too conservative and we could miss some interesting result\nthen, if you want to display not only significant, but all comparisons,\nif you want to hide either Frequentists or Bayesian statistics, or both‚Ä¶\nyou can easily do that and much more. Just ask R about {?ggwithinstats} function and try some things out, I am sure you‚Äôll enjoy it.\n\n\nggwithinstats(\n  data = d,\n  x    = time, \n  y    = score, \n  outlier.tagging = T,\n  type = \"robust\", \n  p.adjust.method = \"bonferroni\", \n  pairwise.display = \"all\",\n  # pairwise.comparisons = FALSE,   \n  results.subtitle = F,\n  bf.message = F\n) + \n  ylab(\"selfesteem score\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n?ggwithinstats\n\n\nCheck Sphericity & Normality assumptions\nNow, the last thing is important - please check both the Sphericity & Normality assumptions, otherwise you‚Äôll choose a wrong test and either miss a discovery having a wrong big p-value (also called - Type II Error) or you‚Äôll find some nonsense having a wrong small p-value (also called - Type I Error).\nRepeated Measures ANOVA needs Sphericity, where Sphericity simply means that data-spread inside of the groups is similar. To say it a more correct but more boring way, - Sphericity is a condition where variances of the differences between all combinations of related groups don‚Äôt differ. You know that Variance IS IMPORTANT, because the name of our test - ANOVA - is actually the abbreviation of the ANalises Of VAriances. However, since most of the real world data have different variances, {ggwithinstats} already accounts for Sphericity by default. You can still check Sphericity assumption by yourself and I‚Äôll show you how in a moment, but for now let‚Äôs talk about normality‚Ä¶\nANOVA also needs the data to be normally distributed, or bell shaped, but often compares a lot of groups, so that checking normality of separate groups for usual ANOVA or differences between those groups for Repeated Measures ANOVA, might become cumbersome.\nThe {aov_ez} function from {afex} package allows to easily check both assumptions. First, it conducts Mauchly Test for Sphericity and automatically corrects p-values of omnibus test. So that, if Sphericity was violated, you‚Äôll take the corrected p-value. Secondly, instead of checking the normality of thousands of groups, we can check the normality of the residuals of our ANOVA model, where all groups are already included. We then decide whether we stay with the Parametric Repeated Measures ANOVA if residuals are normally distributed, like in our example, or go to the Nonparametric Friedman test if residuals are not-normally distributed, which is a completely different story.\n\n\n# old hard way\n# install.packages(afex)\n\nlibrary(afex)\nhard <- aov_ez(\n  data   = d,\n  id     = \"id\", \n  dv     = \"score\",  \n  within = \"time\")\n\nsummary(hard)\n\n\nUnivariate Type III Repeated-Measures ANOVA Assuming Sphericity\n\n            Sum Sq num Df Error SS den Df  F value    Pr(>F)    \n(Intercept) 822.72      1   4.5704      9 1620.085 1.795e-11 ***\ntime        102.46      2  16.6237     18   55.469 2.014e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMauchly Tests for Sphericity\n\n     Test statistic  p-value\ntime        0.55085 0.092076\n\n\nGreenhouse-Geisser and Huynh-Feldt Corrections\n for Departure from Sphericity\n\n      GG eps Pr(>F[GG])    \ntime 0.69006  2.161e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        HF eps   Pr(>F[HF])\ntime 0.7743711 6.032582e-07\n\nresiduals(hard) %>% shapiro.test()\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.95183, p-value = 0.1892\n\nSphericity is conceptually similar to homogeneity of variances in a between-subjects ANOVA. The violation of Sphericity makes repeated measures ANOVA to become too liberal (increases the Type I Error, or, as I like to say - increases the probability to find nonsense). Luckely, there are corrections for sphericity already build into {ggwithingstats}.\nWhat‚Äôs next, or when not to use Repeated Measures ANOVA\nif samples are independent and normally distributed apply Student‚Äôs (variances are similar) or Welsh ANOVA (homogeneity of variances is violated)\nif samples are small (n<30) and not-normally distributed, use Friedman test\nRepeated Measures ANOVA is actually capricious and cranky. It does not like missing values or not exactly the same number of repeated measures for all individuals (imbalanced design), it often overfits, checking assumptions gets to cumbersome if we want to add more then one predictors. Thus, the solution for almost all of these problems and the most logical next step in your learning journey are - Mixed Effects Models.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-01-30-rmanova/thumbnail.png",
    "last_modified": "2022-10-05T11:46:20+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-09-12-manymodels/",
    "title": "R demo| Many Models with Nested (Grouped) Data Easily",
    "description": "In this blog-post, we'll learn how to produce grouped / nested models, with an amazing \"map()\" function from {purrr} package in R. We'll use linear models in this example for the sake of simplicity, but you can apply any model you want (robust, logistic, poisson etc.). We'll see, how to effectively store and use the information from multiple models. And while in this blog-post we'll produce \"only\" 10 models, you can produce any number of models you want.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-09-22",
    "categories": [
      "videos",
      "statistics",
      "visualization",
      "models"
    ],
    "contents": "\n\nContents\nThis post as a video\nWhy do we need many models?\nNested (grouped) data\nHow does this work amd why it is so useful?\nmap() function rocks!\n\nUnnest results\nUnnest coefficients\nUnnest model quality\nUnnest predictions\n\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 7 minutes long.\n\n\n\n\n\n\n\nWhy do we need many models?\nHave a close look at this linear model. It tells you that the only thing you need to do in order to earn significantly more money - is to get older. But why does one group of people earn so much more, than the others? And is a single model able to catch that groups?\n\n\nlibrary(ggpubr) # for stat_cor() function\nggplot(Wage, aes(age, wage))+\n  geom_point(alpha = 0.2)+\n  geom_smooth(method = lm)+\n  stat_cor()\n\n\nggsave(\"one_model.jpg\", plot = last_plot())\n\n\nWell, when look at education of these 3000 people, we‚Äôll see that most of the richest people have an advanced degree, while most of the poorest people have just a high school or below. And if we create 5 models instead of one, we‚Äôll see a much more useful story.\n\n\nggplot(Wage, aes(age, wage))+\n    geom_point(alpha = 0.7, aes(color = education))+\n    geom_smooth(method = lm)+\n    stat_cor()\n\n\nggsave(\"one_model_data.jpg\", plot = last_plot())\n\n\nFor instance, the increase of salary with age is much higher when you have at least some college degree as compared to no education. So that at the end of life we‚Äôll end up with an impressive salary of 150 thousand dollars, while without any education we‚Äôll never cross 100 thousand mark. So, it seems like education matters, and the slope clearly tells us that! However, despite the fact that the slope of the advanced degree is much smaller, which could suggest that education is not worth the effort, the intercept tells a different story. Namely, that folks who invested into education upfront start their life with the same salary, ‚Äúsome-college‚Äù guys reach only at the end of their life.\n\n\nggplot(Wage, aes(age, wage))+\n  geom_point(alpha = 0.2)+\n  geom_smooth(method = lm)+\n  facet_grid(. ~ education)+\n  stat_cor()+\n  stat_regline_equation(label.y = 240)\n\n\nggsave(\"five_models.jpg\", plot = last_plot(), width = 8.1, height = 3)\n\n\nSo, you see how much more useful 5 models are compared to one! But what about 10 model, for example when we group our data for health insurance? What about 20 models when we account for different jobclasses? The more models you create, the more useful insights you‚Äôll get! Then what about 1000 models? ‚Ä¶ ok, ok, we don‚Äôt have to exaggerate. Let‚Äôs stick to only 10 models and learn how to easily compute them and get all useful information, like slopes, measures of fit and p-values, out of them.\n\n\nggplot(Wage, aes(x = age, y = wage, color = health_ins)) +\n   geom_point(alpha = 0.1, shape = 1) +\n   geom_smooth(method = \"lm\") +\n   facet_grid(. ~ education, scales = \"free\")+\n    stat_cor()+\n  theme(legend.position = \"none\")\n\n\nggsave(\"10_models.jpg\", plot = last_plot(), width = 8.5, height = 3)\n\n\n\n\nggplot(Wage, aes(age, wage, color = health_ins))+\n    geom_point(alpha = 0.1)+\n    geom_smooth(method = lm)+\n    facet_grid(jobclass ~ education)\n\n\nggsave(\"20_models.jpg\", plot = last_plot(), width = 8.1, height = 5)\n\n\nNested (grouped) data\nThe Wage data you have seen on the plot is part of the ISLR package. If we have a glimpse at it, we‚Äôll see categorical variables ‚Äúeducation‚Äù and ‚Äúhealth insurance‚Äù. A simple cross table reveals how many observations we‚Äôll have in every of our 10 models.\n\n\nlibrary(tidyverse) # for everything good in R ;)\nlibrary(ISLR)      # for Wage dataset\n\nWage %>% glimpse()\n\nRows: 3,000\nColumns: 11\n$ year       <int> 2006, 2004, 2003, 2003, 2005, 2008, 2009, 2008, 2‚Ä¶\n$ age        <int> 18, 24, 45, 43, 50, 54, 44, 30, 41, 52, 45, 34, 3‚Ä¶\n$ maritl     <fct> 1. Never Married, 1. Never Married, 2. Married, 2‚Ä¶\n$ race       <fct> 1. White, 1. White, 1. White, 3. Asian, 1. White,‚Ä¶\n$ education  <fct> 1. < HS Grad, 4. College Grad, 3. Some College, 4‚Ä¶\n$ region     <fct> 2. Middle Atlantic, 2. Middle Atlantic, 2. Middle‚Ä¶\n$ jobclass   <fct> 1. Industrial, 2. Information, 1. Industrial, 2. ‚Ä¶\n$ health     <fct> 1. <=Good, 2. >=Very Good, 1. <=Good, 2. >=Very G‚Ä¶\n$ health_ins <fct> 2. No, 2. No, 1. Yes, 1. Yes, 1. Yes, 1. Yes, 1. ‚Ä¶\n$ logwage    <dbl> 4.318063, 4.255273, 4.875061, 5.041393, 4.318063,‚Ä¶\n$ wage       <dbl> 75.04315, 70.47602, 130.98218, 154.68529, 75.0431‚Ä¶\n\ntable(Wage$education, Wage$health_ins)\n\n                    \n                     1. Yes 2. No\n  1. < HS Grad          124   144\n  2. HS Grad            612   359\n  3. Some College       467   183\n  4. College Grad       529   156\n  5. Advanced Degree    351    75\n\nBut before we can model, we need to split our data into 10 groups using ‚Äúgroup_by()‚Äù function and then lock these 10 groups into 10 different data-sets using ‚Äúnest()‚Äù function.\n\n\nnested_data <- Wage %>% \n  group_by(education, health_ins) %>% \n  nest() \n\nnested_data\n\n# A tibble: 10 √ó 3\n# Groups:   education, health_ins [10]\n   education          health_ins data              \n   <fct>              <fct>      <list>            \n 1 1. < HS Grad       2. No      <tibble [144 √ó 9]>\n 2 4. College Grad    2. No      <tibble [156 √ó 9]>\n 3 3. Some College    1. Yes     <tibble [467 √ó 9]>\n 4 4. College Grad    1. Yes     <tibble [529 √ó 9]>\n 5 2. HS Grad         1. Yes     <tibble [612 √ó 9]>\n 6 2. HS Grad         2. No      <tibble [359 √ó 9]>\n 7 5. Advanced Degree 2. No      <tibble [75 √ó 9]> \n 8 5. Advanced Degree 1. Yes     <tibble [351 √ó 9]>\n 9 3. Some College    2. No      <tibble [183 √ó 9]>\n10 1. < HS Grad       1. Yes     <tibble [124 √ó 9]>\n\nIn a nested data frame each row is a meta-observation (‚àû üòÇ) where categorical variables ‚Äúeducation and health insurance‚Äù define our 10 groups, while the list-column of 10 data-sets could be seen as 10 lockers which contain individual observations belonging only to a particular combination of education and health insurance. In the first case, 144 people have no education (‚Äú1. < HS Grad‚Äù) and no health-insurance (‚Äú2. No‚Äù). And if you think, that a list-column of data-sets is a crazy idea, wait a second, and you‚Äôll see how useful it is.\n\n\nnested_data$data[[1]] %>% glimpse()\n\nRows: 144\nColumns: 9\n$ year     <int> 2006, 2003, 2004, 2006, 2008, 2003, 2009, 2007, 200‚Ä¶\n$ age      <int> 18, 27, 43, 25, 44, 24, 53, 41, 43, 35, 27, 25, 38,‚Ä¶\n$ maritl   <fct> 1. Never Married, 2. Married, 1. Never Married, 1. ‚Ä¶\n$ race     <fct> 1. White, 1. White, 1. White, 1. White, 1. White, 1‚Ä¶\n$ region   <fct> 2. Middle Atlantic, 2. Middle Atlantic, 2. Middle A‚Ä¶\n$ jobclass <fct> 1. Industrial, 1. Industrial, 1. Industrial, 1. Ind‚Ä¶\n$ health   <fct> 1. <=Good, 2. >=Very Good, 1. <=Good, 2. >=Very Goo‚Ä¶\n$ logwage  <dbl> 4.318063, 4.193125, 3.812913, 4.193125, 4.477121, 3‚Ä¶\n$ wage     <dbl> 75.04315, 66.22941, 45.28217, 66.22941, 87.98103, 5‚Ä¶\n\nHow does this work amd why it is so useful?\nImagine you‚Äôd need to write a code for 10 different models. That is not only a lot of work, but is also prone to mistakes. Moreover, you‚Äôd need to store and organize 10 different model objects somehow, because they contain information you need. And while it kind of works for 10 models, what if you really need 1000 or more?\n\n\nm1 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"1. < HS Grad\", health_ins == \"2. No\"))\nm2 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"4. College Grad\", health_ins == \"2. No\"))\nm3 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"3. Some College\", health_ins == \"1. Yes\"))\nm4 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"4. College Grad\", health_ins == \"1. Yes\"))\nm5 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"2. HS Grad\", health_ins == \"1. Yes\"))\nm6 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"2. HS Grad\", health_ins == \"2. No\"))\nm7 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"5. Advanced Degree\", health_ins == \"2. No\"))\nm8 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"5. Advanced Degree\", health_ins == \"1. Yes\"))\nm9 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"3. Some College\", health_ins == \"2. No\"))\nm10 <- lm(wage ~ age, \n         data = Wage %>% filter(education == \"1. < HS Grad\", health_ins == \"1. Yes\"))\n\n\nmap() function rocks!\nWell, map() function from {purrr} package provides a much better way! Because it applies a function of your choice to each element of a list. For example, if we want to multiply every element of our list by 10, we ‚Äúmap()‚Äù over every element of this list, where every element is represented by the DOT - ‚Äú.‚Äù\n\n\ndata <- list(1, 2, 3)\n\nmap(data, ~ . * 10) %>% \n  t()\n\n     [,1] [,2] [,3]\n[1,] 10   20   30  \n\nSimilarly, we can ‚Äúmap()‚Äù over every meta-observation (‚àû üòÇ) of our nested data-frame and apply a linear regression to every of the 10 data-frames which are stored in the list-column we called ‚Äúdata‚Äù. Moreover, rather than leaving the list of models as a free-floating objects (flies flying around trash, or free floating things in space), it‚Äôs much better to store all our models in the next list-column, let‚Äôs call this list-column ‚Äúmodels‚Äù. On top of that let‚Äôs now ‚Äúmap()‚Äù over our models in order to extract the coefficients with 95% CIs, model quality indicators and even predictions and store them all in separate list-columns.\n\n\nlibrary(broom)   # for tidy(), glance() & augment() functions\nnested_models <- nested_data %>%\n  mutate(models  = map(data, ~ lm(wage ~ age, data = .)), \n         coefs   = map(models, tidy, conf.int = TRUE),\n         quality = map(models, glance),\n         preds   = map(models, augment)) \n\nnested_models\n\n# A tibble: 10 √ó 7\n# Groups:   education, health_ins [10]\n   education        healt‚Ä¶¬π data     models coefs    quality  preds   \n   <fct>            <fct>   <list>   <list> <list>   <list>   <list>  \n 1 1. < HS Grad     2. No   <tibble> <lm>   <tibble> <tibble> <tibble>\n 2 4. College Grad  2. No   <tibble> <lm>   <tibble> <tibble> <tibble>\n 3 3. Some College  1. Yes  <tibble> <lm>   <tibble> <tibble> <tibble>\n 4 4. College Grad  1. Yes  <tibble> <lm>   <tibble> <tibble> <tibble>\n 5 2. HS Grad       1. Yes  <tibble> <lm>   <tibble> <tibble> <tibble>\n 6 2. HS Grad       2. No   <tibble> <lm>   <tibble> <tibble> <tibble>\n 7 5. Advanced Deg‚Ä¶ 2. No   <tibble> <lm>   <tibble> <tibble> <tibble>\n 8 5. Advanced Deg‚Ä¶ 1. Yes  <tibble> <lm>   <tibble> <tibble> <tibble>\n 9 3. Some College  2. No   <tibble> <lm>   <tibble> <tibble> <tibble>\n10 1. < HS Grad     1. Yes  <tibble> <lm>   <tibble> <tibble> <tibble>\n# ‚Ä¶ with abbreviated variable name ¬π‚Äãhealth_ins\n\nNow, with a minimum of code, where it is difficult to make any mistake, we have created a small and clean nested data-frame with 5 list-columns, where all the related objects are stored together. Hallelujah! ;) Such nested data-frame could be seen as a well organized cabinet with 50 lockers containing all important information we need, which is easily accessible anytime we want. For example:\nwe can have a look at the first model or it‚Äôs coeffitients,\nwe can check all assumptions of the second model at once using check_model() function from the {performance} package, which I already reviewed on this channel,\nwe can look at the model quality of, let‚Äôs say, a model N¬∞4 or\nwe can plot predictions of a model N¬∞9 using plot_model() function from another amazing package {sjPlot} I also have an extra video about\n\n\nnested_models$models[[1]]\n\n\nCall:\nlm(formula = wage ~ age, data = .)\n\nCoefficients:\n(Intercept)          age  \n    69.8032       0.1572  \n\nnested_models$coefs[[1]]\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   69.8       5.36      13.0  5.00e-26   59.2      80.4  \n2 age            0.157     0.131      1.20 2.30e- 1   -0.101     0.415\n\n\n\nnested_models$models[[2]] %>% performance::check_model()\n\n\n\n\n\nnested_models$quality[[4]] %>% glimpse()\n\nRows: 1\nColumns: 12\n$ r.squared     <dbl> 0.01700412\n$ adj.r.squared <dbl> 0.01513886\n$ sigma         <dbl> 40.33913\n$ statistic     <dbl> 9.116186\n$ p.value       <dbl> 0.002656313\n$ df            <dbl> 1\n$ logLik        <dbl> -2705.5\n$ AIC           <dbl> 5417\n$ BIC           <dbl> 5429.813\n$ deviance      <dbl> 857558.2\n$ df.residual   <int> 527\n$ nobs          <int> 529\n\n\n\nnested_models$models[[9]] %>% sjPlot::plot_model(type =  \"pred\", show.data = TRUE)\n\n$age\n\n\nUnnest results\n\n\nmap(nested_models$models, sjPlot::plot_model, type = \"pred\", show.data = TRUE)\n\n\nAnd despite the fact, that we could easily plot all 10 models by ‚Äúmapping‚Äù through the whole list of models, it is sometimes better to simply ‚Äúunnest()‚Äù the list-column back into a regular data frame. This is useful, when we want to put all the results below each other to see the big picture, be able to sort, compare or plot all 10 models simultaneously.\nUnnest coefficients\nFor example, we can unnest() the coefficients and see all 10 models below each other.\n\n\nlibrary(flextable) # for a good looking table\nnested_models %>%\n  unnest(coefs) %>% \n  select(-data, -models, -quality, -preds) %>% \n  mutate_if(is.numeric, ~ round(., 2)) %>% \n  regulartable() %>% \n  autofit()\n\neducationhealth_instermestimatestd.errorstatisticp.valueconf.lowconf.high1. < HS Grad2. No(Intercept)69.805.3613.030.0059.2180.391. < HS Grad2. Noage0.160.131.200.23-0.100.424. College Grad2. No(Intercept)91.8410.488.760.0071.13112.554. College Grad2. Noage0.270.241.150.25-0.200.743. Some College1. Yes(Intercept)86.144.9317.480.0076.4695.833. Some College1. Yesage0.630.115.570.000.410.854. College Grad1. Yes(Intercept)109.067.3614.830.0094.61123.514. College Grad1. Yesage0.500.173.020.000.180.832. HS Grad1. Yes(Intercept)90.594.4720.250.0081.8099.372. HS Grad1. Yesage0.270.102.740.010.080.462. HS Grad2. No(Intercept)66.754.4914.860.0057.9275.582. HS Grad2. Noage0.450.114.120.000.240.675. Advanced Degree2. No(Intercept)97.0625.913.750.0045.43148.705. Advanced Degree2. Noage0.730.531.380.17-0.331.805. Advanced Degree1. Yes(Intercept)131.1513.0210.080.00105.55156.755. Advanced Degree1. Yesage0.540.291.880.06-0.031.103. Some College2. No(Intercept)44.848.745.130.0027.5962.083. Some College2. Noage1.340.226.040.000.901.771. < HS Grad1. Yes(Intercept)78.746.9111.390.0065.0692.431. < HS Grad1. Yesage0.330.152.220.030.040.62\n\nUnnest model quality\nWe could:\nunnest() list-column ‚Äúquality‚Äù to extract some model quality indicators,\neasily remove some unnecessary columns and\nsort the data-frame for ‚Äúr.squared‚Äù in order to rank the goodness of fit of our model and see models that don‚Äôt fit well first.\nThe worst model appears to be for College Graduates with no health insurance ‚Ä¶ how could they?\n\n\nnested_models %>% \n  unnest(quality) %>% \n  select(-data, -models, -coefs, -df, -df.residual, -deviance, -preds) %>%\n  arrange(adj.r.squared) %>% \n  mutate_if(is.numeric, ~ round(., 2)) %>% \n  regulartable() %>% \n  autofit()\n\neducationhealth_insr.squaredadj.r.squaredsigmastatisticp.valuelogLikAICBICnobs4. College Grad2. No0.010.0035.771.310.25-778.391,562.771,571.921561. < HS Grad2. No0.010.0020.141.450.23-635.691,277.381,286.291445. Advanced Degree1. Yes0.010.0153.333.530.06-1,892.833,791.663,803.243512. HS Grad1. Yes0.010.0127.567.510.01-2,897.115,800.225,813.476125. Advanced Degree2. No0.030.0151.511.890.17-401.04808.08815.04754. College Grad1. Yes0.020.0240.349.120.00-2,705.505,417.005,429.815291. < HS Grad1. Yes0.040.0318.904.920.03-539.421,084.831,093.301242. HS Grad2. No0.050.0425.7817.000.00-1,674.983,355.963,367.613593. Some College1. Yes0.060.0626.9931.020.00-2,200.624,407.244,419.684673. Some College2. No0.170.1636.0436.510.00-914.661,835.311,844.94183\n\nUnnest predictions\nAnd lastly,\nwe could easily unnest() our predictions in a separate data-frame, then\nplot() original data and linear models which are already build into the classic ggplot() commands, by intentionally living same blue color for different insurances and making them a little bigger, and finally\nplot our predictions on top of them with different colors in order to see whether our predictions worked well, and voil√†, our predictions perfectly fitted the blue lines!\n\n\nunnested_preds <- \n  nested_models %>% \n  unnest(preds)\n\nunnested_preds \n\n# A tibble: 3,000 √ó 14\n# Groups:   education, health_ins [10]\n   education    health‚Ä¶¬π data     models coefs    quality   wage   age\n   <fct>        <fct>    <list>   <list> <list>   <list>   <dbl> <int>\n 1 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  75.0    18\n 2 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  66.2    27\n 3 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  45.3    43\n 4 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  66.2    25\n 5 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  88.0    44\n 6 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  51.5    24\n 7 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  68.1    53\n 8 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  88.0    41\n 9 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  86.7    43\n10 1. < HS Grad 2. No    <tibble> <lm>   <tibble> <tibble>  59.1    35\n# ‚Ä¶ with 2,990 more rows, 6 more variables: .fitted <dbl>,\n#   .resid <dbl>, .hat <dbl>, .sigma <dbl>, .cooksd <dbl>,\n#   .std.resid <dbl>, and abbreviated variable name ¬π‚Äãhealth_ins\n\n\n\nggplot(Wage, aes(x = age, y = wage, group = health_ins)) +\n   geom_point(aes(color = health_ins), alpha = 0.2, shape = 1) +\n   geom_smooth(method = \"lm\", size = 2) +\n   facet_grid(. ~ education, scales = \"free\") +\n   geom_line(data = unnested_preds, aes(y = .fitted, age, color = health_ins)) \n\n\n\nThis beautiful picture is worth a thousand words, but if you need words and want to learn how to easily and correctly report statistical results with text, you need to watch this video!\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-09-12-manymodels/thumbnail_many_models.png",
    "last_modified": "2022-09-22T12:37:00+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-09-02-robustregression/",
    "title": "R demo | Robust Regression (don't depend on influential data!)",
    "description": "Linear regression can be very sensitive to unusual data, like outliers, high leverage observations or a combination of both. A robust regression suppose to provide a solution for that. So, let's build both an ordinary and a robust regressions, compare them to find out whether outliers are a serious problem and see whether robust model performs better then usual linear model.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-09-11",
    "categories": [
      "videos",
      "statistics",
      "visualization",
      "models"
    ],
    "contents": "\n\nContents\nThis post as a video\nMake sure you have\nunusual data\nPerform robust\nregression\nHow and why does\nrobust regression works?\nCompare both\nmodels and choose the best\nWorks also in a\ncomplicated model\nThere are\nfurther packages for robust regression, but‚Ä¶\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk\nabout. It‚Äôs less then 5 minutes long.\n\n\n\n\n\n\n\n\n\n\nMake sure you have unusual\ndata\nFor the sake of simplicity let‚Äôs take only 5 observations with one\nobvious outlier.\nFirst of all, how do we know that we have influential observations?\nWell, plotting the raw data sometimes helps, but if you have a lot of\ndata and many predictors, the best way to find unusual data\nis to conduct a linear regression and to run residuals\ndiagnostics. ols_plot_resid_lev() function from\n{olsrr} package displays all contaminations of data on a single plot. In\nour case it finds observation 4 to be an obvious outlier.\n\n\n# 1. create data\nlibrary(tidyverse)\nd <- tibble(\n  predictor = c(  1,   2,   3,  4,   5),\n  outcome   = c(0.8, 2.3, 2.8,  0, 5.3)\n)\n\nplot(d)\n\n\n# 2. run ordinary least squares regression\nm <- lm(outcome ~ predictor, data= d)\n\n# 3.make residual diagnostics\nlibrary(olsrr)\nols_plot_resid_lev(m)\n\n\n\nPerform robust regression\nNow, we‚Äôll use lmrob() function form {robustbase}\npackage to conduct the robust regression and have a look at the\nsummary() and residuals plot of the model, because they\nexplain how robust regression actually works.\n\n\n# 4. conduct robust regression\nlibrary(robustbase)\nrm <- lmrob(outcome ~ predictor, data = d)\n\nsummary(rm)\n\n\nCall:\nlmrob(formula = outcome ~ predictor, data = d)\n \\--> method = \"MM\"\nResiduals:\n       1        2        3        4        5 \n-0.09832  0.31533 -0.27102 -4.15738  0.05627 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.18804    0.18312  -1.027     0.38    \npredictor    1.08635    0.03763  28.873 9.12e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nRobust residual standard error: 0.6541 \nMultiple R-squared:  0.9791,    Adjusted R-squared:  0.9721 \nConvergence in 5 IRWLS iterations\n\nRobustness weights: \n     1      2      3      4      5 \n0.9979 0.9789 0.9844 0.0000 0.9993 \nAlgorithmic parameters: \n       tuning.chi                bb        tuning.psi \n        1.548e+00         5.000e-01         4.685e+00 \n       refine.tol           rel.tol         scale.tol \n        1.000e-07         1.000e-07         1.000e-10 \n        solve.tol       eps.outlier             eps.x \n        1.000e-07         2.000e-02         9.095e-12 \nwarn.limit.reject warn.limit.meanrw \n        5.000e-01         5.000e-01 \n     nResample         max.it       best.r.s       k.fast.s \n           500             50              2              1 \n         k.max    maxit.scale      trace.lev            mts \n           200            200              0           1000 \n    compute.rd fast.s.large.n \n             0           2000 \n                  psi           subsampling                   cov \n           \"bisquare\"         \"nonsingular\"         \".vcov.avar1\" \ncompute.outlier.stats \n                 \"SM\" \nseed : int(0) \n\nlibrary(sjPlot)\nplot_residuals(m)\n\n\n\nHow and why does\nrobust regression works?\nNamely, a robust regression gives different robustness\nweights, from 0 to 1, to every observation based on it‚Äôs\nresidual. Where a residual is simply the differences between observed\nand predicted values of data. So, the smaller the residual, the larger\nthe weight. For example, observations 1 and 3 have the smallest\nresiduals and therefore the highest weight, which means - they have the\nstrongest influence on our model. And while all observations with a\nnon-zero residual get down-weighted at least a little, our outlier gets\ndown-weighting the most ‚Ä¶ to ‚Ä¶ actually zero, so that our outlier has\nzero influence on our model, which in fact makes our model ROBUST.\nThe assignment of weight happens by Iteratively ReWeighting\nLeast Squares (IRWLS), thus we have to make sure, robust\nregression algorithm converged. In our case, the model converged in only\na few iterations.\nBut why don‚Äôt we just remove outliers and run a normal linear model,\nright? Well, in most of the cases it‚Äôs a bad idea, because we‚Äôll loose\ninformation. For example in our case of 5 observations, we‚Äôd loose 20%\nof data. In contrast, robust regression still squizzes some knowledge\nout of unusual data, but lowers their weight which does not let unusual\ndata to influence our regression to much. Now lets ‚Ä¶\nCompare both models and\nchoose the best\nFirst, plot_model() command from {sjPlot} package easily\nvisualizes predictions of both models.\nThe ordinary linear model shows no trend. However, the absence of a\ntrend may only be caused by the outlier N¬∞4, which drags the line down\nand widens the confidence intervals, making us less confident in our\nresults. In contrast, a robust regression ignores the outlier and shows\na clear trend with a narrow confidence intervals.\n\n\n# 5. visualize both models\nplot_model(m,  type = \"pred\", show.data = T)\nplot_model(rm, type = \"pred\", show.data = T)\n\n\nMoreover, tab_model() command from {sjPlot} package\nshows that a robust model has much higher coefficient of determination\n\\(R^2\\), which means that robust model\nfits the data much better then the ordinary model. And finally, we can\nsee that the results can be dramatically different. Namely, a slope is\nsignificant in a robust regression, while not significant in the\nordinary linear model, indicating that ordinary model (1) was soo\nheavily biased by the outlier, (2) that it produced a wrong result, (3)\nwhich made me miss an important discovery (4) and never win a Nobel\nPrice ;)\n\n$predictor\n$predictor\n\n\n\n\n# 6. compare coefficients and goodness of fit of both models\ntab_model(m)\n\n\n¬†\n\n\noutcome\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n0.23\n\n\n-6.52¬†‚Äì¬†6.98\n\n\n0.921\n\n\npredictor\n\n\n0.67\n\n\n-1.37¬†‚Äì¬†2.71\n\n\n0.372\n\n\nObservations\n\n\n5\n\n\nR2 / R2 adjusted\n\n\n0.268 / 0.024\n\n\ntab_model(rm)\n\n\n¬†\n\n\noutcome\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n-0.19\n\n\n-0.77¬†‚Äì¬†0.39\n\n\n0.380\n\n\npredictor\n\n\n1.09\n\n\n0.97¬†‚Äì¬†1.21\n\n\n<0.001\n\n\nObservations\n\n\n5\n\n\nR2 / R2 adjusted\n\n\n0.979 / 0.972\n\n\nSo, having robust regression in your statistical toolbox will already\nstep up your data-science game, but robust regression does not save you\nfrom violations of other model assumptions, which you definitely need to\ncheck, otherwise you might again get a completely wrong result.\nFortunately, there is only one function which\nchecks and visualizes all the assumptions of any model at\nonce which you can learn more about from this video.\nWorks also in a complicated\nmodel\n\n\nlibrary(carData)\ncm <- lm(prestige ~ income * education, data = Duncan)\nols_plot_resid_lev(cm)\n\n\ncrm <- lmrob(prestige ~ income * education, data = Duncan)\n\nlibrary(performance)\ncompare_performance(cm, crm, rank = T)\n\n# Comparison of Model Performance Indices\n\nName | Model |    R2 | R2 (adj.) |   RMSE |  Sigma | Performance-Score\n----------------------------------------------------------------------\ncrm  | lmrob | 0.885 |     0.876 | 13.506 | 10.170 |            75.00%\ncm   |    lm | 0.829 |     0.816 | 12.895 | 13.509 |            25.00%\n\nThere are\nfurther packages for robust regression, but‚Ä¶\nMASS::rlm() does not provide \\(R^2\\)\nwhile robust::lmRob() does not provide info on outliers and has a\nsmaller \\(R^2\\). There are also other\noptions to conduct a non-parametric regression, like least trimmed\nsquares, quantile regression or bootstrapped regression. But, while all\nof them have merits, I personally decided to always use\nrobustbase::lmrob() if I have unsual data.\n\n\nrm2 <- MASS::rlm(outcome ~ predictor, data= d)\nrm3 <- robust::lmRob(outcome ~ predictor, data= d)\n\nplot_model(rm2, type = \"pred\", show.data = T)\n\n$predictor\n\nplot_model(rm3, type = \"pred\", show.data = T)\n\n$predictor\n\n\nIf you think, I missed something, please comment on it, and I‚Äôll\nimprove this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-09-02-robustregression/thumbnail_robust.png",
    "last_modified": "2022-09-11T15:22:51+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-06-18-report/",
    "title": "R package reviews {report} How To Report Statistical Results!",
    "description": "If you ever wandered how to correctly describe the results of statistical tests and models, this blog is for you. In a few minutes you'll learn how to report the results of correlations, t-tests, Generalised Linear Models, Mixed-Effects models, Bayesian Models and even more üòâ So, let's start with a simple t-test.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-07-11",
    "categories": [
      "videos",
      "statistics",
      "R package reviews",
      "models"
    ],
    "contents": "\n\nContents\nThis post as a video\nWhy do we need {report}?\n1. Reporting Statistical\nTests\nWelch t-test\nStudent‚Äôs Two Sample\nt-test\nPaired Two Sample\nt-test\nWilcoxon\nrank sum test (aka Mann-Whitney test)\nPaired Wilcoxon signed\nrank test\nPearson‚Äôs correlation\nSpearman‚Äôs rank\ncorrelation\nKendall‚Äôs rank\ncorrelation (as a table)\nReporting ANOVAs\n\n2. Reporting Regression\nModels\nReporting (General)\nLinear Models\nLogistic regression\nMixed-effects /\nmultilevel models\nReporting\nBayesian Models and Bayesian Mixed-Effects models\nExotic and not-supported\nmodels üò® ???\n\n3. Reporting Data\nAs text\nAs table\n\n4. Reporting R\nenvironment, packages, system\nReferences\nFurther readings and\nreferences\n\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk\nabout. It‚Äôs less then 8 minutes long.\n\n\n\n\n\n\n\nWhy do we need {report}?\nThe result of a statistical test are often hardly digestible. So,\nwhat we really want is the small but condense peace of text about what\nour result are and what they mean. And that‚Äôs what the {report} package\ngives you.\n\nHere is a preliminary list of objects you can report now, in Julne,\n2022. However, the package constantly improves and will only get better\nover time:\nCorrelations, t-tests, Wilcoxon tests, paired & unpaired\n(htest)\nANOVAs one-way, two-way etc. (aov, anova, aovlist, ‚Ä¶)\nBayes factors (from bayestestR)\n‚Ä¶ (more tests to come)\nRegression models (glm, lm, ‚Ä¶)\nMixed-effects models (glmer, lmer, glmmTMB, ‚Ä¶)\nBayesian models (stanreg, brms‚Ä¶)\nStructural Equation Models (SEM) (from lavaan)\n‚Ä¶ (more models to come)\nModel comparison (from performance())\nSystem and packages (sessionInfo)\nDataframes and vectors\n‚Ä¶ (more features to come)\n1. Reporting Statistical Tests\nLoad all needed packages at once, to avoid interruptions.\n\n\nlibrary(ISLR)        # provides \"Wage\" dataset\nlibrary(report)      # to report results\nlibrary(sjPlot)      # visualizes model results\nlibrary(flextable)   # for beautiful tables\nlibrary(tidyverse)   # provides a lot of useful stuff !!! \nlibrary(ggstatsplot) # to visualize test results\n\n\n\nWelch t-test\n\n\nt.test(wage ~ jobclass, data = Wage)\n\n\n\n    Welch Two Sample t-test\n\ndata:  wage by jobclass\nt = -11.489, df = 2714.9, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group 1. Industrial and group 2. Information is not equal to 0\n95 percent confidence interval:\n -20.21940 -14.32378\nsample estimates:\n mean in group 1. Industrial mean in group 2. Information \n                    103.3211                     120.5927 \n\nThe usual output of a t-test is ‚Ä¶ not really\nappealing ‚Ä¶ to say the least. And if we‚Äôd try to write down the\nresults, we might end up reporting only a p-value. However, if we add\nonly one word to this code, we‚Äôll get SOO MUCH\nMORE:\n\n\nt.test(wage ~ jobclass, data = Wage) %>% \n  report()\n\n\n\nEffect sizes were labelled following Cohen‚Äôs (1988)\nrecommendations.\nThe Welch Two Sample t-test testing the difference of wage by\njobclass (mean in group 1. Industrial = 103.32, mean in group 2.\nInformation = 120.59) suggests that the effect is negative,\nstatistically significant, and small (difference = -17.27, 95% CI\n[-20.22, -14.32], t(2714.87) = -11.49, p < .001; Cohen‚Äôs d = -0.44,\n95% CI [-0.52, -0.36])\nFirst of all, we‚Äôll get a digestible peace of\ntext, suitable for any publication\ninstantly, with all important statistics, brackets and\nspecial characters. If you think it‚Äôs not a big deal, try to\nrewrite it at least once without a single mistake üòâ\nSecondly, it gives us the difference between groups\nwith 95% Confidence Intervals, while the test only provides the\nconfidence intervals of ‚Ä¶ probably difference, but not the\ndifference itself - the very thing we are interested in!\nOn top of that, report() function provides a humanly\nreadable p-value of under 0.001, instead of the strange\nscientific notation, nobody really likes and only a few understand.\nAnd finally, it not only calculates the effect size with 95%\nConfidence Intervals for us, which a classic test doesn‚Äôt do,\nbut also interprets it and even provides a\nreference.\nStudent‚Äôs Two Sample t-test\n\n\nt.test(iris$Sepal.Width, iris$Sepal.Length, var.equal = TRUE) %>%\n  report()\n\n\n\nEffect sizes were labelled following Cohen‚Äôs (1988)\nrecommendations.\nThe Two Sample t-test testing the difference between iris\\(Sepal.Width and iris\\)Sepal.Length (mean of\nx = 3.06, mean of y = 5.84) suggests that the effect is negative,\nstatistically significant, and large (difference = -2.79, 95% CI [-2.94,\n-2.64], t(298) = -36.46, p < .001; Cohen‚Äôs d = -4.21, 95% CI [-4.61,\n-3.80])\nPaired Two Sample t-test\n\n\nreport(t.test(iris$Sepal.Width, iris$Sepal.Length, paired = TRUE))\n\n\n\nEffect sizes were labelled following Cohen‚Äôs (1988)\nrecommendations.\nThe Paired t-test testing the difference between iris\\(Sepal.Width and iris\\)Sepal.Length (mean of\nthe differences = -2.79) suggests that the effect is negative,\nstatistically significant, and large (difference = -2.79, 95% CI [-2.94,\n-2.63], t(149) = -34.82, p < .001; Cohen‚Äôs d = -2.84, 95% CI [-3.66,\n-2.49])\nWilcoxon rank sum\ntest (aka Mann-Whitney test)\nReporting results of nonparametric tests, for example Wilcoxon test,\nis even more useful! Because the report() function\ncorrectly says: ‚ÄúThe Wilcoxon rank sum test is testing the\ndifference in ranks‚Ä¶‚Äù, while even some scientific papers\nmistakenly say that Wilcoxon test is testing the difference in\nmedians, which is just wrong, medians are only used to better\ndescribe not-normally distributed data, but medians are not used to\ncompare groups. If fact the difference in ranks can be significant even\nwhen medians are identical. Here again, report() function\napplies a rank biserial correlation coefficient as a\nsuitable effect size for Wilcoxon rank sum test with a\nreference. As you can see, {report} package not only produces more\nresults in a suitable for publication form, but also ensures a\ncorrect interpretation of results.\n\n\n\nwilc_test <- wilcox.test(Wage$wage ~ Wage$jobclass) \nreport(wilc_test)\n\n\n\nEffect sizes were labelled following Funder‚Äôs (2019)\nrecommendations.\nThe Wilcoxon rank sum test with continuity correction testing the\ndifference in ranks between Wage\\(wage and\nWage\\)jobclass suggests that the effect is negative,\nstatistically significant, and medium (W = 8.53e+05, p < .001; r\n(rank biserial) = -0.24, 95% CI [-0.28, -0.20])\nBy the way, we can easily integrate parts of the\nresults into the text of our manuscript, if we prepare our\nmanuscript in RStudio. For instance if we use\nreport_statistics() function in the middle of the text,\nonly numbers from our test will be incorporated into the text. The\nreport_effectsize() function would only report the effect\nsize. Here is how it looks like inside of RStudio and in the\nmanuscript:\n\nWorkers in the IT-Industry earn significantly more (W = 8.53e+05, p\n< .001; r (rank biserial) = -0.24, 95% CI [-0.28, -0.20]) as compared\nto workers in Industrial jobs. This difference is medium (r (rank\nbiserial) = -0.24, 95% CI [-0.28, -0.20]) large and can not be\nignored.\nPaired Wilcoxon signed rank\ntest\n\n\nreport(wilcox.test(iris$Sepal.Width, iris$Sepal.Length, paired = TRUE))\n\n\n\nEffect sizes were labelled following Funder‚Äôs (2019)\nrecommendations.\nThe Wilcoxon signed rank test with continuity correction testing the\ndifference in ranks between iris\\(Sepal.Width\nand iris\\)Sepal.Length suggests that the effect is negative,\nstatistically significant, and very large (W = 0.00, p < .001; r\n(rank biserial) = -1.00, 95% CI [-1.00, -1.00])\nPearson‚Äôs correlation\nSimilarly we can easily report the results of either parametric\nPearson or non-parametric Spearman or Kendall correlations. Besides,\nreport_table() function allows you to display your result\nas a table instead of text (see Kendall‚Äôs correlation below).\n\n\ncor.test(mtcars$mpg, mtcars$wt) %>% \n  report()\n\n\n\nEffect sizes were labelled following Funder‚Äôs (2019)\nrecommendations.\nThe Pearson‚Äôs product-moment correlation between mtcars\\(mpg and mtcars\\)wt is negative,\nstatistically significant, and very large (r = -0.87, 95% CI [-0.93,\n-0.74], t(30) = -9.56, p < .001)\nSpearman‚Äôs rank correlation\n\n\ncor.test(mtcars$mpg, mtcars$wt, method = \"spearman\") %>% \n  report()\n\n\n\nEffect sizes were labelled following Funder‚Äôs (2019)\nrecommendations.\nThe Spearman‚Äôs rank correlation rho between mtcars\\(mpg and mtcars\\)wt is negative,\nstatistically significant, and very large (rho = -0.89, S = 10292.32, p\n< .001)\nKendall‚Äôs rank correlation\n(as a table)\n\n\ncor.test(mtcars$mpg, mtcars$wt, method = \"kendall\") %>% \n  report_table()\n\n\nParameter1 | Parameter2 |   tau |     z |      p |                         Method | Alternative\n-----------------------------------------------------------------------------------------------\nmtcars$mpg |  mtcars$wt | -0.73 | -5.80 | < .001 | Kendall's rank correlation tau |   two.sided\n\nReporting ANOVAs\n\n\naov(wt ~ am + mpg, data = mtcars) %>% \n  report()\n\n\n\nThe ANOVA (formula: wt ~ am + mpg) suggests that:\nThe main effect of am is statistically significant and large (F(1,\n29) = 69.21, p < .001; Eta2 (partial) = 0.70, 95% CI [0.54,\n1.00])\nThe main effect of mpg is statistically significant and large (F(1,\n29) = 46.12, p < .001; Eta2 (partial) = 0.61, 95% CI [0.42,\n1.00])\nEffect sizes were labelled following Field‚Äôs (2013)\nrecommendations.\n2. Reporting Regression Models\nReporting (General) Linear\nModels\n\n\nmodel <- lm(mpg ~ am + hp, data = mtcars)\n\nsummary(model)\n\n\n\nCall:\nlm(formula = mpg ~ am + hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3843 -2.2642  0.1366  1.6968  5.8657 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 26.584914   1.425094  18.655  < 2e-16 ***\nam           5.277085   1.079541   4.888 3.46e-05 ***\nhp          -0.058888   0.007857  -7.495 2.92e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2.909 on 29 degrees of freedom\nMultiple R-squared:  0.782, Adjusted R-squared:  0.767 \nF-statistic: 52.02 on 2 and 29 DF,  p-value: 2.55e-10\n\nBut enough about tests, let‚Äôs see how {report} package handles\nmodels. First of all, the usual summary of a model provides some useful\ninformation but the output is again, not very friendly to the human eye,\nand it is not clear how to describe it. I bet 10 researchers would\nreport these results in 10 slightly different ways. In contrast {report}\npackage provides a standardized way to report model-results, and again\ndelivers soo much more then the classic\nsummary() function. Particularly,\nit describes what kind of model we used, while the\nsummary() does not\nit interprets the \\(R^2\\) and \\(adj.R^2\\), while the summary()\ndoes not\nit uncovers what is behind the mysterious (Intercept),\nwhile the summary() assumes that you already know it\nand finally, report() functions describes parameters\n‚Äúam‚Äù and ‚Äúhp‚Äù by providing the slope with useful! 95% Confidence\nIntervals, while the summary() gives you\nnot really useful Standard Error of the Mean\n\n\nreport(model) \n\n\n\nWe fitted a linear model (estimated using OLS) to predict mpg with am\nand hp (formula: mpg ~ am + hp). The model explains a statistically\nsignificant and substantial proportion of variance (R2 = 0.78, F(2, 29)\n= 52.02, p < .001, adj. R2 = 0.77). The model‚Äôs intercept,\ncorresponding to am = 0 and hp = 0, is at 26.58 (95% CI [23.67, 29.50],\nt(29) = 18.65, p < .001). Within this model:\nThe effect of am is statistically significant and positive (beta =\n5.28, 95% CI [3.07, 7.48], t(29) = 4.89, p < .001; Std. beta = 0.44,\n95% CI [0.25, 0.62])\nThe effect of hp is statistically significant and negative (beta =\n-0.06, 95% CI [-0.07, -0.04], t(29) = -7.50, p < .001; Std. beta =\n-0.67, 95% CI [-0.85, -0.49])\nStandardized parameters were obtained by fitting the model on a\nstandardized version of the dataset. 95% Confidence Intervals (CIs) and\np-values were computed using the Wald approximation.\nIn fact, if used for models, the report() function gives\nyou more than you might want to use. That is why you can apply a\nsummary() command ON TOP OF THE REPORT ü§£\nin order to report only essential information.\n\n\nreport(model) %>% summary()\n\n\n\nWe fitted a linear model to predict mpg with am and hp. The model‚Äôs\nexplanatory power is substantial (R2 = 0.78, adj. R2 = 0.77). The\nmodel‚Äôs intercept is at 26.58 (95% CI [23.67, 29.50]). Within this\nmodel:\nThe effect of am is statistically significant and positive (beta =\n5.28, 95% CI [3.07, 7.48], t(29) = 4.89, p < .001, Std. beta =\n0.44)\nThe effect of hp is statistically significant and negative (beta =\n-0.06, 95% CI [-0.07, -0.04], t(29) = -7.50, p < .001, Std. beta =\n-0.67)\nSimilarly to test results, you can display model results as a table\nwith the report_table() command, or use different parts of\nthe report inside of your text with:\nreport_model(), report_performance(), report_parameters(), report_statistics()\nor report_effectsize() functions:\n\n\nreport_table(model)\n\n\nParameter   | Coefficient |         95% CI | t(29) |      p | Std. Coef. | Std. Coef. 95% CI |    Fit\n-----------------------------------------------------------------------------------------------------\n(Intercept) |       26.58 | [23.67, 29.50] | 18.65 | < .001 |  -2.32e-17 |    [-0.17,  0.17] |       \nam          |        5.28 | [ 3.07,  7.48] |  4.89 | < .001 |       0.44 |    [ 0.25,  0.62] |       \nhp          |       -0.06 | [-0.07, -0.04] | -7.50 | < .001 |      -0.67 |    [-0.85, -0.49] |       \n            |             |                |       |        |            |                   |       \nAIC         |             |                |       |        |            |                   | 164.01\nBIC         |             |                |       |        |            |                   | 169.87\nR2          |             |                |       |        |            |                   |   0.78\nR2 (adj.)   |             |                |       |        |            |                   |   0.77\nSigma       |             |                |       |        |            |                   |   2.91\n\n\n\nreport_model(model)\n\n\n\nlinear model (estimated using OLS) to predict mpg with am and hp\n(formula: mpg ~ am + hp)\n\n\nreport_performance(model)\n\n\n\nThe model explains a statistically significant and substantial\nproportion of variance (R2 = 0.78, F(2, 29) = 52.02, p < .001, adj.\nR2 = 0.77)\n\n\nreport_parameters(model)\n\n\n\nThe intercept is statistically significant and positive (beta =\n26.58, 95% CI [23.67, 29.50], t(29) = 18.65, p < .001; Std. beta =\n-2.32e-17, 95% CI [-0.17, 0.17]), The effect of am is statistically\nsignificant and positive (beta = 5.28, 95% CI [3.07, 7.48], t(29) =\n4.89, p < .001; Std. beta = 0.44, 95% CI [0.25, 0.62]), The effect of\nhp is statistically significant and negative (beta = -0.06, 95% CI\n[-0.07, -0.04], t(29) = -7.50, p < .001; Std. beta = -0.67, 95% CI\n[-0.85, -0.49])\n\n\nreport_statistics(model)\n\n\n\nbeta = 26.58, 95% CI [23.67, 29.50], t(29) = 18.65, p < .001; Std.\nbeta = -2.32e-17, 95% CI [-0.17, 0.17], beta = 5.28, 95% CI [3.07,\n7.48], t(29) = 4.89, p < .001; Std. beta = 0.44, 95% CI [0.25, 0.62],\nbeta = -0.06, 95% CI [-0.07, -0.04], t(29) = -7.50, p < .001; Std.\nbeta = -0.67, 95% CI [-0.85, -0.49]\n\n\nreport_effectsize(model)\n\n\n\nvery small (Std. beta = -2.32e-17, 95% CI [-0.17, 0.17]), small (Std.\nbeta = 0.44, 95% CI [0.25, 0.62]), medium (Std. beta = -0.67, 95% CI\n[-0.85, -0.49])\nLogistic regression\n\n\nm <- glm(am ~ mpg, mtcars, family = binomial) \nreport(m)\n\n\n\nWe fitted a logistic model (estimated using ML) to predict am with\nmpg (formula: am ~ mpg). The model‚Äôs explanatory power is substantial\n(Tjur‚Äôs R2 = 0.37). The model‚Äôs intercept, corresponding to mpg = 0, is\nat -6.60 (95% CI [-12.33, -2.77], p = 0.005). Within this model:\nThe effect of mpg is statistically significant and positive (beta =\n0.31, 95% CI [0.12, 0.59], p = 0.008; Std. beta = 1.85, 95% CI [0.74,\n3.54])\nStandardized parameters were obtained by fitting the model on a\nstandardized version of the dataset. 95% Confidence Intervals (CIs) and\np-values were computed using\nMixed-effects / multilevel\nmodels\n\n\nlibrary(lme4)\n\nmixed_model <- lmer(Reaction ~ Days + (Days | Subject), data = sleepstudy)\n\n\n\n\n\nsummary(mixed_model)\n\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: Reaction ~ Days + (Days | Subject)\n   Data: sleepstudy\n\nREML criterion at convergence: 1743.6\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-3.9536 -0.4634  0.0231  0.4634  5.1793 \n\nRandom effects:\n Groups   Name        Variance Std.Dev. Corr\n Subject  (Intercept) 612.10   24.741       \n          Days         35.07    5.922   0.07\n Residual             654.94   25.592       \nNumber of obs: 180, groups:  Subject, 18\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  251.405      6.825  36.838\nDays          10.467      1.546   6.771\n\nCorrelation of Fixed Effects:\n     (Intr)\nDays -0.138\n\nThe {report} package can report logistic\nregressions, but this is not really surprising. Surprising is\nhowever, that it can also easily handle Linear Mixed-Effects\nmodels, whose popularity and usage is currently exploding. The\nfunction explicitly describes random effects and interprets the\nexplanatory power of the model with two coefficients of determination,\nconditional \\(R^2\\) for the whole model\n(including random effects) and the marginal \\(R^2\\) for only fixed effects, or only\npredictors without random effects.\n\n\nreport(mixed_model)\n\n\n\nWe fitted a linear mixed model (estimated using REML and nloptwrap\noptimizer) to predict Reaction with Days (formula: Reaction ~ Days). The\nmodel included Days and Subject as random effects (formula: ~Days |\nSubject). The model‚Äôs total explanatory power is substantial\n(conditional R2 = 0.80) and the part related to the fixed effects alone\n(marginal R2) is of 0.28. The model‚Äôs intercept, corresponding to Days =\n0, is at 251.41 (95% CI [237.94, 264.87], t(174) = 36.84, p < .001).\nWithin this model:\nThe effect of Days is statistically significant and positive (beta =\n10.47, 95% CI [7.42, 13.52], t(174) = 6.77, p < .001; Std. beta =\n0.54, 95% CI [0.38, 0.69])\nStandardized parameters were obtained by fitting the model on a\nstandardized version of the dataset. 95% Confidence Intervals (CIs) and\np-values were computed using\n\n\nreport_random(mixed_model)\n\n\n\nThe model included Days and Subject as random effects (formula: ~Days\n| Subject)\nReporting\nBayesian Models and Bayesian Mixed-Effects models\nBayesian models can also be reported using the new Sequential\nEffect eXistence and sIgnificance Testing framework, or\nabbreviated - SEXIT üòÅ (I swear I did not made it\nup).\nThis report describes:\nhow Markov Chain Monte Carlo was computed,\nwhich primers were used\nit calculates the coefficient of determination \\(R^2\\) and interprets the performance of the\nmodel\nconducts Bayesian probabilistic hypothesis testing via\nSEXIT üòÅ\nand provides references for reported metrics, so that you can learn\nmore if something is not completely clear\nThe full report for Bayesian models is huge though, but here you also\ncan use parts of the report effortlessly. Just choose what you want to\ninclude into your text with one of the following functions:\nreport_priors(), report_model(), report_performance(), report_parameters(), report_statistics()\nor report_effectsize().\n\n\nlibrary(rstanarm)\n\nbayes_model <- stan_glm(mpg ~ qsec + wt, data = mtcars, refresh = 0)\n\n\n\n\n\nreport(bayes_model)\n\n\n\nWe fitted a Bayesian linear model (estimated using MCMC sampling with\n4 chains of 2000 iterations and a warmup of 1000) to predict mpg with\nqsec and wt (formula: mpg ~ qsec + wt). Priors over parameters were set\nas normal (mean = 0.00, SD = 8.43) and normal (mean = 0.00, SD = 15.40)\ndistributions. The model‚Äôs explanatory power is substantial (R2 = 0.81,\n95% CI [0.70, 0.89], adj. R2 = 0.79). The model‚Äôs intercept,\ncorresponding to qsec = 0 and wt = 0, is at 19.83 (95% CI [9.13,\n30.38]). Within this model:\nThe effect of qsec (Median = 0.92, 95% CI [0.38, 1.47]) has a 99.92%\nprobability of being positive (> 0), 98.95% of being significant\n(> 0.30), and 0.18% of being large (> 1.81). The estimation\nsuccessfully converged (Rhat = 1.000) and the indices are reliable (ESS\n= 3847)\nThe effect of wt (Median = -5.06, 95% CI [-6.04, -4.06]) has a\n100.00% probability of being negative (< 0), 100.00% of being\nsignificant (< -0.30), and 100.00% of being large (< -1.81). The\nestimation successfully converged (Rhat = 1.000) and the indices are\nreliable (ESS = 4376)\nFollowing the Sequential Effect eXistence and sIgnificance Testing\n(SEXIT) framework, we report the median of the posterior distribution\nand its 95% CI (Highest Density Interval), along the probability of\ndirection (pd), the probability of significance and the probability of\nbeing large. The thresholds beyond which the effect is considered as\nsignificant (i.e., non-negligible) and large are |0.30| and |1.81|\n(corresponding respectively to 0.05 and 0.30 of the outcome‚Äôs SD).\nConvergence and stability of the Bayesian sampling has been assessed\nusing R-hat, which should be below 1.01 (Vehtari et al., 2019), and\nEffective Sample Size (ESS), which should be greater than 1000 (Burkner,\n2017).\n\n\nreport_priors(bayes_model)\n\n\n\nPriors over parameters were set as normal (mean = 0.00, SD = 8.43)\nand normal (mean = 0.00, SD = 15.40) distributions\n\n\nreport_model(bayes_model)\n\n\n\nBayesian linear model (estimated using MCMC sampling with 4 chains of\n2000 iterations and a warmup of 1000) to predict mpg with qsec and wt\n(formula: mpg ~ qsec + wt). Priors over parameters were set as normal\n(mean = 0.00, SD = 8.43) and normal (mean = 0.00, SD = 15.40)\ndistributions\n\n\nreport_performance(bayes_model)\n\n\n\nThe model‚Äôs explanatory power is substantial (R2 = 0.81, 95% CI\n[0.70, 0.89], adj. R2 = 0.79)\n\n\nreport_parameters(bayes_model)\n\n\n\nThe intercept (Median = 19.83, 95% CI [9.13, 30.38]) has a 99.92%\nprobability of being positive (> 0), 99.92% of being significant\n(> 0.30), and 99.80% of being large (> 1.81). The estimation\nsuccessfully converged (Rhat = 1.000) and the indices are reliable (ESS\n= 3758), The effect of qsec (Median = 0.92, 95% CI [0.38, 1.47]) has a\n99.92% probability of being positive (> 0), 98.95% of being\nsignificant (> 0.30), and 0.18% of being large (> 1.81). The\nestimation successfully converged (Rhat = 1.000) and the indices are\nreliable (ESS = 3847), The effect of wt (Median = -5.06, 95% CI [-6.04,\n-4.06]) has a 100.00% probability of being negative (< 0), 100.00% of\nbeing significant (< -0.30), and 100.00% of being large (< -1.81).\nThe estimation successfully converged (Rhat = 1.000) and the indices are\nreliable (ESS = 4376)\n\n\nreport_statistics(bayes_model)\n\n\n\nMedian = 19.83, 95% CI [9.13, 30.38], pd = 99.92%, 0% in ROPE; Std.\nbeta = -3.21e-03, 95% CI [-0.16, 0.15]; Rhat = 1.00, ESS = 3757.53,\nMedian = 0.92, 95% CI [0.38, 1.47], pd = 99.92%, 9.24% in ROPE; Std.\nbeta = 0.27, 95% CI [0.11, 0.44]; Rhat = 1.00, ESS = 3846.97, Median =\n-5.06, 95% CI [-6.04, -4.06], pd = 100%, 0% in ROPE; Std. beta = -0.82,\n95% CI [-0.98, -0.66]; Rhat = 1.00, ESS = 4375.77\n\n\nreport_effectsize(bayes_model)\n\n\n\nvery small (Std. beta = -3.67e-04, 95% CI [-0.15, 0.17]), small (Std.\nbeta = 0.27, 95% CI [0.11, 0.44]), large (Std. beta = -0.82, 95% CI\n[-0.98, -0.66])\n\n\nreport_table(bayes_model)\n\n\nParameter   | Median |         95% CI |     pd | % in ROPE |  Rhat |     ESS |                   Prior | Std. Median | Std_Median 95% CI |    Fit\n-------------------------------------------------------------------------------------------------------------------------------------------------\n(Intercept) |  19.83 | [ 9.13, 30.38] | 99.92% |        0% | 1.000 | 3758.00 | Normal (20.09 +- 15.07) |   -6.19e-04 |    [-0.15,  0.16] |       \nqsec        |   0.92 | [ 0.38,  1.47] | 99.92% |     9.24% | 1.000 | 3847.00 |   Normal (0.00 +- 8.43) |        0.27 |    [ 0.11,  0.44] |       \nwt          |  -5.06 | [-6.04, -4.06] |   100% |        0% | 1.000 | 4376.00 |  Normal (0.00 +- 15.40) |       -0.82 |    [-0.98, -0.66] |       \n            |        |                |        |           |       |         |                         |             |                   |       \nELPD        |        |                |        |           |       |         |                         |             |                   | -79.08\nLOOIC       |        |                |        |           |       |         |                         |             |                   | 158.17\nWAIC        |        |                |        |           |       |         |                         |             |                   | 157.82\nR2          |        |                |        |           |       |         |                         |             |                   |   0.81\nR2 (adj.)   |        |                |        |           |       |         |                         |             |                   |   0.79\nSigma       |        |                |        |           |       |         |                         |             |                   |   2.66\n\nExotic and not-supported models\nüò® ???\nDespite the large number of models supported, at the time of making\nthis blog (June 2022), {report} package doesn‚Äôt support all the models\nin the world. But it evolves very quick. For instance when you found\nsimilar ‚ÄúError message‚Äù to the one you see on the screen, go to this\nlink and let the authors know, what kind of models you want {report}\npackage to be able to handle.\n\n\nd <- foreign::read.dta(\"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\")\n\nm <- nnet::multinom(prog ~ ses + write, d)\n\n\n# weights:  15 (8 variable)\ninitial  value 219.722458 \niter  10 value 179.985215\nfinal  value 179.981726 \nconverged\n\nreport(m)\n\n\nError in report.default(m): Oops, objects of class [multinom, nnet] are not supported (yet) by report() :(\n\nWant to help? Check out https://easystats.github.io/report/articles/new_models.html\n\n3. Reporting Data\nAs text\nNow, you can also quickly describe your data by simply using {report}\nfunction on your dataset. It provides the most common descriptive\nstatistics for every variable, doesn‚Äôt matter numeric or\ncategorical.\n\n\nreport(iris)\n\n\n\nThe data contains 150 observations of the following 5 variables:\nSepal.Length: n = 150, Mean = 5.84, SD = 0.83, Median = 5.80, MAD =\n1.04, range: [4.30, 7.90], Skewness = 0.31, Kurtosis = -0.55, 0%\nmissing\nSepal.Width: n = 150, Mean = 3.06, SD = 0.44, Median = 3.00, MAD =\n0.44, range: [2, 4.40], Skewness = 0.32, Kurtosis = 0.23, 0%\nmissing\nPetal.Length: n = 150, Mean = 3.76, SD = 1.77, Median = 4.35, MAD =\n1.85, range: [1, 6.90], Skewness = -0.27, Kurtosis = -1.40, 0%\nmissing\nPetal.Width: n = 150, Mean = 1.20, SD = 0.76, Median = 1.30, MAD =\n1.04, range: [0.10, 2.50], Skewness = -0.10, Kurtosis = -1.34, 0%\nmissing\nSpecies: 3 levels, namely setosa (n = 50, 33.33%), versicolor (n =\n50, 33.33%) and virginica (n = 50, 33.33%)\nBesides, {Report} package works perfectly with famous {tidyverse}\npackages, like {dplyr}. Thus, the data can be easily grouped by any\ncategorical variable, e.g.¬†Species, and the descriptive stats for every\nlevel of the grouping variable will be returned:\n\n\niris %>%\n  select(-starts_with(\"Sepal\")) %>%\n  group_by(Species) %>%\n  report() \n\n\nThe data contains 150 observations, grouped by Species, of the following 3 variables:\n\n- setosa (n = 50):\n  - Petal.Length: n = 50, Mean = 1.46, SD = 0.17, Median = 1.50, MAD = 0.15, range: [1, 1.90], Skewness = 0.11, Kurtosis = 1.02, 0 missing\n  - Petal.Width: n = 50, Mean = 0.25, SD = 0.11, Median = 0.20, MAD = 0.00, range: [0.10, 0.60], Skewness = 1.25, Kurtosis = 1.72, 0 missing\n\n- versicolor (n = 50):\n  - Petal.Length: n = 50, Mean = 4.26, SD = 0.47, Median = 4.35, MAD = 0.52, range: [3, 5.10], Skewness = -0.61, Kurtosis = 0.05, 0 missing\n  - Petal.Width: n = 50, Mean = 1.33, SD = 0.20, Median = 1.30, MAD = 0.22, range: [1, 1.80], Skewness = -0.03, Kurtosis = -0.41, 0 missing\n\n- virginica (n = 50):\n  - Petal.Length: n = 50, Mean = 5.55, SD = 0.55, Median = 5.55, MAD = 0.67, range: [4.50, 6.90], Skewness = 0.55, Kurtosis = -0.15, 0 missing\n  - Petal.Width: n = 50, Mean = 2.03, SD = 0.27, Median = 2.00, MAD = 0.30, range: [1.40, 2.50], Skewness = -0.13, Kurtosis = -0.60, 0 missing\n\nAs table\nWant the same as a huge or a minimalistic table? No, problems! Use\nreport_table() or report_sample()\nfunctions.\n\n\niris %>%\n  group_by(Species) %>%\n  report_table()\n\n\nGroup      |     Variable | n_Obs | Mean |   SD | Median |  MAD |  Min |  Max | Skewness | Kurtosis | n_Missing\n---------------------------------------------------------------------------------------------------------------\nversicolor | Sepal.Length |    50 | 5.94 | 0.52 |   5.90 | 0.52 | 4.90 | 7.00 |     0.11 |    -0.53 |         0\nversicolor |  Sepal.Width |    50 | 2.77 | 0.31 |   2.80 | 0.30 | 2.00 | 3.40 |    -0.36 |    -0.37 |         0\nversicolor | Petal.Length |    50 | 4.26 | 0.47 |   4.35 | 0.52 | 3.00 | 5.10 |    -0.61 |     0.05 |         0\nversicolor |  Petal.Width |    50 | 1.33 | 0.20 |   1.30 | 0.22 | 1.00 | 1.80 |    -0.03 |    -0.41 |         0\nvirginica  | Sepal.Length |    50 | 6.59 | 0.64 |   6.50 | 0.59 | 4.90 | 7.90 |     0.12 |     0.03 |         0\nvirginica  |  Sepal.Width |    50 | 2.97 | 0.32 |   3.00 | 0.30 | 2.20 | 3.80 |     0.37 |     0.71 |         0\nvirginica  | Petal.Length |    50 | 5.55 | 0.55 |   5.55 | 0.67 | 4.50 | 6.90 |     0.55 |    -0.15 |         0\nvirginica  |  Petal.Width |    50 | 2.03 | 0.27 |   2.00 | 0.30 | 1.40 | 2.50 |    -0.13 |    -0.60 |         0\n\n\n\nreport_sample(iris, group_by = \"Species\")\n\n\n# Descriptive Statistics\n\nVariable               | setosa (n=50) | versicolor (n=50) | virginica (n=50) | Total (n=150)\n---------------------------------------------------------------------------------------------\nMean Sepal.Length (SD) |   5.01 (0.35) |       5.94 (0.52) |      6.59 (0.64) |   5.84 (0.83)\nMean Sepal.Width (SD)  |   3.43 (0.38) |       2.77 (0.31) |      2.97 (0.32) |   3.06 (0.44)\nMean Petal.Length (SD) |   1.46 (0.17) |       4.26 (0.47) |      5.55 (0.55) |   3.76 (1.77)\nMean Petal.Width (SD)  |   0.25 (0.11) |       1.33 (0.20) |      2.03 (0.27) |   1.20 (0.76)\n\n4. Reporting R\nenvironment, packages, system\nAnd finally, using only one command:\nreport(sessionInfo()) you first report the Statistical\nsoftware you used for data analysis, secondly, you list ALL packages you\nused and lastly you cite all packages you used automatically without any\ntyping mistakes! How cool is that? And if you want to know how to find\nTHE BEST MODEL which you then can report, check out {glmulti}\npackage!\n\n\nreport(sessionInfo())\n\n\n\nAnalyses were conducted using the R Statistical language (version\n4.1.3; R Core Team, 2022) on macOS Big Sur/Monterey 10.16, using the\npackages flextable (version 0.7.0; David Gohel, 2022), Rcpp (version\n1.0.8.3; Dirk Eddelbuettel and Romain Francois, 2011), Matrix (version\n1.4.1; Douglas Bates, Martin Maechler and Mikael Jagan, 2022), lme4\n(version 1.1.29; Douglas Bates et al., 2015), ISLR (version 1.4; Gareth\nJames et al., 2021), rstanarm (version 2.21.3; Goodrich B et al., 2022),\nggplot2 (version 3.3.6; Wickham. ggplot2: Elegant Graphics for Data\nAnalysis. Springer-Verlag New York, 2016.), stringr (version 1.4.0;\nHadley Wickham, 2019), forcats (version 0.5.1; Hadley Wickham, 2021),\ntidyr (version 1.2.0; Hadley Wickham and Maximilian Girlich, 2022),\nreadr (version 2.1.2; Hadley Wickham, Jim Hester and Jennifer Bryan,\n2022), dplyr (version 1.0.9; Hadley Wickham et al., 2022), tibble\n(version 3.1.7; Kirill M√ºller and Hadley Wickham, 2022), purrr (version\n0.3.4; Lionel Henry and Hadley Wickham, 2020), sjPlot (version 2.8.10;\nL√ºdecke D, 2021), report (version 0.5.1; Makowski et al., 2020),\nggstatsplot (version 0.9.3; Patil, 2021) and tidyverse (version\n1.3.1.9000; Wickham et al., 2019).\nReferences\nDavid Gohel (2022). flextable: Functions for Tabular Reporting. R\npackage version 0.7.0. https://CRAN.R-project.org/package=flextable\nDirk Eddelbuettel and Romain Francois (2011). Rcpp: Seamless R and\nC++ Integration. Journal of Statistical Software, 40(8), 1-18, doi:10.18637/jss.v040.i08.\nDouglas Bates, Martin Maechler and Mikael Jagan (2022). Matrix:\nSparse and Dense Matrix Classes and Methods. R package version 1.4-1. https://CRAN.R-project.org/package=Matrix\nDouglas Bates, Martin Maechler, Ben Bolker, Steve Walker (2015).\nFitting Linear Mixed-Effects Models Using lme4. Journal of Statistical\nSoftware, 67(1), 1-48. doi:10.18637/jss.v067.i01.\nGareth James, Daniela Witten, Trevor Hastie and Rob Tibshirani\n(2021). ISLR: Data for an Introduction to Statistical Learning with\nApplications in R. R package version 1.4. https://CRAN.R-project.org/package=ISLR\nGoodrich B, Gabry J, Ali I & Brilleman S. (2022). rstanarm:\nBayesian applied regression modeling via Stan. R package version 2.21.3\nhttps://mc-stan.org/rstanarm.\nH. Wickham. ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York, 2016.\nHadley Wickham (2019). stringr: Simple, Consistent Wrappers for\nCommon String Operations. R package version 1.4.0. https://CRAN.R-project.org/package=stringr\nHadley Wickham (2021). forcats: Tools for Working with Categorical\nVariables (Factors). R package version 0.5.1. https://CRAN.R-project.org/package=forcats\nHadley Wickham and Maximilian Girlich (2022). tidyr: Tidy Messy\nData. R package version 1.2.0. https://CRAN.R-project.org/package=tidyr\nHadley Wickham, Jim Hester and Jennifer Bryan (2022). readr: Read\nRectangular Text Data. R package version 2.1.2. https://CRAN.R-project.org/package=readr\nHadley Wickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller\n(2022). dplyr: A Grammar of Data Manipulation. R package version 1.0.9.\nhttps://CRAN.R-project.org/package=dplyr\nKirill M√ºller and Hadley Wickham (2022). tibble: Simple Data Frames.\nR package version 3.1.7. https://CRAN.R-project.org/package=tibble\nLionel Henry and Hadley Wickham (2020). purrr: Functional\nProgramming Tools. R package version 0.3.4. https://CRAN.R-project.org/package=purrr\nL√ºdecke D (2021). sjPlot: Data Visualization for Statistics in\nSocialScience. R package version 2.8.10, <URL:https://CRAN.R-project.org/package=sjPlot>.\nMakowski, D., Ben-Shachar, M.S., Patil, I. & L√ºdecke, D. (2020).\nAutomated Results Reporting as a Practical Tool to Improve\nReproducibility and Methodological Best Practices Adoption. CRAN.\nAvailable from https://github.com/easystats/report. doi: .\nPatil, I. (2021). Visualizations with statistical details: The\n‚Äòggstatsplot‚Äô approach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167\nR Core Team (2022). R: A language and environment for statistical\ncomputing. R Foundation for Statistical Computing, Vienna, Austria. URL\nhttps://www.R-project.org/.\nWickham et al., (2019). Welcome to the tidyverse. Journal of Open\nSource Software, 4(43), 1686, https://doi.org/10.21105/joss.01686\nYou can also use parts of sessionInfo(), but why would\nyou? ü§ì\n\n\nreport_system()\n\n\n\nAnalyses were conducted using the R Statistical language (version\n4.1.3; R Core Team, 2022) on macOS Big Sur/Monterey 10.16\n\n\nreport_packages()\n\n\n\nflextable (version 0.7.0; David Gohel, 2022), Rcpp (version 1.0.8.3;\nDirk Eddelbuettel and Romain Francois, 2011), Matrix (version 1.4.1;\nDouglas Bates, Martin Maechler and Mikael Jagan, 2022), lme4 (version\n1.1.29; Douglas Bates et al., 2015), ISLR (version 1.4; Gareth James et\nal., 2021), rstanarm (version 2.21.3; Goodrich B et al., 2022), ggplot2\n(version 3.3.6; Wickham. ggplot2: Elegant Graphics for Data Analysis.\nSpringer-Verlag New York, 2016.), stringr (version 1.4.0; Hadley\nWickham, 2019), forcats (version 0.5.1; Hadley Wickham, 2021), tidyr\n(version 1.2.0; Hadley Wickham and Maximilian Girlich, 2022), readr\n(version 2.1.2; Hadley Wickham, Jim Hester and Jennifer Bryan, 2022),\ndplyr (version 1.0.9; Hadley Wickham et al., 2022), tibble (version\n3.1.7; Kirill M√ºller and Hadley Wickham, 2022), purrr (version 0.3.4;\nLionel Henry and Hadley Wickham, 2020), sjPlot (version 2.8.10; L√ºdecke\nD, 2021), report (version 0.5.1; Makowski et al., 2020), ggstatsplot\n(version 0.9.3; Patil, 2021), R (version 4.1.3; R Core Team, 2022),\ntidyverse (version 1.3.1.9000; Wickham et al., 2019)\n\n\ncite_packages()\n\n\n\nDavid Gohel (2022). flextable: Functions for Tabular Reporting. R\npackage version 0.7.0. https://CRAN.R-project.org/package=flextable, Dirk\nEddelbuettel and Romain Francois (2011). Rcpp: Seamless R and C++\nIntegration. Journal of Statistical Software, 40(8), 1-18, doi:10.18637/jss.v040.i08., Douglas Bates, Martin\nMaechler and Mikael Jagan (2022). Matrix: Sparse and Dense Matrix\nClasses and Methods. R package version 1.4-1. https://CRAN.R-project.org/package=Matrix, Douglas\nBates, Martin Maechler, Ben Bolker, Steve Walker (2015). Fitting Linear\nMixed-Effects Models Using lme4. Journal of Statistical Software, 67(1),\n1-48. doi:10.18637/jss.v067.i01., Gareth James, Daniela\nWitten, Trevor Hastie and Rob Tibshirani (2021). ISLR: Data for an\nIntroduction to Statistical Learning with Applications in R. R package\nversion 1.4. https://CRAN.R-project.org/package=ISLR, Goodrich B,\nGabry J, Ali I & Brilleman S. (2022). rstanarm: Bayesian applied\nregression modeling via Stan. R package version 2.21.3 https://mc-stan.org/rstanarm., H. Wickham. ggplot2:\nElegant Graphics for Data Analysis. Springer-Verlag New York, 2016.,\nHadley Wickham (2019). stringr: Simple, Consistent Wrappers for Common\nString Operations. R package version 1.4.0. https://CRAN.R-project.org/package=stringr, Hadley\nWickham (2021). forcats: Tools for Working with Categorical Variables\n(Factors). R package version 0.5.1. https://CRAN.R-project.org/package=forcats, Hadley\nWickham and Maximilian Girlich (2022). tidyr: Tidy Messy Data. R package\nversion 1.2.0. https://CRAN.R-project.org/package=tidyr, Hadley\nWickham, Jim Hester and Jennifer Bryan (2022). readr: Read Rectangular\nText Data. R package version 2.1.2. https://CRAN.R-project.org/package=readr, Hadley\nWickham, Romain Fran√ßois, Lionel Henry and Kirill M√ºller (2022). dplyr:\nA Grammar of Data Manipulation. R package version 1.0.9. https://CRAN.R-project.org/package=dplyr, Kirill M√ºller\nand Hadley Wickham (2022). tibble: Simple Data Frames. R package version\n3.1.7. https://CRAN.R-project.org/package=tibble, Lionel Henry\nand Hadley Wickham (2020). purrr: Functional Programming Tools. R\npackage version 0.3.4. https://CRAN.R-project.org/package=purrr, L√ºdecke D\n(2021). sjPlot: Data Visualization for Statistics in\nSocialScience. R package version 2.8.10, <URL:https://CRAN.R-project.org/package=sjPlot>.,\nMakowski, D., Ben-Shachar, M.S., Patil, I. & L√ºdecke, D. (2020).\nAutomated Results Reporting as a Practical Tool to Improve\nReproducibility and Methodological Best Practices Adoption. CRAN.\nAvailable from https://github.com/easystats/report. doi: ., Patil, I.\n(2021). Visualizations with statistical details: The ‚Äòggstatsplot‚Äô\napproach. Journal of Open Source Software, 6(61), 3167, doi:10.21105/joss.03167,\nR Core Team (2022). R: A language and environment for statistical\ncomputing. R Foundation for Statistical Computing, Vienna, Austria. URL\nhttps://www.R-project.org/., Wickham et al., (2019).\nWelcome to the tidyverse. Journal of Open Source Software, 4(43), 1686,\nhttps://doi.org/10.21105/joss.01686\nFurther readings and\nreferences\nhttps://easystats.github.io/report/index.html\nAll functions from {report} package: https://easystats.github.io/report/reference/index.html\nIf you think, I missed something, please comment on it, and I‚Äôll\nimprove this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-06-18-report/thumbnail_report.png",
    "last_modified": "2022-07-11T09:27:38+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-05-31-glmulti/",
    "title": "R package reviews {glmulti} find the best model!",
    "description": "‚ÄúAll models are wrong, some are useful‚Äù - said a statistician George Box. And he was right. Thus, in this post we'll find the set of very useful models from the set of all possible models and will be able to choose THE MOST USEFUL model which adressed all our questions.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-06-09",
    "categories": [
      "videos",
      "statistics",
      "machine learning",
      "R package reviews",
      "models"
    ],
    "contents": "\n\nContents\nThis post as a\nvideo (will be added soon)\nWhy do we need {glmulti}?\nStepwise variable\nselection approach\n‚ÄúBrute force‚Äù approach\nwith {glmulti}\nHow to\ncompute glmulti to find the best model\nPerformance impovement\ntechniques\n1. Remove unnecessary\nterms\n2. Use genetic algorithm\n\nWhat the hell is THE\nBEST model then ???\nSome exotic\napplications: GLMER or multinom\nExtract results\nProblems\nFurther readings and\nreferences\n\nThis post as a video\n(will be added soon)\nI recommend to watch a video first, because I highlight things I talk\nabout. It‚Äôs less then ‚Ä¶ minutes long.\n\n\n\n\n\n\n\nWhy do we need {glmulti}?\nThe goal of ANY model is to explain a dependent variable by several\nindependent variables, sometimes called predictors. But which predictors\nare useful(?) and how many should we include into our model(?), is\nusually unknown. These questions are important, because if we take to\nmany predictors, we‚Äôll overfit the model and explain the noise in the\ndata instead of uncovering true relationships. While, if we include only\na few predictors into our model, we‚Äôll underfit the model and probably\nmiss some potentially important relationships. Thus, we need to find\nTHE BEST model, with an optimal set of\npredictors which explains maximum of our dependent variable, without\nexplaining the noise.\n\nStepwise variable\nselection approach\nOne of the most common solutions for finding THE\nBEST model is a stepwise variable selection.\nBut it‚Äôs not the best solution out there, and here is why. Stepwise\nselection applies two main techniques: forwards and backwards selection.\n\nBut there are two problems with it. First, forwards and\nbackwards approaches would often not converge to the same\nmodel, like in our example. And secondly, even if they converge\nto the same model, this model might not be the optimal one (gray circle\non the picture below). These problems occur simply because stepwise\nselection doesn‚Äôt look at all possible models at the same\ntime. They just remove or add terms one by one, compare two\nmodels, take the best model of the two, remove or add another term\netc..\n\nLoad all needed packages at once, to avoid interruptions.\n\n\nlibrary(car)        # extracts model results\nlibrary(MASS)       # provides \"birthwt\" dataset\nlibrary(ISLR)       # provides \"Wage\" dataset\nlibrary(tictoc)     # checks running time\nlibrary(sjPlot)     # visualizes model results\nlibrary(glmulti)    # finds the BEST model\nlibrary(flextable)  # beautifies tables\nlibrary(tidyverse)  # provides a lot of useful stuff !!! \nlibrary(performance)# checks and compares quality of models\n\ntheme_set(theme_light(base_size = 12)) # beautifies plots\ntheme_update(panel.grid.minor = element_blank())\n\n\n\n\n\n# prepare selection\nfull_model <- glm(mpg ~ (hp + drat + wt + qsec + gear)^2, \n                 data = mtcars, family = gaussian)\n\nnull_model <- glm(mpg ~ 1, data = mtcars, family = gaussian)\n\n# run stepwise selection\noptimal_model_backward <- step(full_model, direction = \"backward\",\n                        scope = list(upper = full_model, lower = null_model))\n\noptimal_model_forward <- step(null_model, direction = \"forward\",\n                        scope = list(upper = full_model, lower = null_model))\n\n\n\n\n\n# compare two final models\nanova(optimal_model_backward, optimal_model_forward, test = \"Chisq\")\n\n\nAnalysis of Deviance Table\n\nModel 1: mpg ~ hp + drat + wt + qsec + gear + hp:drat + hp:wt + hp:qsec + \n    hp:gear + drat:wt + drat:qsec + wt:qsec + wt:gear\nModel 2: mpg ~ wt + hp + qsec + gear + wt:hp\n  Resid. Df Resid. Dev Df Deviance Pr(>Chi)   \n1        18      51.32                        \n2        26     112.06 -8  -60.743  0.00638 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncompare_performance(optimal_model_backward, optimal_model_forward)\n\n\n# Comparison of Model Performance Indices\n\nName                   | Model |     AIC | AIC weights |     BIC | BIC weights |    R2 |  RMSE | Sigma\n------------------------------------------------------------------------------------------------------\noptimal_model_backward |   glm | 135.927 |       0.989 | 157.913 |       0.203 | 0.954 | 1.266 | 1.689\noptimal_model_forward  |   glm | 144.919 |       0.011 | 155.179 |       0.797 | 0.900 | 1.871 | 2.076\n\n‚ÄúBrute force‚Äù approach with\n{glmulti}\nIn contrast, {glmulti} R package builds all possible models\nwith all possible combinations of predictors and, optionally, even their\npairwise interactions. Such approach was called ‚Äúbrute\nforce‚Äù.\n{glmulti} then compares the amount of useful information models\nprovide. Such model comparison is done with the help of information\ncriteria (IC), for example Akaike‚Äôs IC (aic) or Bayesian IC (bic).\nInformation criteria are used instead of other metrics, such as \\(R^2\\), because they show the ‚Äúfitness‚Äù of\nthe model, where this fitness is penalized by the number of predictors a\nmodel incorporates. In contrast to information criteria, \\(R^2\\) will always increase with the\nincreasing number of terms and will eventually overfit the model. And as\nmentioned before, an overfitted model is bad, because it describes the\nnoise rather than genuine relationships between variables. Consequently,\nwe can‚Äôt trust the coefficients and p-values of overfitted models.\n\nThis picture originates from here\n\nThat‚Äôs why we need to create all possible models,\ninstead of using stepwise selection, and we need to compare models using\nInformation Criteria, instead of \\(R^2\\). And while ‚ÄúBrute force‚Äù approach is\ngreat, the number of models to be considered can easily become\nexorbitant. However, there are several possibilities to reduce the\nnumber of models and to decrease calculation time. Let‚Äôs get into the\nCode and see how to do that.\nHow to compute\nglmulti to find the best model\nThe code is similar to any other model, you use in R:\nfirst you have the formula with the dependent\nvariable on the left side of the tilde (~), and all possible predictors\non the right side of the tilde. For this example we‚Äôll study the salary\nof 3000 american workers with 5 predictors: jobclass, education, age,\nhealth and health-insurance\nthen we‚Äôll tell R which dataset to use. In this\ncase we‚Äôll use the ‚ÄúWage‚Äù dataset from ISLR package\ncrit specifies the Information\nCriterion to be used. Default is the Akaike IC (aic). Other\noptions are the Bayesian IC (bic), quasi-AIC for overdispersed or count\ndata (qaic and qaicc) and the small-sample corrected AIC (aicc), which I\npersonally prefer, because for big samples it always gets the same\nresult as Akaike‚Äôs IC, while with small samples it performs better\nlevel - argument is important! It specifies weather\nall possible models supposed to be build without interactions (level =\n1) or with interactions (level = 2)\nargument - method - explores the candidate set of\nmodels. Method = ‚Äúd‚Äù counts the number of candidate models without\ncalculating anything. For our example of 5 predictors we‚Äôll have 32\nmodels without interactions and 1921 models with interactions. If method\n= ‚Äúh‚Äù, an exhaustive screening is undertaken, which\nmeans that all possible models will be created. If method = ‚Äúg‚Äù, the\ngenetic algorithm is employed (recommended for large candidate\nsets)\n\n\nglmulti(wage   ~ jobclass + education + age + health + health_ins,\n        data   = Wage, \n        crit   = aicc,       # AICC corrected AIC for small samples\n        level  = 1,          # 2 with interactions, 1 without  \n        method = \"d\",        # \"d\", or \"h\", or \"g\"\n        family = gaussian, \n        fitfunction = glm,   # Type of model (LM, GLM etc.)\n        confsetsize = 100)   # Keep 100 best models\n\n\nInitialization...\nTASK: Diagnostic of candidate set.\nSample size: 3000\n4 factor(s).\n1 covariate(s).\n0 f exclusion(s).\n0 c exclusion(s).\n0 f:f exclusion(s).\n0 c:c exclusion(s).\n0 f:c exclusion(s).\nSize constraints: min =  0 max = -1\nComplexity constraints: min =  0 max = -1\nYour candidate set contains 32 models.\n[1] 32\n\nglmulti(wage   ~ jobclass + education + age + health + health_ins,\n        data   = Wage, \n        crit   = aicc,       # AICC corrected AIC for small samples\n        level  = 2,          # 2 with interactions, 1 without  \n        method = \"d\",        # \"d\", or \"h\", or \"g\"\n        family = gaussian, \n        fitfunction = glm,   # Type of model (LM, GLM, GLMER etc.)\n        confsetsize = 100)   # Keep 100 best models\n\n\nInitialization...\nTASK: Diagnostic of candidate set.\nSample size: 3000\n4 factor(s).\n1 covariate(s).\n0 f exclusion(s).\n0 c exclusion(s).\n0 f:f exclusion(s).\n0 c:c exclusion(s).\n0 f:c exclusion(s).\nSize constraints: min =  0 max = -1\nComplexity constraints: min =  0 max = -1\nYour candidate set contains 1921 models.\n[1] 1921\n\nyou then specify the distribution family and\nthe\nfitfunction, where any function similar to\nlm, glm or glmer can be used\nlastly, confsetsize argument allows you to keep a\nparticular number of the best models, so called - confident set\nof best models. One hundred - is a default value.\nSo, now let‚Äôs run the exhaustive algorithm and see how much time it\ntakes to compute 1921 regressions and to find the BEST model for our 5\npredictors with interactions. ‚Äútic()‚Äù and ‚Äútoc()‚Äù functions from\n{tictoc} package would record running time for us.\nFortunately, the exhaustive method took only 19 seconds. Not bad at\nall, if you ask me. However, I usually have way more then five\npredictors, which could cause performance problems. That‚Äôs why we need\nto talk about the‚Ä¶\n\n\ntic()\n\nh_model <- glmulti(wage ~ jobclass + education + age + health + health_ins,\n          data   = Wage, \n          crit   = aicc,       # AICC corrected AIC for small samples\n          level  = 2,          # 2 with interactions, 1 without  \n          method = \"h\",        # \"d\", or \"h\", or \"g\"\n          family = gaussian, \n          fitfunction = glm,   # Type of model (LM, GLM, GLMER etc.)\n          confsetsize = 100)   # Keep 100 best models\n\ntoc() # 19 sec elapsed: 1921 models \n\n\n\nPerformance impovement\ntechniques\n1. Remove unnecessary terms\nAnd the first one is to remove all unnecessary predictors or\ninteractions. For example a weight and body mass index\n(BMI) provide very similar information - the statisticians would\nsay - they are highly multicollinear. Anyway, if both, weight\nand BMI are included, they would dramatically increase the\nnumber of models without providing any value. Check this out, adding\nonly two additional categorical predictors (maritl & region) into\nthe Wage model above increases the number of models to over 2.5 millions\n(2604485 to be exact, see below). And while it‚Äôs unimaginable to run so\nmany models in our life time, genetic algorithm\nprovides a solution for it.\n\n\nglmulti(wage ~ jobclass + education + age + health + health_ins + maritl + region,\n        data   = Wage, \n        crit   = aicc,       # AICC corrected AIC for small samples\n        level  = 2,          # 2 with interactions, 1 without  \n        method = \"d\",        # \"d\", or \"h\", or \"g\"\n        family = gaussian, \n        fitfunction = glm,   # Type of model (LM, GLM, GLMER etc.)\n        confsetsize = 100,\n        plotty=FALSE)\n\n\nInitialization...\nTASK: Diagnostic of candidate set.\nSample size: 3000\n6 factor(s).\n1 covariate(s).\n0 f exclusion(s).\n0 c exclusion(s).\n0 f:f exclusion(s).\n0 c:c exclusion(s).\n0 f:c exclusion(s).\nSize constraints: min =  0 max = -1\nComplexity constraints: min =  0 max = -1\nYour candidate set contains 2604485 models.\n[1] 2604485\n\n2. Use genetic algorithm\n\nParticularly, having 6 numeric predictors with interactions, the\n‚Äúbrute force‚Äù approach needs almost 3 hours, while genetic algorithm\nruns only 40-80 seconds and produces almost identical results (with\nsometimes slightly worse IC value).\n\n\ntic()\n\ntest_h <- glmulti(mpg ~ hp + drat + wt + qsec + gear, \n                 data   = mtcars, \n                 method = \"h\",       # Exhaustive approach\n                 crit   = aic,      # AICC corrected AIC for small samples\n                 level  = 2,         # 2 with interactions, 1 without\n                 family = gaussian,\n                 fitfunction = glm,  # Type of model (LM, GLM, GLMER etc.)\n                 confsetsize = 100)  # Keep 100 best models\n\ntoc() # 32768 models \"h\" takes 104-109 seconds\n# 6 numeric predictors with interactions produces 2.097.152 models, and \"h\" method takes 9715.466 seconds or ca. 2.7 hours\n\ntic()\n\ntest_g <- glmulti(mpg ~ hp + drat + wt + qsec + gear, \n                 data   = mtcars, \n                 method = \"g\",       # genetic algorithm approach\n                 crit   = aic,      # AICC corrected AIC for small samples\n                 level  = 2,         # 2 with interactions, 1 without\n                 family = gaussian,\n                 fitfunction = glm,  # Type of model (LM, GLM, GLMER etc.)\n                 confsetsize = 100)  # Keep 100 best models\n\ntoc() # 32768 models \"g\" takes 40-59 seconds\n# 6 numeric predictors with interactions produces 2.097.152 models, and \"g\" method takes 40-80 seconds or ca. 1 minute\n\n\n\nSo, if genetic algorithm is sooo cool, why not use genetic algorithm\nall the time? Well, interestingly enough, with categorical predictors,\nhaving a lot of categories, genetic algorithms may perform slower as\ncompared to the exhaustive one. For instance, our Wage-model, which has\nlots of categorical predictors took only 19 second with the exhaustive\nscreening, while needed 117 seconds till genetic algorithm converged,\nso, almost 6 times longer. Moreover, genetic algorithm might have\nconvergence problem and might run indefinitely long, without you having\nany idea of WHEN, or IF it ever stops. And lastly, exhaustive method\nalmost always delivers better IC values. That‚Äôs why I‚Äôd recommend to\nproduce all possible models (aka. using exhaustive screening,\naka. applying ‚Äúbrute force‚Äù approach) whenever possible and\nonly use genetic algorithm for a high number of numeric predictors.\n\n\n\n\nWhat the hell is THE BEST\nmodel then ???\nBy the way, remember, in the beginning of the video I said, that\nstepwise selection is not the best method, implying that {glmulti}\napproach is better? Well, let‚Äôs compare the results of exhaustive and\ngenetic algorithms, to the results of forward and backwards selections\nand see which is a TRULY BEST model:\n\n\noptimal_model_glmulti_exhaustive <- test_h@objects[[1]]\noptimal_model_glmulti_genetic    <- test_g@objects[[1]]\ncompare_performance(optimal_model_glmulti_exhaustive, optimal_model_glmulti_genetic, optimal_model_backward, optimal_model_forward)\n\n\n# Comparison of Model Performance Indices\n\nName                             | Model |     AIC | AIC weights |     BIC | BIC weights |    R2 |  RMSE | Sigma\n----------------------------------------------------------------------------------------------------------------\noptimal_model_glmulti_exhaustive |   glm | 134.734 |       0.391 | 146.460 |       0.496 | 0.932 | 1.547 | 1.750\noptimal_model_glmulti_genetic    |   glm | 134.734 |       0.391 | 146.460 |       0.496 | 0.932 | 1.547 | 1.750\noptimal_model_backward           |   glm | 135.927 |       0.215 | 157.913 |       0.002 | 0.954 | 1.266 | 1.689\noptimal_model_forward            |   glm | 144.919 |       0.002 | 155.179 |       0.006 | 0.900 | 1.871 | 2.076\n\noptimal_model_glmulti_exhaustive$formula\n\n\nmpg ~ 1 + wt + qsec + gear + drat:hp + qsec:wt + gear:wt\n<environment: 0x7ff55fa44e80>\n\noptimal_model_glmulti_genetic$formula\n\n\nmpg ~ 1 + wt + qsec + gear + drat:hp + qsec:wt + gear:wt\n<environment: 0x7ff539a23e08>\n\noptimal_model_backward$formula\n\n\nmpg ~ hp + drat + wt + qsec + gear + hp:drat + hp:wt + hp:qsec + \n    hp:gear + drat:wt + drat:qsec + wt:qsec + wt:gear\n\noptimal_model_forward$formula\n\n\nmpg ~ wt + hp + qsec + gear + wt:hp\n\nAs you can see {glmulti} approach produced lower AIC and much lower\nBIC Information criteria, and interestingly enough, the \\(R^2\\) produced by {glmulti} is right in\nbetween the \\(R^2\\)s of forwards and\nbackwards selections, suggesting that {glmulti} models are neither\nunderfitted not overfitted. Moreover, in our example both exhaustive and\ngenetic algorithms have identical result (will not always be the case)\nand showed three interactions (drat:hp + qsec:wt + gear:wt) to be\nimportant, while backwards selection found 8 interactions to be\nimportant, which to me sound like overfitting, which is in line with\nit‚Äôs highest \\(R^2\\), and forwards\nselection found only one interaction, which looks like underfitting,\nwhich is in line with it‚Äôs lowest \\(R^2\\).\nSo, I hope I could convince you that {glmulti} approach is\nsuperior to the stepwise selection approach and produces a\ntruly BEST model.\n\n\n\n\n\n\n\n\n\n\n\n\nSome exotic\napplications: GLMER or multinom\nAnd while {glmulti} works fine with the classic functions like LM and\nGLM, it can also fit some exotic models, such as ‚Äúmultinomial‚Äù models\nvia Neural Networks from {nnet} package.\n\nlibrary(nnet)\n\nmultinom_glmulti <- glmulti(\n  education ~ wage + jobclass + health, \n  data   = Wage, \n  level  = 2, \n  method = \"h\"\n  fitfunction = multinom)\n\nHere are the predictions of the best multinomial model:\n\n\nplot(effects::allEffects(multinom_glmulti@objects[[1]]),\n     lines = list(multiline = T),\n     confint = list(style = \"auto\"))\n\n\n\n\nAnd lastly, despite the fact there is no straightforward fitting\nfunction for the mixed-effects models, such as GLMER from {lme4}\npackage, we can easily write our own wrapper-function and use it inside\nof {glmulti}:\n\n\nglmer.glmulti<-function(formula, data, random = \"\", ...){\n   glmer(paste(deparse(formula),random),\n         data    = data, REML = F, ...)\n}\n\nmixed_model <- glmulti(\n  y = response ~ predictor_1 + predictor_2 + predictor_3,\n  random  = \"+(1|random_effect)\",\n  crit    = aicc,\n  data    = data,\n  family  = binomial,\n  method  = \"h\",\n  fitfunc = glmer.glmulti,\n  marginality = F,\n  level   = 2 )\n\n\n\nNow, let‚Äôs have a look at the results of our BEST model and interpret\nthem.\nExtract results\nThe output of a {glmulti} analysis is an object containing the\nconfidence set of models (100 best models by default).\nStandard R regression functions like ‚Äúsummary()‚Äù, ‚Äúcoef()‚Äù or ‚Äúplot()‚Äù\ncan all be used to make a multi-model inference. But let‚Äôs start with\nthe brief summary of the results which can be obtained with via\n‚Äúprint()‚Äù command:\n\n\nprint(h_model)\n\n\nglmulti.analysis\nMethod: h / Fitting: glm / IC used: aicc\nLevel: 2 / Marginality: FALSE\nFrom 100 models:\nBest IC: 29793.4306133546\nBest model:\n[1] \"wage ~ 1 + jobclass + education + health + health_ins + age + \"  \n[2] \"    education:jobclass + health_ins:education + education:age + \"\n[3] \"    health:age + health_ins:age\"                                 \nEvidence weight: 0.0786680339413555\nWorst IC: 29801.3206286612\n6 models within 2 IC units.\n74 models to reach 95% of evidence weight.\n\n‚Ä¶ were we see the most important information, such as fitting\nfunction, the information criteria used to rank the models, the formula\nof the best model and even the number of models which as good as the\nbest model. There are 6 models, which we can also see if we plot our\nobject:\n\n\nplot(h_model)\n\n\n\n\nThis plot shows the IC values for all 100 models from the confidence\nset. A horizontal line separates 6 best models, that are less than 2 IC\nunits away from THE BEST model. But what predictors and\ninteractions do those 6 models contain? Using {weightable} function, we\ncan easily display them:\n\n\nweightable(h_model)[1:6,] %>% \n  regulartable() %>%       # beautifying tables\n  autofit()\n\n\nmodelaiccweightswage ~ 1 + jobclass + education + health + health_ins + age + education:jobclass + health_ins:education + education:age + health:age + health_ins:age29,793.430.07866803wage ~ 1 + jobclass + education + health + health_ins + age + education:jobclass + health_ins:education + education:age + health:age29,793.680.06952606wage ~ 1 + jobclass + education + health_ins + age + education:jobclass + health_ins:education + education:age + health:age29,794.400.04836431wage ~ 1 + jobclass + education + health_ins + age + education:jobclass + health_ins:education + education:age + health:age + health_ins:age29,794.430.04776916wage ~ 1 + jobclass + education + health + health_ins + age + education:jobclass + health_ins:education + jobclass:age + education:age + health:age + health_ins:age29,795.310.03070167wage ~ 1 + jobclass + education + health + health_ins + age + education:jobclass + health_ins:jobclass + health_ins:education + education:age + health:age + health_ins:age29,795.360.02990738\n\nHere we see the formulas, Information Criteria and the Akaike weights\nof our 6 best models. The Akaike weight for a particular model shows the\nprobability that the model is the best model out of all models\nconsidered. To say it in a simple lingo - the model with the highest\nweight minimizes the loss of information. So, while the ‚Äúbest‚Äù model has\nthe highest weight, its weight in this example is not substantially\nlarger than that of the second model (and also the third, fourth, and so\non). So, we shouldn‚Äôt be all too certain here that the top model is\nreally the best model in the set. Several models are\nalmost equally plausible. So, which model should we take\nthen?\nIf all 6 models are great, but have different combinations of\npredictors and interactions, figuring out which terms are important may\nhelp to choose the best model. Fortunately for us, the\nplot() command with type=\"s\" argument displays\nthe relative importance of model terms across all models. The importance\nvalue for a particular predictor or interaction is equal to the sum of\nthe weights for the models in which the variable appears. So, a\nvariable that shows up in lots of models with large weights will receive\na high importance value. A vertical line is drawn at 80% (where\nterms to the right of the line are part of 80% of the models), which is\nsometimes used as a cutoff to differentiate between very important and\nless important variables. This threshold is somewhat arbitrary though,\nso that we are free to set it at ‚Ä¶ let‚Äôs say 50% and include all the\npredictors and interactions with the importance above 50% into the final\nmodel.\n\n\nplot(h_model, type = \"s\")\n\n\n\n\nInterestingly, the very first model contains the\nage:health_ins interaction, which has ca. 50% importance. And\nit would be totally fine to go with that. But, since we have so many\nterms with the importance around 80%, I am happy to use only those,\nincluding education:health_insurance interaction and predictor\nhealth, because they are far enough from the rest. And if I\nlook at 6 best models, I‚Äôll see that the second model has exactly those\nterms. The third model is a bit worse because it does not contain\nvariable health, but since health is part of the most important\ninteraction - age:health, I‚Äôd prefer to include it. So, now we\ndid not blindly trust the algorithm and took it‚Äôs BEST\nMODEL, but examined the results carefully and made a grounded\ndecision to take the second model as OUR BEST\nMODEL.\nNow, we can easily interpret and visualize and check assumptions of\nOUR BEST model as we always do:\n\n\nbest_model <- h_model@objects[[2]]\n\ncar::Anova(best_model)\n\n\nAnalysis of Deviance Table (Type II tests)\n\nResponse: wage\n                     LR Chisq Df Pr(>Chisq)    \njobclass                 4.91  1   0.026764 *  \neducation              626.21  4  < 2.2e-16 ***\nhealth                  28.16  1  1.117e-07 ***\nhealth_ins             158.79  1  < 2.2e-16 ***\nage                     90.94  1  < 2.2e-16 ***\njobclass:education      16.14  4   0.002838 ** \neducation:health_ins    10.22  4   0.036890 *  \neducation:age           12.11  4   0.016572 *  \nhealth:age              10.13  1   0.001459 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nplot_model(best_model, type = \"int\") %>% \n  plot_grid()\n\n\n\n\nAnd if you want to learn how to test ALL model-assumptions\nusing only one function, check out {performance} package:\n\n\n\n\n\n\n\nProblems\nSo, while {glmulti} is an amazing package, but it is not perfect and\nhere are three things I found challenging:\nrJava package needed. If you can‚Äôt easily install rJava package\nfrom RStudio, chances are your computed does not have Java installed.\nDoing this can take some time and nerves.\nsome arguments are poorly described (e.g.¬†‚Äúmarginality‚Äù), or\nsimply do not work (e.g.¬†‚Äúexclude‚Äù). Please, let me know in the comments\nbelow, if you managed to use ‚Äúexclude‚Äù.\nand while the coef() and predict()\ncommands are useful multi-model inference tools for models without\ninteractions and only with numeric predictors and could provide\nmulti-model averaged estimates, confidence intervals and predictions, I\nfind them less intuitive for the models with several interactions and\nwith many categorical predictors.\nFurther readings and\nreferences\nhttps://www.jstatsoft.org/article/view/v034i12\nhttps://cran.r-project.org/web/packages/glmulti/glmulti.pdf\nhttps://www.igi-global.com/gateway/chapter/235052\nIf you think, I missed something, please comment on it, and I‚Äôll\nimprove this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-05-31-glmulti/thumbnail_glmulti.png",
    "last_modified": "2022-06-09T20:07:21+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-04-22-tidydata/",
    "title": "Tidy Data and Why We Need It!",
    "description": "Tidy data are easy to manipulate, visualise and analyse, while messy data always interrupts the analysis and invates mistakes. So, tidying up data before analysis pays off a great deal in the long term. In this post you'll learn how do we tidy up data.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-05-23",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrinciples of tidy data\nDoes messy data exist?\n1. One variable is stored in multiple columns, or when column headers are actually values, not variable names\n2. Multiple variables are stored in one column\n3. More then ONE value in ONE cell\n4. Different types of data (numbers & text) are stored in the same column\n5. Understand what a missing value really is\n6. Variables are stored in both rows and columns.\n7. Examples of messyness and the checklist to follow in order to keep the data tidy\n\nConclusion\nReferences\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about and you have visual examples. It‚Äôs ca. 12 minutes long.\n\n\n\n\n\n\n\nWell, if I had to summarize the whole idea of tidy data into one sentence, I‚Äôd say: ‚ÄúWhatever changes in your data, put it into a column.‚Äù\nWhy columns? Because columns are the easiest way to store similar data. Important is that the data in every column is similar, but not identical, so, the data vary. For example age varies from 1 to 100, gender varies from male to female. That‚Äôs actually why a column is always a variable. And a VARIABLE IS what we need to make any type of analysis possible. Let me make tidy data even easier for you.\nPrinciples of tidy data\nThere are only 3 simple principles for tidy data (postulated by the father of tidy data Hadley Wickham):\neach column is a variable,\neach row is an observation, and\neach cell is a single value or only one peace of information\n\nFor the sake of simplicity, let‚Äôs say that any dataset which does not follow these three rules is messy. And the problem with messy data is that it requires different strategies and tons of work to extract different variables, in order to enable different statistical analyses.\nDoes messy data exist?\nThese three rules of tidy data seem so obvious that you might wonder whether messy datasets even exist. Well, unfortunately, most real world data is messy, because there are soo many opportunities to mess things up, and people are usually very creative. Leo Tolstoy once said:\n‚ÄúHappy families are all alike; while every unhappy family is unhappy in its own way‚Äù\n‚ÄúLike families, tidy datasets are all alike; but every messy dataset is messy in its own way.‚Äù - Hadley Wickham\nDoes this mean, that messy data is bad? Absolutely not. Messy datasets might be very convenient for data collection and for having a good overview of the whole dataset, containing all the information we have, including explanatory columns, commentaries, colors etc. . However, while useful for you, it is useless for statistics, which needs only variables and observations. So, you might end up having two tables, one for you, and the other one for statistical analysis. Let me show you the most common cases of messy data and how to fix them. The first one is when‚Ä¶\n1. One variable is stored in multiple columns, or when column headers are actually values, not variable names\n\nDifferent timepoints, for example years or days, are usually stored in different columns. And while it might be convenient for recording data, it‚Äôs hardly possible to analyse it. Why? Well, if we have time, we usually want to study change in something over time, right? But a variable time does not exist if years are spread across different columns. Moreover, if we see this table for the first time, we have no idea what those numbers are, so they are also not a variable, because they are not in a single named column. Making this wide dataset longer creates two new variables which immedeatly allows to study the change in tuberculosis cases over time for every country.\nSo, every combination of a country and a year BECOMES a single observation of tuberculosis cases, and with that - a single row. But if we overdo that, we can end up with a second common problem, where‚Ä¶\n2. Multiple variables are stored in one column\n\nAs mentioned before, the observations within one column suppose to belong together, for example gender with categories ‚Äúfemales‚Äù and ‚Äúmales‚Äù, or countries like Brazil and China. However, it‚Äôs important to separate several categories of the same variable, from the column key, which stores two different variables cases and population in one column. Cases and population do not belong together, and thus can not be analysed. To solve this problem we simply make a long table wider. Now, having two variables we can calculate the rate of tuberculosis by dividing a column cases by the column population. Which would not be possible if both values would be stored in the same cell, being the third common problem‚Ä¶\n3. More then ONE value in ONE cell\n\nThis one can be very sneaky, because it pretends to convey a lot of useful information, like a range of values from zero to five (0-5), undecided values, like 2 or 3 or some borderline values, like <3 or >99. To fix this problem, follow the third principle of tidy data and always put only one value in one cell. And, please, don‚Äôt use any special characters for numeric values, because it will produce the next most common problem, where‚Ä¶\n4. Different types of data (numbers & text) are stored in the same column\n\nData can be either numbers or text (categories). Any text or special characters (like @, ‚Ç¨, *, ^, >, <, +, (), ‚Äù ‚Äú, . etc.) inside of a numeric column converts the whole variable into text. Words like‚Äùunknown‚Äù or ‚Äúmissing‚Äù are the most common examples of text inside of numeric columns. If the value is missing, it‚Äôs better to leave the cell empty. By the way, ironically, missing values can sometimes cause the most damage, so it‚Äôs really important to ‚Ä¶\n5. Understand what a missing value really is\n\nThe real missing value represents a measurement or observation that should have been made, but wasn‚Äôt. One of the non-intentional mistakes is to put zeros (0) into cells with missing values. Think about measuring blood pressure of a cat for example. If cat‚Äôs owner doesn‚Äôt bring the cat to the clinic at Monday, the blood pressure record should remain empty. However, if we put zero instead of a missing value on Monday, that would mean that blood pressure of our cat WAS measured and it WAS zero - so our Monday cat was either dead or a zombie. An example of a real zero is if you measured virus load of that cat on Tuesday, but did not find any virus. In this case a zero means that our cat is absolutely healthy and it‚Äôs important to record that 0! So, a zero conveys a lot of information, while a missing value conveys NO information and should therefore remain empty. But the most complicated form of messy data occurs when (see the picture above)‚Ä¶\n6. Variables are stored in both rows and columns.\n\nFor example, days of the week is our ‚Äúday‚Äù variable which we would like to have in order to study blood pressure of cats over time, so we‚Äôd need to put the days into a single column. Then, if we want to estimate an average for Monday we can‚Äôt do that, because Monday contains two different values, blood pressure and virus load. Thus we need to split the column ‚Äútest‚Äù into two different columns and just move the values. In this tidy data values inside of every columns belong together. Finally, let me quickly show you a few more‚Ä¶\n7. Examples of messyness and the checklist to follow in order to keep the data tidy\n\nremove empty rows\nremove empty and constant columns\nremove merged cells\nuse only the first raw as your header\nstore same data in the same table, for instance:\nif control and treatment are in two tables, put them below each other and create a new variable - ‚Äúgroup‚Äù, because this is exactly what we want - compare groups, then‚Ä¶\nif multiple excel sheets contain similar information, for example multiple years, combine them into one table and create a variable - ‚Äúyears‚Äù, because we often want to study something over time\n\na thing to remember is that statistics has no good taste, thus simple is better then beautiful:\nso, any visual effects like colors, italic or bold font etc. don‚Äôt provide any information, because statistics is blind, thus if colors are important, turn them into variables\nif you want to show that some of the observations aren‚Äôt very good (e.g.¬†calibration error), create a new column ‚Äúerror‚Äù and use 1 every time you aren‚Äôt sure, and 0 every time you are sure about the measurement. That would later allow to exclude these ones easily if we would want to.\n\nuse short, simple, but still clear column names, instead of long explanational names, because when you start to work with them, it will hurt. Too short names, like ‚Äúd‚Äù or ‚Äús‚Äù are also bad, because they don‚Äôt communicate any information, while ‚Äúdays‚Äù or ‚Äúspecies‚Äù do.\ncheck for similar but not identical categories (mostly created by typos), because ‚Äúcat‚Äù, ‚Äúcat_‚Äù with a space and ‚Äúcat.‚Äù with a dot could be considered three different categories. The solution for that is to go to Excel Table > Data > Filter, and to check all categorical variables\nremove all the columns and data which do not participate in the analyses. As mentioned above, you may keep two tables, one for yourself, with all the explanations and colors, and one new minimalistic table for the analyses\nbut please don‚Äôt remove rows or columns only because they have some missing values (empty cells), otherwise we would lose a lot of existing information\ndon‚Äôt summarize, calculate or explain something on the side or below the table, because a software will try to incorporate this information in form of variables or observations\nif you plan to use R software don‚Äôt code categorical variables into numbers, for example instead 1 and 2 for sex, write ‚Äúfemale‚Äù and ‚Äúmale‚Äù. For SPSS or other software you might need to code them, and last but not least ‚Ä¶\nif you think your table will become too long or that tidying up the data is too much work ‚Ä¶ stop thinking that :), because approximately 80% of time in data analysis is spent on cleaning and preparing the data 1 for calculations\nConclusion\nSo, these were just the most common messyness examples I encountered. But the human creativity is not be underestimated. Thus, as you can see - it is much easier to learn what to do, namely only three principles of tidy data, then what not to do.\nReferences\nhttps://vita.had.co.nz/papers/tidy-data.pdf\nDasu T, Johnson T (2003). Exploratory Data Mining and Data Cleaning. Wiley-IEEE.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\nDasu T, Johnson T (2003). Exploratory Data Mining and Data Cleaning. Wiley-IEEE.‚Ü©Ô∏é\n",
    "preview": "posts/2022-04-22-tidydata/tidydata_2.jpeg",
    "last_modified": "2022-11-30T08:47:18+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-03-anova/",
    "title": "R demo | ANOVA (One-Way ) | Fisher's, Welch's, Bayesian, Robust",
    "description": "How does education influence our salary? ANOVA which is just the abbreviation for Analysis Of Variances you see on the thumbnail answeres this question with Frequentists and Bayesian tests. It also privides two different effect sizes, compares education levels pairwisely and even corrects p-values for multiple comparisons. ALL OF THAT is done by this simple command. So, in this blog-post you'll learn how to produce the statistically rich plot, you'll understand when to conduct Welch's ANOVA and when Fisher's ANOVA and you'll know how to interpret every little detail on this plot. Lets get into it.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-05-16",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nGet the data\nCheck normality\nCheck\nHomogeneity of Variances (Homoscedasticity)\nCompute ANOVA\nInterpret the result\nCustomise the result\nA little bit of theory\nabout ANOVA\nOld way to conduct anova in\nR\nANOVA =\nt-test = Linear Regression! Check this out‚Ä¶\nWhat‚Äôs\nnext, or when not to use Repeated Measures ANOVA\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk\nabout. It‚Äôs ca. 8 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nTwo-Samples\nt-Test would help.\nGet the data\n\n\n# install.packages(\"tidyverse\")  # for everything ;)\nlibrary(tidyverse)\n\n#install.packages(\"ISLR\")\nlibrary(ISLR)\n\nset.seed(4)  # for reproducibility\nd <- Wage %>% \n  group_by(education) %>% \n  sample_n(30)\n\n\n\n{ISLR} package provides a {Wage} dataset, with salaries for 5\ndifferent educational groups, starting with people who did not finish a\nhigh school and ending with people having university or doctoral degree.\nWe‚Äôll sample 30 random people from every group and compare their AVERAGE\nsalaries. But wait, is average actually a good choice?\nThat question is very important, because comparing averages only makes\nsense if the data is normally distributed. While if data is not-normally\ndistributed, an average would not represent our data well and ANOVA\nwould be a wrong test - producing wrong result. Kruskal-Wallis test\nwould be better for not normally distributed data, but that‚Äôs a topic\nfor another blog-post.\nCheck normality\nFor now, it‚Äôs obvious that we NEED to check for normality. For that\nwe‚Äôll use the {normality} function from {dlookr} package, which conducts\nShapiro-Wilk normality tests with every educational group. High p-values\nin all groups indicate that our data IS normally distributed, so now we\nare sure that using ANOVA is a right choice.\n\n\n\n# install.packages(\"dlookr\")\nlibrary(dlookr)\nd %>% \n  group_by(education) %>% \n  normality(wage) \n\n\n# A tibble: 5 √ó 5\n  variable education          statistic p_value sample\n  <chr>    <fct>                  <dbl>   <dbl>  <dbl>\n1 wage     1. < HS Grad           0.966  0.427      30\n2 wage     2. HS Grad             0.972  0.606      30\n3 wage     3. Some College        0.980  0.814      30\n4 wage     4. College Grad        0.965  0.412      30\n5 wage     5. Advanced Degree     0.941  0.0942     30\n\nCheck\nHomogeneity of Variances (Homoscedasticity)\nHowever, the normality alone is not enough to make a right decision,\nbecause there are two different ANOVAs: Fisher‚Äôs ANOVA\nfor similar variances across groups and Welch‚Äôs ANOVA\nfor different variances across groups. In fact, the variance is sooo\nimportant that it‚Äôs even part of the name, were Analysis Of\nVariances compares the variances between the\ngroups to the variances within the groups.\nIt‚Äôs also important, because (1) even very different means with huge\nvariance (samples a and b) may not be significantly different (p = 0.1)\nwhile (2) even very similar means with small variance (samples c and d)\ncan be significantly different (p = 0.04). And a classic - Fisher‚Äôs\nANOVA can only be applied when variances are similar! While groups with\ndifferent variances (samples b and c) should be analyzed with Welch‚Äôs\nANOVA.\n\nLevene‚Äôs Test for Homogeneity of Variance helps to decide which ANOVA\nto use. A small p-value of Levene‚Äôs Test tells us that our variances\ndiffer and that we need to use Welch‚Äôs ANOVA.\n\n\n#install.packages(\"car\")\nlibrary(car)\nleveneTest(wage ~ education, d)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(>F)    \ngroup   4  8.4462 3.737e-06 ***\n      145                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNow, having checked both, Normality and Homogeneity of Variance\nassumptions, we are ready to compute Welch‚Äôs ANOVA.\nCompute ANOVA\nAnd the best way to compute ANOVA is the {ggbetweenstats} function\nfrom {ggstatsplot} package, which needs only 5 arguments:\nfirst, our data - d, with\nx-axes - having grouping variable - education,\nand\ny-axes - having salaries\nthen, since our data is normally distributed, we‚Äôll choose a\nparametric type of statistical approach,\nand since our education groups have different variances, we set\nvar.equal argument to FALSE\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nset.seed(4)   # for Bayesian reproducibility of 95% CIs\nggbetweenstats(\n  data = d,\n  x    = education, \n  y    = wage, \n  type = \"parametric\", \n  var.equal = FALSE)\n\n\n\n# you can save the picture in the format and size of your choice\nggsave(filename = \"anova.jpg\", plot = last_plot(), width = 8, height = 7)\n\n\n\nSuch simple command results in this statistically rich and\npublication ready plot! Now, let‚Äôs interpret the results.\nInterpret the result\nWelch‚Äôs F-statistics is the reason ANOVA is\ncalled Analysis of Variances, because F is a ratio of the\nvariance between groups to the variance withing\ngroups. If that ratio is close to one, samples are similar, and\nthe further F-value is from one, the more different are the\nsamples. But F-value by itself can not say how far from\none is far enough, to conclude that this difference is significant.\nThat‚Äôs why F-value and the degrees of freedom were\npreviously used to get a p-value. But nowadays every\nsoftware delivers both F and p-values by default. That is why nowadays\nnobody calculates F values anymore, but if you wanna know how to\ncalculate it, check out the chapter on that below.\nour very small P-value shows a very strong\nevidence against the null hypothesis (H0), that mean salaries\nare similar, in favor of the alternative hypothesis (HAlt),\nthat mean salaries differ. However, a significant P-value only tells you\nthat a difference between groups definitely exists and did not happen\njust by chance, but a p-value can not tell how large this difference\nis.\n\nFortunately, {ggbetweenstats} provides partial omega\nsquared with 95% Confidence Intervals as the measure of the\nEffect Size for ANOVA. The {interpret_omega_squared}\nfunction from {effectsize} package helps to interpret this effect size\nand even provides a reference. Our effect size of 0.34 indicates that\nthe effect of education on salaries is large. For\nexample a person who invested a lot of years and effort into studying\nearns twice as much on average, as the person who did not even finish a\nhigh school. So, the effect size makes total sense to me.\n\n\n# install.packages(\"effectsize\")\nlibrary(effectsize)\n\ninterpret_omega_squared(0.34)\n\n\n[1] \"large\"\n(Rules: field2013)\n\n# ?interpret_omega_squared\n\n\n\n\nBut that‚Äôs not all, {ggbetweenstats} also provides a\nBayesian Effect Size, namely the coefficient of\ndetermination - \\(R^2\\) with\n95% Highest Density Intervals. \\(R^2\\)\nshows the explanatory power of our ANOVA model and\n\\(R^2\\) of 27% is substantial, which\nmeans - we can totally trust these results.\n\n\ninterpret_r2(0.27)\n# ?interpret_r2\n\n\n\n\nMoreover, the Bayes Factor, which is conceptually\nsimilar to the p-value indicates an extreme\nevidence for the alternative hypothesis - that education does\naffect wages ‚Ä¶ which IS in line with a p-value on the top of the\nplot.\n\n\ninterpret_bf(exp(-17.13))\n\n\n[1] \"extreme evidence against\"\n(Rules: jeffreys1961)\n\n# ?interpret_bf\n\n\n\n\nnow, both, Bayes Factor and\np-value tell us that a difference between groups\nexists, however, they don‚Äôt show between which groups exactly. That‚Äôs\nwhy we need to compare every education category to every other education\ncategory pairwisely.\nand luckily for us {ggbetweenstats} automatically knows\nthat we need Games-Howell pairwise Tests for a significant Welch‚Äôs\nANOVA, conducts those tests, displays p-values and even corrects these\np-values for multiple comparisons without any additional code.\nHow cool is that!\n\nby the way, our two global tests are often called with a strange\nname - omnibus test, while the pairwise tests between\ntime-points, are sometimes described in a dead Latin language as -\npost-hoc - which in English means - after the\nevent. So many unnecessary names just confuse people and I hate\nthat. A test we are learning about right now is called ONE-WAY\nANOVA simply because these is only one categorical variable -\neducation. But there is no one-way regression, or one-way t-test ‚Ä¶\nCustomise the result\nHowever, if we want to, we can easily customize the results by using\neither additional code within the function, or code from {ggplot2}\npackage outside of it. For example,\nif you found outliers in your data, you can display them on the plot\nand\nuse a robust ANOVA to minimize the effect of\noutliers,\nhere again, the function automatically uses correct Yuen‚Äôs\ntrimmed means pairwise tests for this robust\nANOVA and corrects p-values for multiple comparisons with a\nHolm method,\nwhich you can easily change to a more famous Bonferroni\ncorrection ‚Ä¶ but I wouldn‚Äôt recommend it, because Bonferroni\ncorrection is too conservative and miss an important discovery. And\nthat‚Äôs exactly what happens with our robust ANOVA, the bonferroni\ncorrection finds only 6 instead of 8 significant pairwise\ncomparisons.\nthen, if you want to display not only significant,\nbut all comparisons,\nif you want to hide either Frequentists or Bayesian statistics, or\nboth‚Ä¶\nor change the appearance of your plot\n‚Ä¶ you can easily do that and much more. Just ask R about\n{ggbetweenstats} by writing a question mark in front of the function and\ntry some things out, I am sure you‚Äôll enjoy it.\n\n\nggbetweenstats(\n  data = d,\n  x    = education, \n  y    = wage, \n  outlier.tagging = T,\n  type = \"robust\")\n\n\n\nggbetweenstats(\n  data = d,\n  x    = education, \n  y    = wage, \n  outlier.tagging = T,\n  type = \"robust\", \n  p.adjust.method = \"bonferroni\", \n  pairwise.display = \"all\",\n  results.subtitle = F,\n  bf.message = F\n) + \n  ylab(\"pay check\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\n?ggbetweenstats\n\n\n\nBut what you could enjoy even more is the Repeated Measures\nANOVA, which you would use if you followed a destiny of the\nsame 30 people throughout their life and see whether their wage increase\nevery time they step up their education.\n\n\nset.seed(1)   # for Bayesian reproducibility of 95% CIs\nggwithinstats(\n    data = d,\n    x    = education, \n    y    = wage, \n    type = \"parametric\", \n    var.equal = FALSE)\n\n\n\n\nA little bit of theory about\nANOVA\nThe variance between groups can be easily calculated by the variance\nof the group means multiplied by the number of observations per group.\nThe within group variance is also called residual\nvariance (or ‚Äúerror‚Äù), because it is what is left when the\ngroup effect is removed. That can be confusing, because there is nothing\nerroneous about within group variance (real biological variance among\nindividuals).\nFor unequal group sized or unequal variances (Welch‚Äôs ANOVA) the\ncalculation get more complex, but there is no need to get there, if you\nunderstood the ‚Äúclassic‚Äù case below, because all of those calculations\nare done by computer.\nThe among-group degrees of freedom is the number of groups minus one.\nThe within-groups degrees of freedom is the total number of\nobservations, minus the number of groups.\n\n\n# overall sums of squares\nd %>% \n  ungroup() %>% \n  mutate(squares = (wage - mean(d$wage))^2) %>% \n  summarise(overall_ss   = sum(squares))\n\n\n# A tibble: 1 √ó 1\n  overall_ss\n       <dbl>\n1    287217.\n\nmeans <- d %>% \n  group_by(education) %>% \n  summarise(n = n(), \n            means = mean(wage))\n\nd <- left_join(d, means)\n\n# wgss - within groups sum of squares\nwgvar <- d %>% \n  mutate(squares = (wage - means)^2) %>% \n  #group_by(education) %>% # already grouped before\n  summarise(wgss   = sum(squares)) %>%    # wgss - within groups sum of squares\n  summarise(ss_total = sum(wgss)) %>% \n  mutate(df = dim(d)[1] - length(unique(d$education))) %>% \n  mutate(var = ss_total / df ) %>% \n  mutate(variance_type = \"withing groups variance\") # wgvar - within group or residual variance\n\n## between groups sums of squares\nbgvar <- means %>% \n  mutate(sample_mean = mean(d$wage)) %>% \n  mutate(ss = n*(sample_mean - means)^2) %>% \n  summarise(ss_total = sum(ss)) %>% \n  mutate(df = length(unique(d$education))-1) %>% \n  mutate(var = ss_total / df ) %>% \n  mutate(variance_type = \"between groups variance\")\n  \n\n# actually it goes easies\nvar(means$means)*30\n\n\n[1] 20741.91\n\nrbind(bgvar, wgvar) %>% \n  mutate(F_value = var[1]/var[2])\n\n\n# A tibble: 2 √ó 5\n  ss_total    df    var variance_type           F_value\n     <dbl> <dbl>  <dbl> <chr>                     <dbl>\n1   82968.     4 20742. between groups variance    14.7\n2  204249.   145  1409. withing groups variance    14.7\n\nOld way to conduct anova in\nR\n\n\naov(wage ~ education, d) %>% summary()\n\n\n             Df Sum Sq Mean Sq F value   Pr(>F)    \neducation     4  82968   20742   14.72 4.05e-10 ***\nResiduals   145 204249    1409                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\noneway.test(wage ~ education, d, var.equal = FALSE) \n\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  wage and education\nF = 10.78, num df = 4.000, denom df = 71.237, p-value =\n6.877e-07\n\nANOVA = t-test =\nLinear Regression! Check this out‚Ä¶\nFor only two groups, ANOVA is exactly the same as the Student‚Äôs\nt-test (that‚Äôs why var.equal = T is there) and, moreover,\nexactly the same as linear regression ;). Just square the t-value from\nthe t-test and you‚Äôll get the F value from ANOVA and regression. And the\np-values are identical ;)\n\n\nt.test(wage ~ jobclass, d, var.equal = T) # square t-value to get F value\n\n\n\n    Two Sample t-test\n\ndata:  wage by jobclass\nt = -2.1854, df = 148, p-value = 0.03044\nalternative hypothesis: true difference in means between group 1. Industrial and group 2. Information is not equal to 0\n95 percent confidence interval:\n -29.634285  -1.489976\nsample estimates:\n mean in group 1. Industrial mean in group 2. Information \n                    107.2967                     122.8588 \n\naov(wage ~ jobclass, d) %>% summary()\n\n\n             Df Sum Sq Mean Sq F value Pr(>F)  \njobclass      1   8978    8978   4.776 0.0304 *\nResiduals   148 278238    1880                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nlm(wage ~ jobclass, d) %>% summary()\n\n\n\nCall:\nlm(formula = wage ~ jobclass, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-84.635 -26.013  -6.122  23.335 170.503 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             107.297      4.759  22.545   <2e-16 ***\njobclass2. Information   15.562      7.121   2.185   0.0304 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 43.36 on 148 degrees of freedom\nMultiple R-squared:  0.03126,   Adjusted R-squared:  0.02471 \nF-statistic: 4.776 on 1 and 148 DF,  p-value: 0.03044\n\nWhat‚Äôs\nnext, or when not to use Repeated Measures ANOVA\nKruskal-Wallis\nif data is not-normally distributed or\nRepeated-Measures\nANOVA if observations are dependend / paired / repeated\nmeasures\nIf you think, I missed something, please comment on it, and I‚Äôll\nimprove this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-04-03-anova/thumbnail.png",
    "last_modified": "2022-05-16T21:52:29+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-04-13-kw/",
    "title": "R demo | Kruskal-Wallis test | How to conduct, visualize, interpret & more üòâ",
    "description": "If we have ordinal or not-normally distributed data, ANOVA might produce a wrong result. That's why we need Kruskal-Wallis test. Kruskal-Wallis test you see on the screen answers two question (1) whether at least one group is different from other groups and (2) between which groups exactly this difference is. So, let's learn how to get and interpret all these results.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-05-16",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nGet the data\nHow to compute\nKruskal‚ÄìWallis test\nInterpretation\nCustomise the result\nKruskal-Wallis test\nfor two groups\nA\nbit of theorie: How Kruskal‚ÄìWallis test works and why it‚Äôs called\n‚Äúrank-sum‚Äù and ‚ÄúH‚Äù\nOld\nway to compute Kruskal-Wallis test and Post-Hoc\nKruskal\nWallis = Mann-Whitney test = Ranked Linear Regression! Check this\nout‚Ä¶\nConclusion\nWhat‚Äôs next?\nOr, don‚Äôt use Kruskal‚ÄìWallis test if:\nFurther readings and\nreferences\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk\nabout. It‚Äôs less then 5 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nTo get the most out of this post, familiarize yourself with one\nparametric method - One-Way\nANOVA and one non-parametric method - Mann-Whitney\nU test.\nGet the data\nFor that we‚Äôll take 50 individuals from 5 different educational\nbackgrounds, compare their salaries and find out whether\neducation matters?\n\nThe picture above is borrowed from here.\n\n\n# install.packages(\"tidyverse\")  # for everything ;)\nlibrary(tidyverse)\n\n# install.packages(\"ISLR\")\nlibrary(ISLR)\n\n# stabilize the output of \"sample_n()\"\nset.seed(1)\nd <- Wage %>% \n  group_by(education) %>% \n  sample_n(50, replace = TRUE)\n\n\n\nHow to compute Kruskal‚ÄìWallis\ntest\nAnd the best way to compute Kruskal-Wallis test IMO is the\n{ggbetweenstats} function from {ggstatsplot} package, which needs only 4\narguments:\nfirst, our data - d, with\nx-axes - having grouping variable - education,\nand\ny-axes - having salaries\nthen, since our data is not-normally distributed or ordinal, we‚Äôll\nchoose a nonparametric type of statistical approach,\nand {ggbetweenstats} automatically takes Kruskal-Wallis test if number\nof groups in ‚Äúeducation‚Äù is higher then two..\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nggbetweenstats(\n  data = d,\n  x    = education, \n  y    = wage, \n  type = \"nonparametric\")\n\n\n\n# you can save the picture in the format and size of your choice\nggsave(filename = \"kw.jpg\", plot = last_plot(), width = 8, height = 7)\n\n\n\nSuch simple command results in this statistically rich and\npublication ready plot! Now, let‚Äôs interpret the results.\nInterpretation\nChi-squared test statistics and the degrees of\nfreedom were previously used to manually calculate p-value, but\nnowadays, since p-values are always calculated by computers, we can\nsafely ignore it\nour very small P-value shows a very strong\nevidence against the null hypothesis (H0), that salaries\nacross all groups are similar, in favor of the alternative hypothesis\n(HAlt), that salaries of at least one group differ. However,\nthere are two problems with a p-value. First, a\nsignificant P-value only tells you that a difference between groups\ndefinitely exists and did not happen just by chance, but a\np-value can not tell how large this difference is.\n\nFortunately, {ggbetweenstats} provides partial epsilon\nsquared with 95% Confidence Intervals as the measure of the\nEffect Size for Kruskal-Wallis test. Our effect size of\n0.36 indicates that the effect of education on salaries is\nlarge. For example a person who studied a lot earns\napproximately twice as much, as the person who did not even finish a\nhigh school. So, the effect size makes total sense to me.\n\n\n# install.packages(\"effectsize\")\nlibrary(effectsize)\n\ninterpret_epsilon_squared(0.36)\n\n\n[1] \"large\"\n(Rules: field2013)\n\n?interpret_epsilon_squared()\n\n\n\na second problem p-value has is that it tell us\nthat a difference between groups exists, but it doesn‚Äôt show\nbetween which groups exactly. That‚Äôs why we need to compare\nevery education category to every other education category\npairwisely.\nand luckily for us {ggbetweenstats} automatically knows\nthat we need Dunn pairwise Tests for a significant Kruskal-Wallis,\nconducts those tests, displays p-values and even corrects these p-values\nfor multiple comparisons without any additional code. How cool\nis that!\n\nCustomise the result\nHowever, if we want to, we can easily customize the results by using\neither additional code within the function, or code from {ggplot2}\npackage outside of it. For example,\nyou can manipulate the data\nyou can display outliers\nyou can change Holm correction for multiple\ncomparisons to a more famous Bonferroni\ncorrection ‚Ä¶ but I wouldn‚Äôt recommend it, because Bonferroni\ncorrection is too conservative and we might miss an important\ndiscovery\nthen, if you want to display not only significant,\nbut all comparisons,\nor change the appearance of your plot\n‚Ä¶ you can easily do that and much more. Just write a question mark in\nfront of {ggbetweenstats} and explore this interesting function.\n\n\nggbetweenstats(\n  data = d %>% mutate(wage = round(wage)),\n  x    = education, \n  y    = wage, \n  outlier.tagging = T,\n  type = \"np\", \n  p.adjust.method = \"bonferroni\",\n  pairwise.display = \"all\"\n) + \n  ylab(\"pay check\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\nggsave(filename = \"kw2.jpg\", plot = last_plot(), width = 8, height = 7)\n\n\n\n\n\n?ggbetweenstats\n\n\n\nKruskal-Wallis test for two\ngroups\nBut what is even more interesting, is the fact, that the two-group\ncase of Kruskal‚ÄìWallis test is identical to the\nWilcoxon rank sum test, because they both compare\nrank-sums of groups, not medians, although medians are\noften displayed. In fact the groups can be significantly different even\nwhen medians are identical, and if you wanna learn more about it, check\nout this video.\n\n\nkruskal.test(wage ~ jobclass, data = d)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  wage by jobclass\nKruskal-Wallis chi-squared = 8.4377, df = 1, p-value =\n0.003675\n\nwilcox.test(wage ~ jobclass, data = d, correct = F)\n\n\n\n    Wilcoxon rank sum test\n\ndata:  wage by jobclass\nW = 6141, p-value = 0.003675\nalternative hypothesis: true location shift is not equal to 0\n\nA\nbit of theorie: How Kruskal‚ÄìWallis test works and why it‚Äôs called\n‚Äúrank-sum‚Äù and ‚ÄúH‚Äù\nIt takes just 4 steps to manually calculate the test manually: 1\nrank values independently of the group\nsum the ranks of every group (\\(R_j\\)). This is where the\nrank-sum part of the name comes from. Also,\naverage the ranks of every group (\\(\\bar{r_j}\\)). The mean\nrank is not needed for test statistics, but will be reported\nlater.\n\\[R_j = \\sum_{i=1}^{n_j}r_i\\]\n\\[\\bar{r_j} = \\frac\n{\\sum_{i=1}^{n_j}r_i}{n_j} \\]\ncalculate test-statistic: H-value for n\n< 5 (per group) or Chi-square for n > 5.\nThis is where the H part of the test name comes\nfrom.\n\\[ H = (N-1)\\frac{\\sum_{i=1}^g\nn_i(\\bar{r}_{i\\cdot} - \\bar{r})^2}{\\sum_{i=1}^g\\sum_{j=1}^{n_i}(r_{ij} -\n\\bar{r})^2} \\]\n\\[ Chi-square = \\frac\n{(N-1)(S_t^2-C)}{S_r^2-C} \\]\nwhere:\nr - rank\nN - total number of observations\nn - number of observations per group\n\\[ S_t^2 = \\sum_{i=1}^{g}\n\\frac{R_i^2}{n_i} \\]\n\\[ S_r^2 = \\sum_{i=1}^{N} {r_{ij}^2}\n\\]\n\\[ C = \\frac {N(N+1)^2}{4}  \\]\ncompare your test statistics to its critical value in the table of\ncritical values (Chi-squared here\nor H-value here)\nto get a p-value, which will answer our initial question about\nwhether the difference is significant, namely whether education\nmatters. If calculated test statistics is greater than or equal\nto the critical value, we reject H0 and\naccept Halt, if lower, we accept\nH0 and reject\nHalt.\nJust my opinion: Calculating test statistic and\nlooking up critical values in tests is not very pragmatic, since every\nstatistical software always delivers both statistics and\np-value. With multiple groups, e.g.¬†Kruskal-Wallis\ntest, it gets really messy. Thus, it is always good to understand\nhow things work, but don‚Äôt feel bad if you never conduct the test\nmanually.\nOld way to\ncompute Kruskal-Wallis test and Post-Hoc\nThe {ggbetweenstats} is definitely the best way, but if you wanna\ncheck the old way of conducting Kruskal-Wallis and make either pairwise\nMann-Whitney U or pairwise Conover tests instead of the Dunn test, use\nthe code below.\n\n\nkruskal.test(wage ~ education, data = d)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  wage by education\nKruskal-Wallis chi-squared = 89.198, df = 4, p-value < 2.2e-16\n\nsource(\"http://www.statmethods.net/RiA/wmc.txt\")\nwmc(wage ~ education, data = d)\n\n\nDescriptive Statistics\n\n       2. HS Grad 1. < HS Grad 3. Some College 4. College Grad\nn        50.00000     50.00000        50.00000        50.00000\nmedian   86.03955     88.73676       106.72398       129.04896\nmad      29.42843     22.18120        17.03991        27.20001\n       5. Advanced Degree\nn                50.00000\nmedian          137.58775\nmad              27.72964\n\nMultiple Comparisons (Wilcoxon Rank Sum Tests)\nProbability Adjustment = holm\n\n           Group.1            Group.2      W            p    \n1       2. HS Grad       1. < HS Grad 1222.0 8.495624e-01    \n2       2. HS Grad    3. Some College  746.5 2.090676e-03  **\n3       2. HS Grad    4. College Grad  449.0 2.374652e-07 ***\n4       2. HS Grad 5. Advanced Degree  297.0 4.596310e-10 ***\n5     1. < HS Grad    3. Some College  702.0 7.974574e-04 ***\n6     1. < HS Grad    4. College Grad  390.0 2.468970e-08 ***\n7     1. < HS Grad 5. Advanced Degree  242.5 3.815055e-11 ***\n8  3. Some College    4. College Grad  783.5 3.933652e-03  **\n9  3. Some College 5. Advanced Degree  506.0 1.765640e-06 ***\n10 4. College Grad 5. Advanced Degree  951.0 7.906592e-02   .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# install.packages(\"conover.test\")\nlibrary(conover.test)\nconover.test(d$wage, d$education)\n\n\n  Kruskal-Wallis rank sum test\n\ndata: x and group\nKruskal-Wallis chi-squared = 89.1976, df = 4, p-value = 0\n\n                           Comparison of x by group                            \n                                (No adjustment)                                \nCol Mean-|\nRow Mean |   1. < HS    2. HS Gr   3. Some    4. Colle\n---------+--------------------------------------------\n2. HS Gr |  -0.174682\n         |     0.4307\n         |\n3. Some  |  -3.816479  -3.641797\n         |    0.0001*    0.0002*\n         |\n4. Colle |  -7.220228  -7.045545  -3.403748\n         |    0.0000*    0.0000*    0.0004*\n         |\n5. Advan |  -9.232507  -9.057824  -5.416027  -2.012279\n         |    0.0000*    0.0000*    0.0000*    0.0226*\n\nalpha = 0.05\nReject Ho if p <= alpha/2\n\nKruskal\nWallis = Mann-Whitney test = Ranked Linear Regression! Check this\nout‚Ä¶\nFor only two groups, Kruskal-Wallis is exactly the same as the\nMann-Whitney U test (in R lingo, the function is\nwilcox.test by they are synonyms, and you‚Äôd need\nvar.equal = T argument to stop the continuity correction)\nand, moreover, almost exactly the same as ranked-linear regression ;).\nAlmost, because the p.value of a regression is not completely\nidentical.\n\n\nwilcox.test(wage ~ jobclass, data = d, correct = F)\n\n\n\n    Wilcoxon rank sum test\n\ndata:  wage by jobclass\nW = 6141, p-value = 0.003675\nalternative hypothesis: true location shift is not equal to 0\n\nkruskal.test(wage ~ jobclass, d) \n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  wage by jobclass\nKruskal-Wallis chi-squared = 8.4377, df = 1, p-value =\n0.003675\n\nlm(rank(wage) ~ jobclass, data = d) %>% summary()\n\n\n\nCall:\nlm(formula = rank(wage) ~ jobclass, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-137.262  -60.322    1.738   55.825  133.825 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)             111.675      6.500  17.180  < 2e-16 ***\njobclass2. Information   26.587      9.014   2.949  0.00349 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 71.21 on 248 degrees of freedom\nMultiple R-squared:  0.03389,   Adjusted R-squared:  0.02999 \nF-statistic: 8.699 on 1 and 248 DF,  p-value: 0.003489\n\nConclusion\nSince the real world data is never perfect, the\nnon-parametric tests are very important tools in a toolbox of\nevery data scientist. Thus, Kruskal-Wallis rank-sum unpaired H\ntest (such a beautiful name! üòÇ) is more powerful then\nANOVA for highly skewed distributions and presence of outliers.\nHoweverm, Kruskal-Wallis looses some information, due to replacement of\nreal data by ranks, and is therefore less powerful for noramlly\ndistributed data. But please, don‚Äôt use it instead of ANOVA just out of\nlaziness to check ANOVAs assumptions, you might get a wrong result.\nWhat‚Äôs next? Or,\ndon‚Äôt use Kruskal‚ÄìWallis test if:\ngroups are dependent (paired), in this case apply either paired\nANOVA if data is normally distributed or\nFriedman\ntest if data is not normally distributed.\nFurther readings and\nreferences\nIf you think, I missed something, please comment on it, and I‚Äôll\nimprove this tutorial.\nThank you for learning!\n\nhttps://www.sciencedirect.com/topics/medicine-and-dentistry/kruskal-wallis-test‚Ü©Ô∏é\n",
    "preview": "posts/2022-04-13-kw/thumbnail.png",
    "last_modified": "2022-05-16T22:24:05+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-03-04-cochran/",
    "title": "R demo | Cochran‚Äôs Q Test + Pairwise McNemar Tests (post-hoc)",
    "description": "Cochran test is an extension of the McNemar test for comparing MORE than two PAIRED categorical samples in which the same individuals appear in each sample. If Cochran test is significant, we'd need to compare samples among each other pairwisely with McNemar tests. So, let's do that.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-04-20",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nGet the data\nVisualize proportions\nCompute Cochran‚Äôs Q Test\nPairwise McNemar Tests\nFinal interpretation\nWhat‚Äôs the next best\nthing to learn?\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk\nabout. It‚Äôs ca. 5 minutes long.\n\n\n\n\n\n\n\nGet the data\n\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n# get the data\nset.seed(9) # for reproducibility \ndata_wide <- data.frame(\n  before = sample(c(\"+\",\"-\",\"+\"), 30, replace = TRUE),\n  month  = sample(c(\"-\",\"+\",\"-\"), 30, replace = TRUE),\n  year   = sample(c(\"-\",\"-\",\"+\"), 30, replace = TRUE)) %>% \n  mutate(id = 1:nrow(.))\n\ndata_long <- data_wide %>% \n  gather(key = \"vaccine_time\", value = \"outcome\", before:year) %>% \n  mutate_all(factor)\n\n\n\nImagine that scientists tried to understand the impact of a\nanti-zombie-vaccine over time. So, they randomly capture 30 ‚Äúvolunteers‚Äù\nand test their zombieness on three different timepoints: first - before\ngiving the anti-zombie-vaccine, secondly - one month after vaccination\nand lastly - one year after vaccination, just to see how long the effect\nof the vaccine holds.\nAny of the two paired time-points could be compared with a McNemar\ntest, but if you have 100s of time-points it‚Äôs a lot of work. So,\nthe lazy scientists found a shortcut - namely Cochran‚Äôs Q Test, which\nchecks whether there is any difference among time-points at all. For\nthat, they gathered all time-points below each other to create only\nthree variables. The vaccine time-points themselves, the test results\nand finally the id of each individual volunteer, which helps not to mess\nthings up.\nCochran‚Äôs Q Test IS useful, because if there is no\ndifference between 100s of time-points, it will give you a high p.value\nand you wouldn‚Äôt need to compare 100s of time-points among each other\npairwisely. That saves time! Hovewer, if Cochran test is significant,\nlike in our example, we‚Äôd need to compare samples among each other\npairwisely with McNemar tests.\nBut what does Cochran test actually do? Visualizing the data helps to\nunderstand it better.\nVisualize proportions\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\nggbarstats(\n  data = data_long, \n  x    = outcome, \n  y    = vaccine_time, \n  paired = T, \n  label = \"both\"\n)\n\n\n\n\nCochran‚Äôs Q Test looks for differences in proportions of three or\nmore paired samples, in which the same individuals appear in each\nsample. So, the Null Hypothesis for Cochran‚Äôs Q Test is\nthat the proportion of ‚Äúsuccesses‚Äù is the same for all groups. While the\nAlternate Hypothesis is that the proportions differ for\nat least one group. Visualized data already shows that there is a\ndifference in proportions, we just don‚Äôt know whether this\ndifference is significant. That‚Äôs why we need to conduct a\ntest!\n\nCompute Cochran‚Äôs Q Test\n\n\n# install.packages(rstatix)\nlibrary(rstatix)\n\ncochran_qtest(data_long, outcome ~ vaccine_time|id)\n\n\n# A tibble: 1 √ó 6\n  .y.         n statistic    df       p method          \n* <chr>   <int>     <dbl> <dbl>   <dbl> <chr>           \n1 outcome    30      9.33     2 0.00940 Cochran's Q test\n\nFor that we‚Äôll use {cochran_qtest} function from {rstatix} package,\nwhich needs only 2 arguments:\nour data in a lazy long format and\nthe formula, which takes\nthe outcome on the left side, which needs to be\nbinomial and mutually exclusive, for example + & -, yes &\nno\ntime-points will get on the right side of the\nformula and\nthe id of each volunteer after a vertical\ndash to make sure results don‚Äôt get messed up between\nindividuals\n\nPairwise McNemar Tests\n\n\npairwise_mcnemar_test(data    = data_long, \n                      formula = outcome ~ vaccine_time|id)\n\n\n# A tibble: 3 √ó 6\n  group1 group2       p  p.adj p.adj.signif method      \n* <chr>  <chr>    <dbl>  <dbl> <chr>        <chr>       \n1 before month  0.00952 0.0286 *            McNemar test\n2 before year   0.48    1      ns           McNemar test\n3 month  year   0.0433  0.13   ns           McNemar test\n\nSince the results are significant, we‚Äôd need to conduct Pairwise\nMcNemar Tests, and luckely for us, {rstatix} package provides\n{pairwise_mcnemar_test} function which does just that and needs the same\ntwo arguments, data and formula. However, this simplicity is dangerous\nfor two reasons.\n\n\npairwise_mcnemar_test(data    = data_long, \n                      formula = outcome ~ vaccine_time|id, \n                      correct = F)\n\n\n# A tibble: 3 √ó 6\n  group1 group2       p  p.adj p.adj.signif method      \n* <chr>  <chr>    <dbl>  <dbl> <chr>        <chr>       \n1 before month  0.00468 0.014  *            McNemar test\n2 before year   0.346   1      ns           McNemar test\n3 month  year   0.0209  0.0627 ns           McNemar test\n\nFirst, the function uses continuity correction by default, which was\nshown to be very conservative by several scientific papers. Thus, if we\nstop the function from using continuity correction with {correct =\nFALSE} argument and compare the results - with and without correction,\nwe‚Äôll see that continuity corrected p-values are higher, which might\nhelp to miss an important discovery, also known as the Type II\nError.\n\n\npairwise_mcnemar_test(data    = data_long, \n                      formula = outcome ~ vaccine_time|id, \n                      correct = F, \n                      p.adjust.method = \"holm\")\n\n\n# A tibble: 3 √ó 6\n  group1 group2       p  p.adj p.adj.signif method      \n* <chr>  <chr>    <dbl>  <dbl> <chr>        <chr>       \n1 before month  0.00468 0.014  *            McNemar test\n2 before year   0.346   0.346  ns           McNemar test\n3 month  year   0.0209  0.0418 *            McNemar test\n\nThe second danger is that this function uses a Bonferroni\ncorrection for multiple comparisons by default, which was also\nshown to be too conservative. A correction for multiple comparisons is\nimportant though, because otherwise we can discover nonsense, also known\nas the Type I Error. Fortunately,\n{pairwise_mcnemar_test} function allows to change the method easily, and\nif we use Holm method we‚Äôd see that the difference between month and\nyear becomes significant.\nFinal interpretation\nSo, let‚Äôs make final conclusions:\na very small p-value of the general Cochran‚Äôs Q Test allows us to\nreject the Null Hypothesis about similar proportions in favor of the\nAlternative Hypothesis - that proportions between time-points\ndiffer.\nthe consecutive Pairwise McNemar Tests show that vaccination\nsignificantly reduces the proportion of zombies as compared to\nnot-vaccinated people and that we need to refresh the vaccine, because\nthe proportion of zombies one year after vaccination significantly\nincreases again.\nbut, if we used a Bonferroni or continuity correction, we would\nthink, that one vaccine is enough and would increase our risk to become\na zombie.\nSo, as you can see, using right statistics is healthy ;) But you\nmight have wondered - what to do - if the outcome is not only + and -\nbut has more levels? Well, then you can use Friedman\ntest.\nWhat‚Äôs the next best thing\nto learn?\nFriedman\ntest\nMixed-Effects logistic regression (not for the beginners in\nstats)\nIf you think, I missed something, please comment on it, and I‚Äôll\nimprove this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-03-04-cochran/thumbnail.png",
    "last_modified": "2022-04-20T21:33:24+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-03-16-mwutest/",
    "title": "R demo | Mann-Whitney U Test = Wilcoxon Rank Sum Test | How to conduct, visualise & interpret ü•≥ What happens if we use a wrong test üò±",
    "description": "Comparing two groups with not-normally disctributed or ordinal data is the reason we need Mann-Whitney U Test instead of t-Test. So, today we'll learn (1) how to conduct and visualize Mann-Whitney U Test you saw on the thumbnail with one simple command, (2) how to interpret all statistical results on that plot and (3) why this test is sometimes called Wilcoxon Rank Sum Test and why we shouldn't use this name",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-04-16",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nGet the data\nCheck normality\nCompute Mann-Whitney U\nTest\nInterpret the result\nConclusion\nDon‚Äôt use\ntwo-samples Wilcoxon-test if:\nReferences and useful\nlinks\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk\nabout. It‚Äôs ca. 5 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nTwo-Samples\nt-Test would help.\nGet the data\n\n\n# install.packages(\"tidyverse\")  # for everything ;)\nlibrary(tidyverse)\n\n#install.packages(\"ISLR\")\nlibrary(ISLR)\n\nset.seed(5)  # for reproducibility\nd <- Wage %>% \n  group_by(jobclass) %>% \n  sample_n(15)\n\n\n\n{ISLR} package provides a {Wage} dataset, with salaries of IT and\nindustrial workers. We‚Äôll take 15 random people from every group and\ncompare their salaries in order to figure out who ears more.\nCheck normality\nBut before using a nonparametric Mann-Whitney U Test, we have\nto make sure that our data is really not-normally distributed.\nBecause using Mann-Whitney U test for normally distributed data, out of\nlaziness to check the normality, might produce a completely wrong\nresult.\nFor that we‚Äôll use the {normality} function from {dlookr} package,\nwhich conducts Shapiro-Wilk normality tests with every group. Low\np-value in one of the group is enough to conclude that our data is\nnot-normally distributed, so now we are sure that using a\nMann-Whitney U test is a right choice.\n\n\n\n# install.packages(\"dlookr\")\nlibrary(dlookr)\nd %>% \n  group_by(jobclass) %>% \n  normality(wage)\n\n\n# A tibble: 2 √ó 5\n  variable jobclass       statistic p_value sample\n  <chr>    <fct>              <dbl>   <dbl>  <dbl>\n1 wage     1. Industrial      0.916 0.165       15\n2 wage     2. Information     0.808 0.00462     15\n\nCompute Mann-Whitney U Test\nAnd the best way to compute our test (in my opinion) is the\n{ggbetweenstats} function from {ggstatsplot} package, which needs only 4\narguments:\nfirst, our data - d, with\nx - as the grouping variable - jobclass, and\ny - being salaries\nfinally, since our data is not-normally distributed, we‚Äôll choose a\nnonparametric type of statistical approach, and\n{ggbetweenstats} automatically uses Mann-Whitney U Test for comparing\ntwo groups.\nSuch simple command results in this statistically rich and\npublication ready plot! Now, let‚Äôs interpret the results.\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nggbetweenstats(\n  data = d,\n  x    = jobclass, \n  y    = wage, \n  type = \"nonparametric\")\n\n\n\nggsave(filename = \"mwu.jpg\", plot = last_plot(), width = 6, height = 4)\n\n\n\nInterpret the result\nW-statistics explains why our test is sometimes\ncalled Wilcoxon Rank Sum test. Namely,\nthe test first ranks our data independently of the\ngroup\nthen sums the ranks for each group, which IS the\nreason the test is called - Rank Sum\nand finally, sums of the ranks are then used to\ncalculate the W-statistics, where\nW-itself obviously originates from Wilcoxon, a\nscientist, who developed the test at roughly the same time as Mann and\nWhitney,\nthe U-statistics in Mann-Whitney U\nname is calculated slightly differently compared with W-statistics, but\nproduces identical p-value\n\n\n\nd %>% \n  select(jobclass, wage) %>% \n  ungroup() %>% \n  mutate(wage_ranks = rank(wage)) %>% \n  group_by(jobclass) %>% \n  summarise(sum_ranks = sum(wage_ranks),\n            n         = n()) %>% \n  mutate(W = sum_ranks - (n*(n+1))/2)\n\n\n# A tibble: 2 √ó 4\n  jobclass       sum_ranks     n     W\n  <fct>              <dbl> <int> <dbl>\n1 1. Industrial       180.    15  59.5\n2 2. Information      286.    15 166. \n\nand that‚Äôs what both W- and U-statistics were\npreviously used for - to look up a p-value in some table. But nowadays\nall statistical software compute p-values by default, so nobody cares\nabout W- or U-statistics anymore. However, the rank\nsums which we just calculated are incredibly\nuseful for understanding the test! And here is why:\n\nour P-value of 0.03 shows a moderate evidence\nagainst the Null Hypothesis (H0), that groups are\nsimilar, in favor of the alternative hypothesis\n(HAlt), that groups differ. This difference\nis often reported as a difference in medians, but here is the catch!\nNo medians were involved when we calculated sums of\nranks! The p-value shows whether there is a difference in sums\nof ranks, not medians! The medians are simply more\nintuitive then ranks for displaying the not-normally\ndistributed nature of our data. And that is why, it can easily happen\nthat medians are identical (especially with ordinal data), while samples\nare still significantly different. I panicked the first time I saw it ;)\nSo, I hope you will not.\n\nAll right, our P-value tells us that there is a difference between\nsalaries of industrial and IT guys. However, a P-value does not\nsay how big this difference is. Moreover, the difference in\nmedians is also useless, because medians were not compared and could be\nidentical.\n\nfortunately, {ggbetweenstats} provides a Rank biserial\ncorrelation coefficient with 95% confidence intervals as the\nmeasure of the effect size, which shows how large the\ndifference is. The {interpret_rank_biserial} function from {effectsize}\npackage helps to interpret this effect size and even provides a\nreference. Our effect size of -0.47 indicates a very\nlarge difference in salaries between our groups.\n\n\n# install.packages(\"effectsize\")\nlibrary(effectsize)\n\ninterpret_rank_biserial(-0.47)\n\n\n[1] \"very large\"\n(Rules: funder2019)\n\n?interpret_rank_biserial\n\n\n\n\nConclusion\nSo, as you can see, both the test itself and it‚Äôs name are quite\nconfusing. And I prefer to stay with Mann-Whitney U\nname, because Wilcoxon Rank Sum Test can be\neasily mistaken with the Wilcoxon Signed-Rank Test,\nwhich can only be applied for paired samples. People often say - they\nused Wilcoxon test, but when I ask ‚Äúwhich of both they\nmean‚Äù, they mostly don‚Äôt know. But it‚Äôs very important to\nknow!!!, because otherwise, as the p-value shows, we can get a\ncompletely opposite result.\n\n\nwilcox.test(data = d, wage ~ jobclass)\n\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  wage by jobclass\nW = 59.5, p-value = 0.02931\nalternative hypothesis: true location shift is not equal to 0\n\nwilcox.test(data = d, wage ~ jobclass, paired = T)\n\n\n\n    Wilcoxon signed rank exact test\n\ndata:  wage by jobclass\nV = 30, p-value = 0.0946\nalternative hypothesis: true location shift is not equal to 0\n\nHowever, if your data is paired and you want to understand Paired\nWilcoxon Test really well, check out the code below and this video.\n\n\nggwithinstats(\n  data = d,\n  x    = jobclass, \n  y    = wage, \n  type = \"nonparametric\")\n\n\n\n\nDon‚Äôt use two-samples\nWilcoxon-test if:\nsamples are small (n<30) and normally distributed (or big and\nnear normal). In this case use the more powerful two-samples\nt-test\ndata is not-normally distributed and you have more then two\ngroups to compare to, then use Kruskal-Wallis test\nIf you think, I missed something, please comment on it, and I‚Äôll\nimprove this tutorial.\nThank you for learning!\nReferences and useful links\nhttps://www.stat.auckland.ac.nz/~wild/ChanceEnc/Ch10.wilcoxon.pdf\nhttps://datatab.net/tutorial/mann-whitney-u-test\n\n\n\n",
    "preview": "posts/2022-03-16-mwutest/thumbnail.png",
    "last_modified": "2022-04-16T14:48:54+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-01-05-correlationmatrixinr/",
    "title": "R demo | Correlation Matrix | Danger or opportunity?",
    "description": "Having several numeric variables, we often wanna know which of them are correlated and how. Correlation Matrix seems to be a good solution for it. But drawing conclusions from plain correlation coeffitients and p-values is dangerous, if we don't visualize the data. Let's learn a better way to produce a correlation matrix.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-04-01",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nVisualize distribution!\n1. Linearity assumption\n2. Grouping variables\n& Simpson‚Äôs paradox\n3. Normality assumption\n4. Correct for multiple\ncomparisons\n\nWhat‚Äôs next?\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk\nabout. It‚Äôs less then 5 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nCorrelation\nBlogpost would help.\nVisualize distribution!\n\n\n# install.packages(\"dlookr\")\nlibrary(dlookr)\nplot_correlate(iris)\n\n\n\n\n\n\n# install.packages(\"fastStat\")\nlibrary(fastStat)\ncor_sig_star(iris[, 1:3])\n\n\n\nA {chart.Correlation} function from {PerformanceAnalytics} package\ndoes both (1) provides correlation coefficients with significance and\n(2) plots the data. This plot immediately uncovers 4 reasons why\nwe can‚Äôt trust a plain correlation matrix.\n\n\n# install.packages(\"PerformanceAnalytics\")\nlibrary(\"PerformanceAnalytics\")\nchart.Correlation(iris[, 1:3])\n\n\n\n\n1. Linearity assumption\nFirst, the data is not-linearly distributed, while correlation only\nmakes sense for linear relationships. If fact, we could draw three\nstraight lines in this plot, while our negative correlation coefficient\nassumes only one of them.\n2. Grouping variables &\nSimpson‚Äôs paradox\nSecondly, some data seemed to be clustered into groups. Indeed, the\n‚ÄúIris‚Äù dataset, which you already have in R, has a ‚ÄúSpecies‚Äù column with\n3 different Species of iris plant. So, putting all three species in one\nbasket feels wrong.\n A {ggpairs} command from {GGally} package\naccounts for a grouping variable and conducts correlation analysis for\nall variables and groups at once, while also plotting a correlation line\non top of every group.\n\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse) # for \"aes()\" & later for \"%>%\" \n\n# install.packages(\"GGally\")\nlibrary(GGally)\nggpairs(iris, \n        columns = 1:3, \n        aes(colour=Species),\n        lower = list(continuous = \"smooth\"),\n        upper = list(continuous = wrap(\"cor\", \n                         method = \"pearson\")))\n\n\n\n\nThese plots reveal that two of the negative coefficients before\nsplitting into groups show the opposite of what is actually true,\nnamely, the correlation within every group IS\npositive.\nMoreover, NOT-significant-negative-correlation between Sepal.Length\nand Sepal.Width turned out to be a significant positive correlation for\nall three species! If I would have accepted the not-significant negative\ncorrelation, I would have made a Type II Error, in other words, I would\nhave missed an important discovery. So, no Nobel Price for me.\nVice Versa, a highly significant correlation between Petal.Length and\nSepal.Length for all three Iris species, is not significant at all for\n‚ÄúSetosa.‚Äù Here I would have made a Type I Error, meaning, I would have\ndiscovered nonsense.\nThe phenomenon, where grouped data shows the opposite correlation\nscenario as compared to ungrouped data is known as Simpson‚Äôs\nParadox, also called Yule-Simpson effect.\nBut, that‚Äôs not all! The third problem of not visualizing the data is\nthat we have no idea about the normality of distribution, so we actually\ndon‚Äôt know which Correlation method to use, Pearson, Spearman or\nsomething else.\n3. Normality assumption\nBut if we look at histograms from {chart.Correlation} function, we\nimmediately see that two of variables, Sepal.Length and Petal.Length,\nare not-normally distributed. Shapiro-Wilk Normality Tests confirm this\nintuition with low p-values.\n\n\n# install.packages(\"flextable\")\nlibrary(flextable)\n\niris %>% \n  normality() %>% \n  mutate(across(is.numeric, ~round(., 3))) %>%\n  regulartable()\n\n\nvarsstatisticp_valuesampleSepal.Length0.9760.010150Sepal.Width0.9850.101150Petal.Length0.8760.000150Petal.Width0.9020.000150\n\nSo, we could have applied a non-parametric Spearman or Kendall\ncorrelation methods, but if we check the normality for every Species,\nwhich are clustered in groups, we‚Äôll see that ALL OF THEM are perfectly\nnormally distributed, so, we should use a parametric Pearson correlation\nanalysis to ensure the highest statistical power! Otherwise, we‚Äôll again\neither miss a discovery, or discover complete nonsense.\n\n\niris %>% \n  group_by(Species) %>% \n  select(1,3) %>% \n  normality() %>%  \n  mutate(across(is.numeric, ~round(., 3))) %>%\n  regulartable()\n\n\nvariableSpeciesstatisticp_valuesampleSepal.Lengthsetosa0.9780.46050Sepal.Lengthversicolor0.9780.46550Sepal.Lengthvirginica0.9710.25850Petal.Lengthsetosa0.9550.05550Petal.Lengthversicolor0.9660.15850Petal.Lengthvirginica0.9620.11050\n\n4. Correct for multiple\ncomparisons\nThe last thing which makes using plain correlation matrix\nstatistically dangerous is that you have multiple comparisons without\nadjusting p-values for them, which again, increases the probability to\nfind nonsense. A {grouped_ggcorrmat} function from {ggstatsplot} package\nhelps with that. Within this function your can specify only 3 argument\nto get all the results:\nfirst, the data - ‚ÄúIris‚Äù,\nthen ‚Äútype‚Äù of correlation, we‚Äôll go with a ‚Äúparametric‚Äù Pearsons\nmethod since we know that our data is linearly and normally distributed\nand\nthe grouping variable, which is ‚ÄúSpecies‚Äù in our case\nHere you‚Äôll see:\nhow strong the correlation is,\nin which direction it goes and\nwhether correlation is significant or not after p-values were\nadjusted for multiple comparisons.\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\ngrouped_ggcorrmat(\n  iris,\n  type = \"parametric\",\n  grouping.var = Species, \n  plotgrid.args = list(nrow = 2)) # looks better\n\n\n\n\nWhat‚Äôs next?\nAs you can see, while correlation matrix is an useful Exploratory\nTool, we shouldn‚Äôt trust the numbers too quickly. We need to (1)\nvisualize the data, (2) check the assumptions of normality and (3)\nlinearity, (4) check for grouping variables and (5) correct for multiple\ncomparisons. And since some data might be suitable for parametric\nanalysis, while some not, it‚Äôs usually better to make a deeper dive into\nevery single correlation between only two numeric variables, which you\ncan learn from this short\nvideo, where I showed how to produce this statistically rich plot\nusing only one command and how to interpret all these results.\nStart to learn about linear regression, you won‚Äôt regret it\n;)\nIf you think, I missed something, please comment on it, and I‚Äôll\nimprove this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-01-05-correlationmatrixinr/thumbnail.jpg",
    "last_modified": "2022-04-01T13:25:49+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-11-ttest/",
    "title": "R demo | Two-Samples t-Test | Student's & Welch's | How to conduct, visualise, interpret | What happens if we use a wrong test üò±",
    "description": "Two-samples t-test can answer useful questions, for example - where can we get more money, working in a factory or in the IT-industry? So, let's learn (1) how to make sure t-test is a CORRECT test for our data, (2) how to get all these results with one simple command, (3) how to interpret all these results and (4) finally see what happens if we choose a wrong test.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-03-22",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nGet the data\nCheck normality\nCheck Homogeneity of Variances\nCompute A Correct Two-Samples t-Test\nInterpret the result\nFinal conclusion\n\nWhat happens if we choose a wrong test?\n1. Taking non-parametric test out of pure laziness\n2. Wrong homogeneity of variances\n3. Taking paired test (hopefully only) by mistake\n\nWhat‚Äôs next, or Don‚Äôt use t-Test if\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 8 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nOne Sample tests would help.\nGet the data\n\n\n# install.packages(\"tidyverse\")  # for everything ;)\nlibrary(tidyverse)\n\nset.seed(1) # for reproducibility\nd <- tibble(\n  IT      = rnorm(n = 10, mean = 130, sd = 60),\n  factory = rnorm(n = 10, mean = 100, sd = 10)\n) %>% \n  gather(key = \"job\", value = \"wage\")\n\n\n\nWe‚Äôll compare 10 random IT-workers with 10 random factory workers. First of all, we‚Äôll see, that on average, IT-crowd earns more then folks in the factory. But is average actually a good choice? That question is important, because comparing averages only makes sense if the data is normally distributed. While if data is not-normally distributed, an average would not represent our data well!\n\nCheck normality\nSo, it‚Äôs obvious that we NEED to check for normality. For that we‚Äôll use the {normality} function from {dlookr} package, which conducts Shapiro-Wilk normality tests with every sample. High p-values in both samples indicate that our data IS normally distributed, so now we are sure that using a parametric t-test is a right choice.\n\n\n# install.packages(\"dlookr\")\nlibrary(dlookr)\nd %>% \n  group_by(job) %>% \n  normality(wage)\n\n\n# A tibble: 2 √ó 5\n  variable job     statistic p_value sample\n  <chr>    <chr>       <dbl>   <dbl>  <dbl>\n1 wage     factory     0.895   0.193     10\n2 wage     IT          0.938   0.534     10\n\nCheck Homogeneity of Variances\nHowever, the normality alone is not enough to make a right decision, because there are two different t-tests: Student‚Äôs t-test and Welch‚Äôs t-test. In order to choose the right test, we need to understanding variance of our data. And here is why: (1) even very different means with huge variance (samples a and b) may not be significantly different while (2) even very similar means with small variance (samples c and d) can be significantly different.\n\nSo, the variances between two samples can be either different (sometimes called - heterogen) or similar (sometimes called - homogen). And we need to know which one is true, because a classic Student‚Äôs t-Test can be applied only when variances are similar! While two samples with different variances should be analyzed with Welch‚Äôs t-Test.\nLevene‚Äôs Test for Homogeneity of Variance helps to decide which t-test to use. The {leveneTest} function from {car} package shows a small p-value, which tells us that our variances differ and that we need to use Welch‚Äôs t-Test.\nNow, having checked both assumptions, we are ready to compute Welch‚Äôs t-test.\n\n\n#install.packages(\"car\")\nlibrary(car)\nleveneTest(wage ~ job, d)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(>F)   \ngroup  1  10.579 0.00442 **\n      18                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCompute A Correct Two-Samples t-Test\nAnd the best way to compute our test (in my opinion) is the {ggbetweenstats} function from {ggstatsplot} package, which needs only 5 arguments:\nfirst, our data - d, which we just created, with\nx - as the grouping variable - job, and\ny - being wages\nthen, since our data is normally distributed, we‚Äôll choose a parametric type of statistical approach,\nand since our jobs have different variances, we set var.equal argument to FALSE\nSuch simple command results in this statistically rich and publication ready plot! Now, let‚Äôs interpret the results.\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nset.seed(1)   # for Bayesian reproducibility of 95% CIs\nggbetweenstats(\n  data = d,\n  x    = job, \n  y    = wage, \n  type = \"parametric\", \n  var.equal = FALSE)\n\n\n\nggsave(filename = \"t_test.jpg\", plot = last_plot(), width = 5.1, height = 3)\n\n\n\nInterpret the result\nWelch‚Äôs t-statistics is the measure of similarity between compared samples measured in units of standard error. The further t-value is from zero, the more different are the samples. But t-value by itself can not say how far from zero is far enough, to conclude that this difference is significant. That‚Äôs why t-value and the degrees of freedom were previously used to get a p-value. But nowadays every software delivers p-values by default.\nour P-value of 0.04 shows a moderate evidence against the null hypothesis (H0), that mean salaries are similar, in favor of the alternative hypothesis (HAlt), that average salaries differ. Particularly, IT-crowd gets 35.000 dollars on average more than factory-workers. But is a difference of 35.000 dollars large? A P-value can not tell that. A P-value only checks whether there is a difference, but not how large this difference is.\n\n\nFortunately, {ggbetweenstats} provides Hedges‚Äô g as the measure of the Effect Size, which shows how large the difference in salaries is. Hedges‚Äô g is interpreted in the same way as Cohen‚Äôs d Effect size, but outperforms Cohen‚Äôs d for small samples, like in our example. Our effect size of -0.96 is large, and tells us that the difference of 35.000 dollars is literally - large. Well, the effect size is cool, but not particularly intuitive, right?\n\nLuckely, {ggbetweenstats} provides the Bayesian Difference between our samples with 95% Highest Density Intervals as the second and more intuitive measure of the effect size. A difference of 27.000 bucs per year seems large to me, because I could get a new car for that money ;) (show picture/video of a funny car). The difference is intuitive, but neither difference, not effect size, do not test hypotheses.\nAnd that what Bayes Factor is for. Our Bayes Factor (Jeffreys, 1961), which is conceptually similar to the p-value, is king of ‚Ä¶ small. A Bayes Factor of -0.84 indicates that the difference is Not worth more than a bare mention ‚Ä¶ which IS similar to a moderate evidence against the Null Hypothesis found by frequentists statistics on the top of the plot.\n\nFinal conclusion\nBut how can it be, that both the effect size and the difference are large, while this difference is hardly significant??? Well, this happens often, and here is why. First, some factory workers still earn more then some of the IT guys. For example an older worker on BMW factory, ears definitely more than a younger IT-guy in some start-up. Secondly, we only have 10 people in each sample, which is simply not enough to conclude that our huge difference appeared not by accident. Let me prove it to you. If we double the number of people per group, the frequentists effect size and the Bayesian difference remained almost the same, while both, p-value and Bayes factor became very significant and showed a strong evidence against the null hypothesis in favor of the alternative.\n\n\nset.seed(1) # for reproducibility\nd2 <- tibble(\n  IT      = rnorm(n = 20, mean = 130, sd = 60),\n  factory = rnorm(n = 20, mean = 100, sd = 10)\n) %>% \n  gather(key = \"job\", value = \"wage\")\n\nset.seed(1)   # for Bayesian reproducibility of 95% CIs\nggbetweenstats(\n  data = d2,\n  x    = job, \n  y    = wage, \n  type = \"parametric\", \n  var.equal = FALSE)\n\n\n\n\nThus, a p-value and the effect size never contradict each other, simply because they show different things. And if our p-value is around 0.05, like 0.04 in our example, Ronald Fisher - the father of modern statistics - himself recommended to treat such result as suggestive or inconclusive and collect more data. And that‚Äôs all we can do. What we can not do, however, is to use a wrong test. And there are three ways things can get messed up.\nWhat happens if we choose a wrong test?\n1. Taking non-parametric test out of pure laziness\n\n\n\nggbetweenstats(\n  data = d,\n  x    = job, \n  y    = wage, \n  type = \"nonparametric\")\n\n\n\n\n\n\n\nFirst, if we are lazy to check the normality of data, we‚Äôll go straight to the non-parametric Mann‚ÄìWhitney U Test. Here, a higher then the real p-value failed to reject the Null Hypothesis, failing to find a difference, where difference actually exists. Such mistake is called type II Error, or simply - missing a discovery.\n\n2. Wrong homogeneity of variances\n\n\nggbetweenstats(\n  data = d,\n  x    = job, \n  y    = wage, \n  type = \"parametric\", \n  var.equal = TRUE)\n\n\n\n\n\n\n\nSecondly, if we are lazy to check the similarity of variances, we‚Äôll go straight to the classic Student‚Äôs t-test which assumes equal variance, and produces smaller then real p-value indicating more evidence for the difference then necessary. This mistake is called type I Error, or simply - discovering nonsense.\nFor unequal variances and/or unequal sample sizes Welch‚Äôs t-Test is more reliable and more robust then Student‚Äôs t-Test, while has the same power.\n\n3. Taking paired test (hopefully only) by mistake\nBut that‚Äôs not all!!! We could be even more wrong if we took a PAIRED t-Test, because we would test a completely different Null Hypothesis. Namely, that exactly the same 10 factory workers changed their job to IT and compare their salaries before and after this change. This is not the case in our example, but can be your next experiment. Then, Paired t-Test would be absolutely correct, and if you want to understand it really well, check out this video.\n\n\nggwithinstats(\n  data = d,\n  x    = job, \n  y    = wage, \n  type = \"parametric\")\n\n\n\n\n\n\n\nWhat‚Äôs next, or Don‚Äôt use t-Test if\nsamples are paired. In this case applyPaired t-test\nsamples are small (n<30) and not-normally distributed. In this case use two-samples Mann‚ÄìWhitney U Test.\nIf you need to compare more then two samples, first check whether they are normally distributed, and if they are, go to ANOVA, but if they aren‚Äôt, go to Kruskal Wallis\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-03-11-ttest/thumbnail.png",
    "last_modified": "2022-03-22T08:19:47+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-01-22-pairedsamplesttestinr/",
    "title": "R demo | Paired Samples t-Test | How to conduct, visualise and interpret",
    "description": "Can one week of training significantly improve your number of sit-ups? Well, Paired t-Test can answer this question by comparing your performance Before and After this week. So, let's learn how to produce this statistically rich plot using only one simple command, how to interpret all these results and see what happens if we use a wrong test.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-03-11",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nGet the data\nCheck normality of the difference\nCompute Paired Samples t-Test\nInterpret the result\nWhat happens if we choose a wrong test?\nProof of the concept about the difference between two samples\nOne-Tailed (or One-Sided) Parametric Two-Samples Paired t-Tests\nWhat‚Äôs next, or Don‚Äôt use Paired t-Test if\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 6 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nOne Sample tests would help.\nGet the data\n\n\n# install.packages(\"tidyverse\")  # for everything ;)\nlibrary(tidyverse)\n\n# install.packages(\"BSDA\")       # for Fitness data\nlibrary(BSDA)\n\nView(Fitness)\n\n\n\n{BSDA} package provides a {Fitness} dataset, with the sit-up performance of 9 people before and after one week of training.\n\n\n# make wide format\nd <- Fitness %>% \n  pivot_wider(\n    id_cols     = subject, \n    names_from  = test, \n    values_from = number) %>% \n  mutate(difference =  After - Before)\n\nView(d)\n\n\n\nTo see whether training makes any difference, we‚Äôll calculate this difference by subtracting performance Before from performance After and compare our mean difference to zero. If training didn‚Äôt help, our mean difference would be equal to zero, which becomes our Null Hypothesis (H0). If training made a difference, our mean difference would be lower or, hopefully, higher then zero, which becomes our alternative hypothesis (HAlt). And since this difference is more important then paired samples themselves, we only need to check the normality of the difference, not of both samples. Checking normality is important for choosing a correct test, otherwise we could get wrong result. We‚Äôll see what happens if we choose a wrong test in a moment. Until then‚Ä¶\nCheck normality of the difference\n\n\nshapiro.test(d$difference)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  d$difference\nW = 0.95124, p-value = 0.7037\n\n‚Ä¶ a big p-value of the Shapiro-Wilk normality test indicates that our difference is normally distributed. That‚Äôs why we need a parametric paired t-test, to compare two paired samples. If the difference would have been not-normally distributed, we would have taken a non-parametric Wilcoxon test, which you can learn about from this article.\nFor paired tests, the data needs to be sorted, so that the first observation of the before group, pairs with the first observation of the after group. If our data is sorter, we are ready to compute the test.\nCompute Paired Samples t-Test\nAnd the best way to compute our test (in my opinion) is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:\nfirst, our data - Fitness, then\nx - as the grouping variable,\ny - are the numbers of sit-ups and\nthe type of statistical approach. Since our data was normally distributed, we choose a parametric test, and {ggwithinstats} automatically takes Paired t-Test .\nSuch simple command results in this statistically rich and publication ready plot! Now, let‚Äôs interpret the results.\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nset.seed(1)   # for Bayesian reproducibility\nggwithinstats(\n  data = Fitness,\n  x    = test, \n  y    = number, \n  type = \"parametric\"\n)\n\n\n\nggsave(filename = \"paired_t_test.jpg\", plot = last_plot(), width = 6, height = 4)\n\n\n\nInterpret the result\nStudents t-statistics is the measure of similarity between compared samples measured in units of standard error. The further t-value is from zero, the more different are the samples. But t-value by itself can not say how far from zero is far enough, to conclude that this difference is significant. That‚Äôs why we need a p-value.\n\\[t = \\frac{our.mean - expected.mean}{standart.error} = \\frac{our.mean - expected.mean}{ \\frac{standart.deviation} {\\sqrt sample.size} }\\]\nour P-value of 0.025 tells us that our difference IS significant. It shows a moderate evidence against the null hypothesis (H0) in favor of the alternative hypothesis (HAlt). Particularly, our group of 9 students will make 2 sit-ups more on average after one week of training. But is a difference of only 2 sit-ups large? P-value can not tell that. A P-value only tells you that there is an effect from training, but not how strong this effect is.\n\nFortunately, {ggwithinstats} provides Hedges‚Äô g as the measure of the Effect Size, which shows how large the effect of training is. Hedges‚Äô g is interpreted in the same way as Cohen‚Äôs d Effect size, but outperforms Cohen‚Äôs d for small samples, like 9 in our example. Our effect size of 0.83 is large, and tells us that the increase in performance of only 2 sit-ups is actually - a lot. But it doesn‚Äôt seem like a lot. So, we have to double check it!\n\nFor that {ggwithinstats} provides the Bayesian Difference between our samples with 95% Highest Density Intervals, which is conceptually similar to the difference of 2 sit-ups we see on the plot, and the Bayes Factor, which is conceptually similar to the p-value.\nOur Bayes Factor (Jeffreys, 1961) of -1.13 indicates a substantial evidence for the alternative hypothesis - that training actually did make a difference, and we now make substantially more sit-ups then Before ‚Ä¶ which IS in line with the frequentists statistics on the top of the plot.\n\nThe {interpret_hedges_g} function from {effectsize} package helps to interpret this effect size and even provides the reference for interpretation if you ask R about it: ?interpret_hedges_g.\n\n\n# install.packages(\"effectsize\")\nlibrary(effectsize)\n\ninterpret_hedges_g(0.83)\n\n\n[1] \"large\"\n(Rules: cohen1988)\n\n?interpret_hedges_g\n\n\n\nWhat happens if we choose a wrong test?\n\n\nggwithinstats(\n  data = Fitness,\n  x    = test, \n  y    = number, \n  type = \"nonparametric\"\n)\n\n\n\nggsave(filename = \"paired_t_test.jpg\", plot = last_plot(), width = 6, height = 4)\n\n\n\nThere are two ways things can go wrong. First, if we are lazy to check the normality, we‚Äôll go straight to the non-parametric Paired Wilcoxon-Test. Here, a p-value almost failed to reject the Null Hypothesis when we use the threshold of 0.05 to determine significance. And the effect size is lower, which means our study would be underpowered. Let me show you what I mean.\n\n\n# install.packages(\"pwr\")\nlibrary(pwr)\n\npwr.t.test(n = 9, d = 0.83, sig.level = 0.05, type = \"paired\")\n\n\n\n     Paired t test power calculation \n\n              n = 9\n              d = 0.83\n      sig.level = 0.05\n          power = 0.5897082\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\npwr.t.test(n = 9, d = 0.81, sig.level = 0.05, type = \"paired\")\n\n\n\n     Paired t test power calculation \n\n              n = 9\n              d = 0.81\n      sig.level = 0.05\n          power = 0.5693516\n    alternative = two.sided\n\nNOTE: n is number of *pairs*\n\nThe effect size from Wilcoxon test has 2% lower power, which increases the chances to miss an important discovery. And in our example with a p-value close to the significance threshold we almost missed it.\nBut that‚Äôs not all!!! We could be even more wrong if we took a NOT-PAIRED t-Test, because we would get completely opposite result, namely, that the effect of training is small and not significant. This wrong result could even seem plausible, since the difference of only 2 sit-ups is sooo small.\n\n\nggbetweenstats(\n  data = Fitness,\n  x    = test, \n  y    = number, \n  type = \"parametric\", \n  var.equal = T\n)\n\n\n\ninterpret_hedges_g(0.3)\n\n\n[1] \"small\"\n(Rules: cohen1988)\n\nggsave(filename = \"usual_t_test.jpg\", plot = last_plot(), width = 6, height = 4)\n\n\n\nSo, how can such a small difference be significant in a paired test then??? Well, it lies in the nature of the paired test - which does not compare averages of two samples, but compares a mean difference between those samples to zero. And if we look at the difference, which shows individual performance of students, we‚Äôll see that 7 out of 9 improved their performance, some of them by a lot, so that, the group in general was successful, and our significant p-value actually makes sense.\nProof of the concept about the difference between two samples\n\n\ngghistostats(data = d, x = difference, test.value = 0, binwidth = 1)\n\n\n\nggsave(filename = \"one_sample_t_test.jpg\", plot = last_plot(), width = 6, height = 4)\n\n\n\nInterestingly, if we test our mean difference against zero using one-sample t-Test and compare the t-statistics, p-value and the effect size to our Paired t-Test, we‚Äôll see identical result!\nThus, our fancy Paired Samples test is actually One-Sample test on the difference. And if you wanna know more about one-sample tests and how to interpret all these numbers, check out this article.\n\n\n# old way to do the tests\n# install.packages(\"broom\")\nlibrary(broom)\n\nbind_rows(\n  t.test(d$difference) %>% tidy(),\n  t.test(d$After, d$Before, paired = T) %>% tidy()\n) \n\n\n# A tibble: 2 √ó 8\n  estimate statistic p.value parameter conf.low conf.high method      \n     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl> <chr>       \n1        2      2.75  0.0249         8    0.325      3.68 One Sample ‚Ä¶\n2        2      2.75  0.0249         8    0.325      3.68 Paired t-te‚Ä¶\n# ‚Ä¶ with 1 more variable: alternative <chr>\n\nOne-Tailed (or One-Sided) Parametric Two-Samples Paired t-Tests\nWe can also ask the question, whether performance-change is positive or negative by performing One-Tailed (or One-Sided) Parametric Two-Samples Paired t-Tests. (I hate that name! :)\n\n\nt.test(d$After, d$Before, paired = TRUE, alternative = \"less\")\n\n\n\n    Paired t-test\n\ndata:  d$After and d$Before\nt = 2.753, df = 8, p-value = 0.9875\nalternative hypothesis: true difference in means is less than 0\n95 percent confidence interval:\n    -Inf 3.35093\nsample estimates:\nmean of the differences \n                      2 \n\nt.test(d$After, d$Before, paired = TRUE, alternative = \"greater\")\n\n\n\n    Paired t-test\n\ndata:  d$After and d$Before\nt = 2.753, df = 8, p-value = 0.01247\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.6490697       Inf\nsample estimates:\nmean of the differences \n                      2 \n\nLow p-value (p = 0.012) of the greater-sided test confirms that the performance increases after one week of training. The p-value of the less-sided test screams that your performance will not decrease with the probability of 99% (p = 0.99). In fact only one student (N = 6), which ironically was the best Before training week, decreased his/her performance by 2 sit-ups and was overtaken by two students After the week. That‚Äôs the price of arrogance :)\nWhat‚Äôs next, or Don‚Äôt use Paired t-Test if\nsamples are independent. In this case applyStudent‚Äôs or Welsh t-test\nsamples are small (n<30) and not-normally distributed. In this case use paired two-samples Wilcoxon-test.\nIf you need to compare more then two samples, first check whether they are normally distributed, and if they are, go to ANOVA, but if they aren‚Äôt, go to Kruskal Wallis\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-01-22-pairedsamplesttestinr/thumbnail.png",
    "last_modified": "2022-03-11T22:54:48+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-02-20-mcnemar/",
    "title": "R demo | McNemar Test | How to Conduct, Visualise and Interpret",
    "description": "If you need to compare two PAIRED categorical samples, McNemar test is a correct choise for you. Though, people often use Chi-Square test instead. Thus, in this blog-post we'll first conduct, visualize and interpret McNemac test you see on the picture to your right using only one simple command and then see what happens if we use Chi-Square test for paired data.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-03-05",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nGet the data\nCompute McNemar Test\nInterpret the result\nWhat happens if we choose a wrong test\nA bit of theory about McNemar\nThe best McNemar test is ‚Äúmid-P‚Äù ;)\nWhat‚Äôs the next best thing to learn?\nReference\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 5 minutes long. The ‚ÄúA bit of theory about McNemar‚Äù chapter below is also very useful and did not become part of the video for trying-not-to-confuse reasons.\n\n\n\n\n\n\n\nGet the data\n\n\nset.seed(9) # for reproducibility \ndata <- data.frame(\nbefore = sample(c(\"+\", \"-\", \"+\", \"+\"), 20, replace = TRUE),\nafter  = sample(c(\"-\", \"+\", \"-\", \"-\"), 20, replace = TRUE))\n\nView(data)\n\n\n\nCompute McNemar Test\nThis one simple command is the {ggbarstats} function from {ggstatsplot} package, which needs only 5 arguments:\nfirst, our data, which shows 20 people who were tested for having alien parasite in their blood before and after taking a treatment drug. So,\nx argument will be our results before the treatment and\ny will be the results after the treatment. Then\nwe have to tell {ggbarstats} that our samples are TRUE-ly paired and\nlastly, I always use the label argument to displays both - numbers and percentages of observations in order to see what changed\nSuch simple command results in this statistically rich and publication ready plot! Now, let‚Äôs interpret the results.\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nggbarstats(\n  data = data,\n  x    = before, \n  y    = after,\n  paired = TRUE, \n  label = \"both\"\n)\n\n\n\n\nIf you want to save the picture, use the code below.\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(tidyverse)\n\nggsave(filename = \"mcnemar2.jpg\", plot = last_plot(), width = 5, height = 3)\n\n\n\nInterpret the result\nlet‚Äôs first look at the numbers: the 4 which were and remained negative and the 7 which were and remained positive, do not provide any useful information, because nothing has changed there. But since 8 people, which had an alien parasite before and became negative after taking a drug, while only 1 new person got infected, we can conclude that our treatment ‚Ä¶ kind of ‚Ä¶ worked. ‚ÄúKind-off‚Äù because only statistical test can tell for sure. That‚Äôs why {ggbarstats} displays test results on the top of the plot. For instance‚Ä¶\nMcNemars‚Äôs Chi-Squared-Statistics was previously used to get p-values. But, since modern statistical software always deliver p-values nobody cares about it anymore.\nthe p-value helps to test the Null Hypothesis, which assumes that the drug has no impact on disease, while the Alternative Hypothesis assumes that the drug has an effect on disease.\n\nand our P-value of p = 0.02 shows a ‚ùå a moderate evidence against the null hypothesis (H0), ‚úÖ in favor of the alternative hypothesis (HAlt), that our drug worked. However, a P-value only tells you that our drug has an effect, but not how strong this effect is.\nFortunately, {ggbarstats} provides Cohen‚Äôs g with 95% Confidence Intervals as the measure of the Effect Size for McNemar test, which shows how big the effect of the drug is. The {interpret_cohens_g} function from the {effectsize} package reveals that our effect size of 0.39 is actually large.\n\n\n# install.packages(\"effectsize\")\nlibrary(effectsize)\n\ninterpret_cohens_g(0.39)\n\n\n[1] \"large\"\n(Rules: cohen1988)\n\n?interpret_cohens_g\n\n\n\nSo, the final interpretation of our McNemar test is: the ability of our drug to kill the alien parasite is significant and large. Which is amazing, since we can stop aliens. However, if we took a wrong test the things could go really wrong. Let me show you what I mean.\nWhat happens if we choose a wrong test\nFirst, {ggbarstats} delivers even more than you can see on this plot. Namely, it automatically removes the continuity correction which was shown to be very conservative by several scientific papers. We can clearly see it by using {mcnemar.test} function with {correct = FALSE} argument and getting identical results. In contrast, if we don‚Äôt use this argument, {mcnemar.test} will apply the continuity correction and produce unnecessary large p-value, which almost failed to reject the Null Hypothesis, so that we almost missed a discovery of a drug, which saves humanity from alien parasites.\n\n\nmcnemar.test(data$before, data$after, correct = F)\n\n\n\n    McNemar's Chi-squared test\n\ndata:  data$before and data$after\nMcNemar's chi-squared = 5.4444, df = 1, p-value = 0.01963\n\nmcnemar.test(data$before, data$after)\n\n\n\n    McNemar's Chi-squared test with continuity correction\n\ndata:  data$before and data$after\nMcNemar's chi-squared = 4, df = 1, p-value = 0.0455\n\nSecondly, the things can go even more wrong, if we take Chi-Square test by accidentally forgetting the {paired = TRUE} argument, or, God forbid, would take Chi-Squared test for paired data on purpose. Because, Chi-Square test has a completely different Null Hypothesis and may produce opposite results. And that‚Äôs exactly what happens in our example, a huge p-value and a small effect size tell us that our drug is useless, which is dramatically wrong (Type II Error). So, using Chi-Squared test for paired data is like using a wrong drug against the alien parasite! And now, I hope you can not unknow it :)\n\n\nggbarstats(\n  data = data,\n  x    = before, \n  y    = after,\n  label = \"both\"\n)\n\n\n\nggsave(filename = \"chisquared.jpg\", plot = last_plot(), width = 5, height = 5)\n\n\n\nBut IF your samples are not-paired, and you actually want to conduct either frequentists or bayesian Chi-Squared test and want to know how to interpret all these results, check out this video.\nA bit of theory about McNemar\nMcNemar‚Äôs is a non-parametric (distribution-free) test, which checks whether there is some change in proportions on a binomial outcome at two time points on the same population. It is applied only to 2√ó2 contingency table. In medicine, if a researcher wants to determine whether or not a particular drug has an effect on a disease, then a count of the individuals is recorded (as + and ‚Äì sign, or 0 and 1) in a table before and after being given the drug. Then, McNemar‚Äôs test is applied to make statistical decisions as to whether or not a drug has an effect on the disease.\nIn a 2√ó2 table there would be two cells with concordant results (the frequency of individuals which responded positively or negatively to both stimuli or on both time points) and two cells with discordant results (the frequency of individuals who responded differently).\nThe key question McNemar‚Äôs test asks if whether the difference between the values of the two discordant cells is significantly high based on the Null Hypothesis that both types of discord are equally likely. And interestingly enough, the paired McNemar test is exactly the same as One-Sample Chi-Squared Test goodness-of-fit test on two (discordant) numbers. Let me prove it to you:\n\n\nchisq.test(c(1,8))\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(1, 8)\nX-squared = 5.4444, df = 1, p-value = 0.01963\n\nSee, the Test-Statistics and the p-value are completely identical! That‚Äôs fascinating, because it‚Äôs similar to the paired t- or wilcoxon tests, which are also one-sample tests in their nature. The fact is very important, but it would probably confuse the viewers, since the pace of the video is quick and would mix-up two ideas: 1) the McNemar test = One-Sample Chi-Square test and 2) the Two-Samples Chi-Squared test is wrong. That‚Äôs why it did not became a part of the video.\nFinally, there are four versions of the McNemar test (classical, continuity corrected, exact, and mid-P). The traditional advice is to use the classical test in large samples and the exact test in small samples. The argument for using the exact test is that the classical test may violate the nominal significance level for small sample sizes because the required asymptotic do not hold. One disadvantage with the exact and continuity corrected tests is conservatism: they produce unnecessary large p-values and have poor power.\n\n\nMESS::power_mcnemar_test(n=30, paid=.05, psi=2, method = \"exact\")\n\n\n\n     McNemar paired comparison of proportions exact unconditional power calculation \n\n              n = 30\n           paid = 0.05\n            psi = 2\n      sig.level = 0.05\n          power = 0.05462106\n    alternative = two.sided\n\nNOTE: n is number of pairs\n\nMESS::power_mcnemar_test(n=30, paid=.05, psi=2, method = \"normal\")\n\n\n\n     McNemar paired comparison of proportions approximate power calculation \n\n              n = 30\n           paid = 0.05\n            psi = 2\n      sig.level = 0.05\n          power = 0.1032173\n    alternative = two.sided\n\nNOTE: n is number of pairs\n\nThe best McNemar test is ‚Äúmid-P‚Äù ;)\nThe mid-P version is not directly available in any R-package but can be obtained relatively simply using the recipe below, where b and c are the values in the two discordant cells of the contingency table, and where b < c:\n\\[2* pbinom(b, (b+c), lower.tail = T) - dbinom(b, (b+c), 0.5)\\]\n\n\n# cross_table\n2* pbinom(1, 8, 0.5) - dbinom(1, 8, 0.5)\n\n\n[1] 0.0390625\n\n2 * pbinom(3, 18, 0.5) - dbinom(3, 18, 0.5)\nWe can get a very similar p-value when we simulate it in a One-Sample Chi-Square test, which would be more realistic (see below) as compared to a not-simulated one, but most of the scientists who want to compare two paired categorical samples would not bother about it (no time). So, if scientist use the classical McNemar test for paired data instead of Two-Samples Chi-Square, I would already be fulfilled. Because the classical (asymptotic) McNemar test (without CC!) performs surprisingly well, even for quite small sample sizes. It often violates the nominal significance level, but not by much. Thus, my final recommendation here is:\nuse ggbarstat with argument {paired = TRUE} or {mcnemar.test(data\\(before, data\\)after, correct = F)} with no bad feeling, but if you are a perfectionist and have time,\ncalculate mid-P or simulate p-value in a one-sample Chi-Square test on two disconcordant numbers from cross table\n\n\nset.seed(2); chisq.test(c(1,8), simulate.p.value = T)\n\n\n\n    Chi-squared test for given probabilities with simulated\n    p-value (based on 2000 replicates)\n\ndata:  c(1, 8)\nX-squared = 5.4444, df = NA, p-value = 0.03948\n\nWhat‚Äôs the next best thing to learn?\nCochran‚Äôs Q Test + Pairwise McNemar Tests (post-hoc)\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nReference\nPembury Smith, M.Q.R., Ruxton, G.D. Effective use of the McNemar test. Behav Ecol Sociobiol 74, 133 (2020). https://doi.org/10.1007/s00265-020-02916-y\nFagerland, Morten & Lydersen, Stian & Laake, Petter. (2013). The McNemar test for binary matched-pairs data: Mid-p and asymptotic are better than exact conditional. BMC medical research methodology. 13. 91. 10.1186/1471-2288-13-91.\n\n\n\n",
    "preview": "posts/2022-02-20-mcnemar/thumbnail.png",
    "last_modified": "2022-03-05T10:42:58+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-02-08-friedman/",
    "title": "R demo | Friedman Test | How to Conduct, Visualise and Interpret",
    "description": "The Friedman Test is a non-parametric brother of Repeated Measures ANOVA, which does much better job when data is not-normally distributed (which happens pretty often ;). Friedman test is also superior to Repeated Measures ANOVA when our data is ordinal (e.g., scales from 1 to 10). Friedman Test can also be a non-parametric father of the Paired Wilcoxon test, because it can compare more then two groups.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-02-20",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nWhy do we need Friedman test\nHow Friedman Test works\nGet the data\nCompute Friedman Test\nInterpret the result\nCustomise the result\nHow to report the results\nHow to check the normality assumption to make sure you need Friedman test\nWhat‚Äôs next\n\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 5 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nRepeated Measures ANOVA and Paired Wilcoxon Signed-Rank test.\nWhy do we need Friedman test\nThe Friedman Test is a non-parametric brother of Repeated Measures ANOVA, which does much better job when data is not-normally distributed (which happens pretty often ;). Friedman test is also superior to Repeated Measures ANOVA when our data is ordinal (e.g., scales from 1 to 10). Friedman Test can also be a non-parametric father of the Paired Wilcoxon test, because it can compare more then two groups. Having a lot of groups creates tons of work though, because all the groups need to be compared to each other pairwisely if Friedman test is significant.\nHowever, one simple command conducts and visualizes Friedman Test you see on the screen, automatically conducts and displays pairwise tests and even corrects the p-values for multiple comparisons. So, let‚Äôs get the data and learn how to easily get all these results.\nHow Friedman Test works\nA good description about how Friedman test works and the picture below come from this blog-article\n\nGet the data\n\n\n# install.packages(\"tidyverse\")  # for everything ;)\nlibrary(tidyverse)\n\n# install.packages(\"datarium\")   # for marketing data\nlibrary(datarium)\n\nView(marketing)\n\nd <- marketing %>%\n  select(youtube, facebook, newspaper) %>% \n  rowid_to_column() %>% \n  gather(key = \"channel\", value = \"money\", youtube:newspaper) %>% \n  group_by(channel) %>% \n  slice(20:35) # looks better \n\nView(d)\n\n\n\nFor that:\nwe‚Äôll load {datarium} package and\ntake {marketing} data from it, which shows money spend on three advertising channels: youtube, facebook and newspaper\nwe then gather all three channels into one column, so that our channels become a variable for the x-axis of the plot, and our spending in thousands of dollars becomes a variable for the y-axis of the plot,\nfinally, we‚Äôll reduce the dataset a bit to make the plot look better\nFor repeated measures, the data needs to be sorted so that, the first observation of the first channel, pairs with the first observation of other channels. If our data is sorter, we are ready to compute the test.\nCompute Friedman Test\nAnd the best way to compute Friedman Test (in my opinion) is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:\nfirst, our data, which we just prepared, then\nx - as the grouping variable - channel,\ny - is the numeric variable money and\nthe nonparametric type of statistical approach, which tells {ggwithinstats} to conduct Friedman Test\nSuch simple command results in this statistically rich and publication ready plot! Now, let‚Äôs interpret the results.\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nggwithinstats(\n  data = d,\n  x    = channel, \n  y    = money, \n  type = \"nonparametric\"\n)\n\n\n\nggsave(filename = \"friedman.jpg\", plot = last_plot(), width = 5.5, height = 5.5)\n\n\n\nInterpret the result\nFriedman‚Äôs Chi-Squared-Statistics was previously used to get p-values. But, since modern statistical software always report p-values, we can safely ignore it.\nthe p-value helps to test the Null Hypothesis, which says that channels get similar amount of money, while the Alternative Hypothesis says that channels get different amount of money.\n\nour very low P-value (p = 0.0000376) shows a ‚ùå very strong evidence against the null hypothesis (H0), ‚úÖ in favor of the alternative hypothesis (HAlt), that difference exists. However, a P-value only tells you that there is a difference, but not how large this difference is.\nFortunately, {ggwithinstats} provides Kendall‚Äôs coefficient of concordance with 95% Confidence Intervals as the measure of the Effect Size for Friedman test. The {interpret_kendalls_w} function from {effectsize} package helps to interpret this effect size and even provides the reference for interpretation. Our effect size of 0.64 is substantial. (Please, ignore the word ‚Äúagreement,‚Äù it‚Äôs not important there)\n\n\n# install.packages(\"effectsize\")\nlibrary(effectsize)\n\ninterpret_kendalls_w(0.64)\n\n\n[1] \"substantial agreement\"\n(Rules: landis1977)\n\nSo, the final interpretation of our Friedman test is: the amount of money spend on channels differs significantly and this difference is substantial.\nNice, right? However, our Global Friedman Test doesn‚Äôt say which channels exactly do differ???. And that‚Äôs where pairwise comparisons come into play. But wait, which tests should we take?\nWell, fortunately, {ggwithinstats}:\nknows that we need Durbin-Conover pairwise tests after significant Friedman test,\nmoreover, it automatically conducts those tests and displays p-values\nand finally, it even corrects p-values for multiple comparisons without any additional code. I think it‚Äôs just amazing!\n\nHere is a quick proof that {ggwithinstats} indeed uses paired Durbin-Conover tests.\n\n\n# install.packages(\"PMCMRplus\")\nlibrary(PMCMRplus)\ndurbinAllPairsTest(\n  y      = d$money, \n  groups = d$channel, \n  blocks = d$rowid,\n  p.adjust.method = \"holm\") \n\n\n          facebook newspaper\nnewspaper 0.03     -        \nyoutube   2.0e-07  7.6e-05  \n\nby the way, our global Friedman test is often called with a strange name - omnibus test, while the pairwise tests between channels, are sometimes described in a dead latin language as - post-hoc - which simply means - after the event. I really think those unnecessary names make statistics more complicated then it is.\nCustomise the result\nHowever, if we want to, we can easily customize our plot by using either additional arguments within the function, or arguments from {ggplot2} package outside of it. For example,\nwe can easily change the p-values adjustment method from the default Holm-correction, to a more famous Bonferroni correction for multiple comparisons ‚Ä¶ but I wouldn‚Äôt recommend it, because Bonferroni correction is too conservative and we could miss a discovery. And that‚Äôs exactly what happens in our example, namely significant difference between money spend on facebook and newspaper, discovered by the Holm method, disappears when we use Bonferroni. So, no discovery - no Nobel Price for me üò≠.\nthen, if you want to display not only significant, but all comparisons, you can use {pairwise.display = ‚Äúall‚Äù} argument and see a p-value of 0.09 between facebook and newspaper, which was over-corrected by Bonferroni.\nif you want to hide the pairwise comparisons or statistical results, or both‚Ä¶\nyou can easily do that and much more. Just ask R about {?ggwithinstats} function and try some things out, you‚Äôll enjoy it.\nBut the coolest thing about {ggwithinstats} is that if you want to compare only two groups, you just need to change the variable channel, which has three groups in our data, to a variable which has only two groups, and {ggwithinstats} will automatically conduct Paired Wilcoxon test which you can learn all about from this video.\n\n\n# customise the result\nggwithinstats(\n  data = d,\n  x    = channel, \n  y    = money, \n  type = \"nonparametric\",\n  p.adjust.method = \"bonferroni\", \n  # pairwise.display = \"all\",\n  # pairwise.comparisons = FALSE,   \n  # results.subtitle = F\n) + \n  ylab(\"money spend [thousands of US dollars]\")+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\n?ggwithinstats\n\n\n\nHow to report the results\nReport both, the mean ranks and the medians for each group. Besides, report the test statistic \\(\\chi^2\\) value (‚ÄúChi-square‚Äù), degrees of freedom (‚Äúdf‚Äù) and the p-value which show whether the difference between the mean ranks is significant. How to write: There was a statistically significant difference in the amount of money spend on channels \\({\\chi^2}_{Friedman}\\)(df=2) = 20.38, p < 0.001. The effect size \\(W_{Kendall}\\) = 0.64 with 95% CI [0.39-1] turned out to be substantial. Post-hoc analysis with Durbin-Conover tests was conducted with a Bonferroni correction for multiple comparisons. Median (IQR) money spend were 20.16 (12.9 to 29.6), 27.8 (22.7 to 47.0) and 291 (116 to 291) for facebook, newspaper and youtube respectively. There was a suggestive differences between facebook and newspaper (p = 0.09). However, there was a statistically significant increase in money spending in youtube vs.¬†newpaper (p < 0.001) and in youtube vs.¬†facebook (p < 0.001).\nHow to check the normality assumption to make sure you need Friedman test\n\n\n# hard way\nhard <- afex::aov_ez(\n  data   = d,\n  id     = \"rowid\", \n  dv     = \"money\",  \n  within = \"channel\")\n\nresiduals(hard) %>% shapiro.test()\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  .\nW = 0.91729, p-value = 0.002389\n\nWhat‚Äôs next\nFriedman Test is actually not-very flexible, but very capricious and cranky. It does not like missing values or not exactly the same number of repeated measures for all individuals - imbalanced design, it often overfits and it is almost impossible to add more then one predictor. Thus, the solution for almost all of these problems and the most logical next step in your learning journey are - Mixed Effects Models.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-02-08-friedman/thumbnail.png",
    "last_modified": "2022-02-20T10:34:53+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/",
    "title": "R demo | Paired Samples Wilcoxon Signed Rank Test",
    "description": "Can a speed-reading exercise make you a faster reader? Well, Wilcoxon Signed Rank Test displayed here is a correct test to answer this question. So, in this video we'll learn how to choose a correct test and what happens if we use a wrong test, why Wilcoxon test is called Signed Rank and how to produce and interpret this statistically rich plot using only one simple command.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-01-26",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nCheck normality of the difference\nCompute Paired Samples Wilcoxon Signed Rank Test\nInterpret the result\nWhat would happen if we choose the wrong test\nOne-sided two-samples Wilcoxon-test\nProof of the concept about the difference between two samples\nDon‚Äôt use two-samples Wilcoxon-test if:\nConclusion\nWhat‚Äôs next\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 6 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nNot-Paired (independent) Two Samples Mann-Whitney test and Paired Two Samples t-Test would help.\n\n\n# install.packages(\"tidyverse\")  # for everything ;)\nlibrary(tidyverse)\n\n# install.packages(\"BSDA\")       # for Speed data\nlibrary(BSDA)\n\nView(Speed)\n\n\n\n{BSDA} package provides a {Speed} dataset, with four columns on Speed reading, namely, reading scores before and after the speed-reading course, difference between them, which is actually much more important then the scores themselves, and finally, the signranks column, which is the reason our test is called signed ranks.\nA speed reading course should make me a faster reader. But how can I measure the progress? Well, I expect the median difference in reading speed after the course to be higher then zero, which is my alternative hypothesis (HAlt), while my null hypothesis (H0) is that the difference will be equal to zero. And since the difference here is more important then paired samples themselves, we only need to check the normality of the difference, not of both samples. And checking normality is important for choosing a correct test, otherwise we could get completely wrong result. We‚Äôll see what happens if we choose a wrong test in a moment. Until then‚Ä¶\nCheck normality of the difference\n‚Ä¶ a small p-value of the Shapiro-Wilk normality test indicates that our difference is not normally distributed. That‚Äôs why we need a non-parametric Wilcoxon test, to compare two paired samples. If the difference would have been normally distributed, we would have taken a parametric paired t-test.\n\n\nshapiro.test(Speed$differ)\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  Speed$differ\nW = 0.75787, p-value = 0.001112\n\nSo, let‚Äôs bring our data in a tidy format by gathering columns, before and after, beneath each other. For paired tests, the data needs to be sorted, so that the first observation of the before group, pairs with the first observation of the after group. If our data is sorter, we are ready to compute the test.\n\n\n# make long format - tidy data\nd <- Speed %>% \n  gather(key = \"speed\", value = \"score\", before, after)\n\nView(d)\n\n\n\nCompute Paired Samples Wilcoxon Signed Rank Test\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nggwithinstats(\n  data = d,\n  x    = speed, \n  y    = score, \n  type = \"nonparametric\"\n)\n\n\n\nggsave(filename = \"wilcoxon.jpg\", plot = last_plot(), width = 6, height = 4)\n\n\n\nAnd the best way to compute our test is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:\nour data - d,\nx - as the grouping variable,\ny - are the reading scores and\nthe type of statistical approach. Since our data was not normally distributed, we choose a nonparametric test, and {ggwithinstats} automatically knows that we need a Paired Samples non-paramertic Wilcoxon Signed Rank Test ‚Ä¶ (Good Lord, the name is killing me :)\nSuch simple command results in this statistically rich and publication ready plot! Now, let‚Äôs interpret the results.\nInterpret the result\nV-statistics explains why our test is called signed rank. Namely,\nwe first ignore the negative sign and rank our absolute differences from smallest to largest, or from largest to smallest, does not matter. That is where the rank part of the name comes from\nwe then put positive and negative signs of the difference back to the ranks, producing positive and negative ranks. That is where the signed part of the name comes from and where signed rank name comes together.\nSumming up only positive ranks will give you the V - statistics you see on the plot. This Wilcoxon statistics were previously used to get a p-value, but nowadays, since p-values are calculated by computers, we can safely ignore it.\n\n\n\nSpeed %>% \n  filter(signranks > 0) %>% \n  summarise(sum(signranks)) %>% \n  as.data.frame()\n\n\n  sum(signranks)\n1          100.5\n\nour P-value of 0.023 shows a moderate evidence against the null hypothesis (H0), that median difference is equal to zero, in favor of the alternative hypothesis (HAlt), that median difference is not equal to zero (Raiola, 2012). Particularly, we‚Äôll read 7 score points faster after the course. But is a difference of 7 scores large? P-value can not tell that. A P-value only tells you that there is a difference, but not how strong this difference is.\n\nfortunately, {ggwithinstats} provides a Rank biserial correlation coefficient with 95% confidence intervals as the measure of the effect size, which shows how large the difference is. The {interpret_rank_biserial} function from {effectsize} package helps to interpret this effect size and even provides the reference for interpretation. Our effect size of 0.68 means, that speed reading exercise had a very large, positive and significant effect on our speed reading.\n\n\n# install.packages(\"effectsize\")\nlibrary(effectsize)\n\ninterpret_rank_biserial(0.68)\n\n\n[1] \"very large\"\n(Rules: funder2019)\n\n?interpret_rank_biserial\n\n\n\nWhat would happen if we choose the wrong test\nNow, what happens if I ignore the assumption of normality and conduct a Parametric Paired T-Test? Well, I would compare means instead of medians and would get completely opposite result, namely, - speed reading course doesn‚Äôt help me to read faster, which is just wrong. Here I would have made a Type II Error, or, in other words, I would have missed an important discovery. So, no Nobel Price for me.\n\n\nggwithinstats(\n  data = d,\n  x    = speed, \n  y    = score, \n  type = \"parametric\"\n)\n\n\n\n\nOne-sided two-samples Wilcoxon-test\nWithout visualization, the default two.sided alternative of Wilcoxon test only says that a difference is present, but does not say whether reading velocity decreases or increases. To find out exactly this, we need to test two new alternative hypotheses (Halt):\nThe median reading speed after the course decreases\nThe median reading speed after the course increases\nDoing this will add another useful tool to your statistical toolbox, namely one-tailed (or one-sided) non-parametric two-samples paired Wilcoxon signed rank test (the name is slowly killing me :)\n\n\nlibrary(broom)\nrbind(\n  wilcox.test(data = d, score ~ speed, paired = T, alternative = \"less\", conf.int = T, exact = F) %>% tidy(), \n  wilcox.test(data = d, score ~ speed, paired = T, alternative = \"greater\", conf.int = T, exact = F) %>% tidy()\n)\n\n\n# A tibble: 2 √ó 7\n  estimate statistic p.value conf.low conf.high method     alternative\n     <dbl>     <dbl>   <dbl>    <dbl>     <dbl> <chr>      <chr>      \n1     3.50      100.  0.990   -Inf         6.00 Wilcoxon ‚Ä¶ less       \n2     3.50      100.  0.0114     1.50    Inf    Wilcoxon ‚Ä¶ greater    \n\nLow p-value (p = 0.01) of the greater-sided test confirms that the reading velocity increases after speed-reading course. The p-value of the less-sided test screams that your reading velocity will not decrease with the probability of 99% (p = 0.99).\nProof of the concept about the difference between two samples\nInterestingly, since we just tested our median difference against zero, we actually conducted the one-sample Wilcoxon test on that difference. Let me prove it to you! If we compare the results of (1) one-sample Wilcoxon test on the difference with (2) the two-samples paired Wilcoxon test, we‚Äôll get identical V-statistics and p-values. How cool is that?\n\n\nbind_rows(\n  wilcox.test(Speed$differ) %>% tidy(),\n  wilcox.test(Speed$after, Speed$before, paired = T) %>% tidy()\n) \n\n\n# A tibble: 2 √ó 4\n  statistic p.value method                                 alternative\n      <dbl>   <dbl> <chr>                                  <chr>      \n1      100.  0.0229 Wilcoxon signed rank test with contin‚Ä¶ two.sided  \n2      100.  0.0229 Wilcoxon signed rank test with contin‚Ä¶ two.sided  \n\nThus, our fancy Paired Samples test is actually One-Sample test on the difference, where difference is checked against zero. We can of course check our one-sample against a different value, let‚Äôs say 6, which you can learn from this video.\nDon‚Äôt use two-samples Wilcoxon-test if:\nsamples are independent. In this case apply [Mann-Whitney-Wilcoxon-test] (https://yury-zablotski.netlify.com/post/mann-whitney-wilcoxon-test/)\nsamples are small (n<30) and normally distributed (or big and near normal). In this case use the more powerful two-samples t-test.\nConclusion\nTwo-samples Wilcoxon-test can be more powerful then two-samples t-test when difference between two samples at low numbers (<30) is not-normally distributed. For big samples and not to unnormal distribution, t-test will do fine. Another advantage of the median-based non-parametric Wilcoxon test is that it more robust to the outliers.\nWhat‚Äôs next\nCheck out the Mann-Whitney-Wilcoxon-test\nIf you need to compare more then two samples, first check whether they are normally distributed, and if they are, go to ANOVA, but if they aren‚Äôt, go to the non-parametric analogue of ANOVA\nKruskal-Wallis Rank Sum Test would be another idea.\nhttp://faculty.washington.edu/heagerty/Books/Biostatistics/TABLES/Wilcoxon/\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\n\n\n\n",
    "preview": "posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/thumbnail.png",
    "last_modified": "2022-01-26T17:11:46+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-12-29-correlationinr/",
    "title": "Correlation Analysis in R | Pearson, Spearman, Robust, Bayesian | How to conduct, visualise and interpret",
    "description": "Having two numeric variables, we often wanna know whether they are correlated and how. One simple command {ggscatterstats} can answer both questions by visualizing the data and conducting frequentists and bayesian correlation analysis at the same time. So, let's learn how to do that, how to interpret all those results and how to choose the right correlation method in the first place.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2022-01-03",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nThe most effective way to compute correlation in R\nInterpretation\nSpearman nonparametric correlation for not-linearly and not-normally distributed data\nRobust method if you have outliers\nChecking for normality and linearity\nWhat‚Äôs next?\nFurther readings and watchings\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 4 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nUnderstanding hypothesis testing and p-values would be very helpful.\nThe most effective way to compute correlation in R\n{ggscatterstats} function from {ggstatsplot} package is probably the best way to conduct correlation analysis. Within this function we need to specify only 4 arguments:\nour data, e.g.¬†let‚Äôs take mtcars, which you already have in R, so you don‚Äôt need to look for it\nx - as one of your numeric variables, for example horsepower of cars\ny - would be your second numeric variable, let‚Äôs take the Acceleration to a quarter mile and\nthe type of statistical approach. We choose parametric for the classic Pearson‚Äôs correlation, and we‚Äôll get back to other approaches later.\n{?ggscatterstats} function has of coarse many more useful arguments, but using only 4 of them already results in this statistically rich and publication ready plot! Now, let‚Äôs interpret the results.\n\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\nggscatterstats(\n    data = mtcars, \n    x    = hp,\n    y    = qsec,\n    type = \"parametric\"\n    )\n\n\n\n?ggscatterstats\n\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\n# save your plot\nggsave(\"corr_p.jpg\", plot = last_plot(), width = 5.5)\n\n\n\nInterpretation\nt statistics was previously used to manually calculate p-value, but nowadays, since p-values are calculated by computers, we can safely ignore it\nour very low P-value shows a very strong evidence against the null hypothesis, that there is no correlation between our variables, in favor of the alternative hypothesis, that horsepower of cars and their acceleration are correlated (Raiola, 2012)\n\nThe Bayes Factor (Jeffreys, 1961) of - 8.25 in our example agrees with a p-value by showing a decisive evidence for the alternative hypothesis - that correlation exists\n\nHowever, neither p-value nor Bayes Factor can say how strong this correlation is and in which direction, positive or negative, it goes.\nPearson‚Äôs correlation coefficient, however, can say both.\nFirst: the minus in front of our correlation coefficient points to a negative correlation, namely, the increase in horsepower of cars decreases their quarter mile time. In other words, the stronger the car, the faster it accelerates.\nSecondly: Correlation coefficient r itself is the effect size and ranges from -1, for the perfect negative correlation, to 1 for the perfect positive correlation. The further r is from zero and the closer r is to 1 or -1, the stronger the correlation between two variables is:1\n\nOur coefficient of -0.71 shows high negative correlation. But since no statistical method is perfect we should use several methods for the same question in order to be more certain. And that‚Äôs exactly what {ggscatterstats} does! (interpretation of the correlation coefficient in the picture below after Hinkle 2003).\n\nit provides a second measure of the effect size, namely Bayesian Pearson‚Äôs correlation coefficient with 95% Highest Density Intervals. Interestingly, Bayesian correlation is not high, like in the frequentist example, but moderately negative. So, if two methods don‚Äôt agree, which of them should we trust?\nSpearman nonparametric correlation for not-linearly and not-normally distributed data\nWell, I would take the Bayesian one, because it is less susceptible to not-linearly or not-normally distributed data. However, if the data is non-linear or not normally distributed, it is much better to use a non-parametric Spearman rank-based correlation analysis instead of Pearson.\nFortunately, the only thing you need to change in {ggscatterstats} function - is the type argument, from ‚Äúparametric‚Äù to ‚Äúnonparametric.‚Äù\n\n\nggscatterstats(\n    data = mtcars, \n    x    = hp,\n    y    = qsec,\n    type = \"nonparametric\"\n    )\n\n\n\n# save your plot\nggsave(\"corr_np.jpg\", plot = last_plot(), width = 5.5)\n\n\n\nRobust method if you have outliers\nMoreover, if we find some outliers in our data, we could apply a ‚Äúrobust‚Äù correlation to decrease the influence of outliers.\n\n\nggscatterstats(data = mtcars, x = hp, y = qsec, type = \"robust\")\n\n\n\n\nChecking for normality and linearity\nPlotting the relationships before making conclusions, will allow your to make sure they are linear.\nHere is some code to check whether our data is bell shaped, or normally distributed. This can be done in two ways.\nfirst, with Shapiro-Wilk normality test, where p-value > 0.05 would mean normally distributed data,\nor visually, with a Quantile-Quantile plot, using {ggqqplot} function from {ggpubr} package, where all points falling into the gray area would mean normally distributed data.\n\n\n# install.packages(\"ggpubr\")\nlibrary(ggpubr)\n\nshapiro.test(mtcars$hp) \n\n\n\n    Shapiro-Wilk normality test\n\ndata:  mtcars$hp\nW = 0.93342, p-value = 0.04881\n\nggqqplot(mtcars$hp) \n\n\n\nshapiro.test(mtcars$qsec) \n\n\n\n    Shapiro-Wilk normality test\n\ndata:  mtcars$qsec\nW = 0.97325, p-value = 0.5935\n\nggqqplot(mtcars$qsec) \n\n\n\n\nConclusion: Our data is linear and normally distributed, so we are fine with the classic Pearson‚Äôs correlation. However, if it is not the case, take a non-parametric Spearman correlation and if you find lots (>3) of outliers, use the Robust correlation method.\nWhat‚Äôs next?\nfirst of all, never forget to cite this amazing package!\n\n\ncitation(\"ggstatsplot\")\n\n\n\n  Patil, I. (2021). Visualizations with statistical details:\n  The 'ggstatsplot' approach. Journal of Open Source Software,\n  6(61), 3167, doi:10.21105/joss.03167\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    doi = {10.21105/joss.03167},\n    url = {https://doi.org/10.21105/joss.03167},\n    year = {2021},\n    publisher = {{The Open Journal}},\n    volume = {6},\n    number = {61},\n    pages = {3167},\n    author = {Indrajeet Patil},\n    title = {{Visualizations with statistical details: The {'ggstatsplot'} approach}},\n    journal = {{Journal of Open Source Software}},\n  }\n\nOk, after learning about correlation between two numeric variables, you might wonder how to check the association between two categorical variables? Well, this video will shown you a quick and effective way. It shows you how to conduct, visualize and interpret Chi-Square test & pairwise post-hoc tests via {ggbarstats} function from {ggstatsplot} package üì¶.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings and watchings\nFor more info about correlation and older, but maybe more familiar R code, check my older article on correlation: https://yury-zablotski.netlify.app/post/correlation/#how-to-compute-correlation\nHinkle DE, Wiersma W, Jurs SG. Applied Statistics for the Behavioral Sciences. 5th ed.¬†Boston: Houghton Mifflin; 2003.\nJeffreys, H. 1961. Theory of Probability. 3rd ed.¬†Oxford: Oxford University Press.\nRaiola, Gaetano & Di tore, Pio. (2012). Statistical study on bodily communication skills in volleyball to improve teaching methods. Journal of Human Sport and Exercise. 7. 10.4100/jhse.2012.72.12.\n\nhttps://en.wikipedia.org/wiki/Correlation_and_dependence‚Ü©Ô∏é\n",
    "preview": "posts/2021-12-29-correlationinr/thumbnail.jpg",
    "last_modified": "2022-01-03T11:29:43+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/",
    "title": "One-sample Student‚Äôs t-test and One-sample Wilcoxon test: or how to compare your work to the work of others.",
    "description": "Imagine you get 7 out of 10 to-dos from your list done on average. Are you then more productive then others? One-sample t-test and One-sample Wilcoxon test can answer this question. So, in this blog-post you'll learn how to conduct and visualize these tests with only one simple command, how to interpret all these results and how to choose the right test in the first place. Let's get straight into it.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2021-12-27",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nChoose the test\nOne-sample t-test\nInterpretation\nThe old way to do one-sample t-test in R\nOne-sided one-sample t-test (do them only if you really know what you are doing)\n\nOne-sample Wilcoxon test\nWhat‚Äôs next?\nFurther readings and watchings\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 5 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nUnderstanding hypothesis testing and p-values would be very helpful.\n\n\n\n# 1. simulate your to-do results\n\n# install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\nset.seed(1)  # stabilizes random output, so you always get the same result\nmy_to_dos <- round(rnorm(n = 21, mean = 7, sd = 3)) \n\nmy_to_dos\n\n\n [1]  5  8  4 12  8  5  8  9  9  6 12  8  5  0 10  7  7 10  9  9 10\n\nWhen you collect your to-do lists for the last 21 days, you find out that you finish 7 out of 10 tasks per day on average ‚Ä¶ ¬± 3. Now, since we have our data we can compare your average of 7 to the average of others, which is 6 our of 10, or 60%, according to this article. Is 7 significantly different from 6? One-sample t-test or One-sample Wilcoxon test can tell that, but how do we know which test we take?\nChoose the test\nWell, we‚Äôll simply check whether our data is bell shaped, or normally distributed. This can be done in two ways.\nfirst, with Shapiro-Wilk normality test, where p-value > 0.05 would mean normally distributed data,\nor visually, with a Quantile-Quantile plot, using {ggqqplot} function from {ggpubr} package, where all points falling into the gray area would mean normally distributed data.\nIf data is normally distributed, like in our example, the mean is a good representative for our sample, in this case we‚Äôll use one-sample t-test. But, if data is skewed, we choose one-sample Wilcoxon test which uses median.\n\n\n# 2. check for normality\n\nshapiro.test(my_to_dos) \n\n\n\n    Shapiro-Wilk normality test\n\ndata:  my_to_dos\nW = 0.93695, p-value = 0.1896\n\n# install.packages(\"ggpubr\")\nlibrary(ggpubr)\n\nggqqplot(my_to_dos)\n\n\n\n\nOne-sample t-test\nSince one-sample t-test checks the similarity between our sample‚Äôs mean and the expected mean:\nour null hypothesis (H0) suggests that the means are similar, while\nour alternative hypothesis (Halt) suggests that means differ\nThe presence of similarity or difference is not good or bad per se. If the mean of others is known to be correct, and your experiment shows no difference from that, be glad, your results are plausible. But if your results are different from the expected mean, you either did something wrong, or you might have discovered something new.\n{gghistostats} function from {ggstatsplot} package is probably the best way for our test. Within this function we need to specify only 5 arguments:\nour data, which we just created and called my_to_dos\nx - is the numeric variable in our data\ntest.value is the productivity of others we want to compare to\nthe type of statistical approach. We choose parametric for One-sample t-test, and we‚Äôll get back to other approaches later and finally‚Ä¶\nwe set the normal.curve argument to TRUE, only because it looks cool ;)\nThis simple command results in a statistically rich and publication ready plot! Now, let‚Äôs interpret the results.\n\n\n# 3. compare your mean to the other mean (6 tasks = 60%)\n\n# install.packages(\"ggstatsplot\")\nlibrary(ggstatsplot)\n\ngghistostats(\n    data       = my_to_dos %>% as_tibble,\n    x          = value,\n    test.value = 6, ## default value is 0\n    type       = \"p\", \n    normal.curve = T\n    )\n\n\n\n# save your plot\nggsave(\"ostt.jpg\", plot = last_plot(), width = 5.5)\n\n\n\nInterpretation\nt-value is the measure of similarity between compared means measured in units of standard error. The further t-value is from zero, the more different are the means:\n\\[t = \\frac{our.mean - expected.mean}{standart.error} = \\frac{our.mean - expected.mean}{ \\frac{standart.deviation} {\\sqrt sample.size} }\\]\nDF (not shown) stands for degrees of freedom. DF is always equal to the sample size - 1. For our 21 measurements, DF = 21 - 1 = 20.\nt-values and DFs were previously used to calculate p-values. But, modern statistical software always report p-values, so, we can safely ignore t. P-values can also be seeing as the measure of similarity, the smaller p-value, the less similar the means are. Our p-value of 0.013 indicates that our means are different. Actually, significantly different, if we follow the threshold of 0.05. Thus, we can conclude that we are significantly more productive than other people. So, we can feel proud about ourselves for a second‚Ä¶ and then get back to work ;).\nIndeed, if you imagine doing 1.7 tasks more every day for one year, at the end of this year you‚Äôll accomplish over 620 more tasks then the average person. But, more importantly, this will make YOU significantly better then yourself-one-year-ago in whatever you do.\nHedges‚Äô g is the estimated standardized difference between the means, and is the Effect Size. It goes from zero to one and is interpreted in the same way as Cohen‚Äôs d Effect size. In our example the effect is medium strong, which supports the conclusion of the p-value that difference exists.\n\nThere is one problem with Hedges‚Äô g though - it is a standardized difference between the means, not the normal difference. And that is where Bayesian Difference with it‚Äôs 95% Highest Density Intervals on the bottom of the plot seems more intuitive and is a second measure of the effect size here. It shows that the average person accomplishes 1.56 tasks less then you.\nIf that‚Äôs not enough, we can look at the Bayes Factor (Jeffreys, 1961), which tests both null and alternative hypotheses at the same time. Bayes Factor of -1.37 in our example indicates a substantial evidence for the alternative hypothesis - that our productivity is above average, which IS in line with the frequentists statistics on the top of the plot.\n\nThe old way to do one-sample t-test in R\n\n\nt.test(my_to_dos, mu = 6) \n\n\n\n    One Sample t-test\n\ndata:  my_to_dos\nt = 2.7116, df = 20, p-value = 0.01343\nalternative hypothesis: true mean is not equal to 6\n95 percent confidence interval:\n 6.384558 8.948776\nsample estimates:\nmean of x \n 7.666667 \n\nWe can also conduct the t-test in a classical way in R, which gives us some additional metrics, for instance, 95% confidence intervals (CI) of the mean. The previous results of 6 [tasks/day] is outside of our 95% CI [6.38, 8.95], thus our result is indeed different. It also gives us the Degrees of freedom, which were used to get p-values.\nOne-sided one-sample t-test (do them only if you really know what you are doing)\nR performs a two-sided test by default. Performing a one-sided test allows us to say whether our result is significantly lower or significantly greater. We could also change the confidence level, to i.e.¬†conf.level = 0.99 to be 99% sure of our conclusion:\n\n\nt.test(my_to_dos, mu = 6, alternative = \"less\",    conf.level = 0.99) \n\n\n\n    One Sample t-test\n\ndata:  my_to_dos\nt = 2.7116, df = 20, p-value = 0.9933\nalternative hypothesis: true mean is less than 6\n99 percent confidence interval:\n     -Inf 9.220453\nsample estimates:\nmean of x \n 7.666667 \n\nt.test(my_to_dos, mu = 6, alternative = \"greater\", conf.level = 0.99)\n\n\n\n    One Sample t-test\n\ndata:  my_to_dos\nt = 2.7116, df = 20, p-value = 0.006716\nalternative hypothesis: true mean is greater than 6\n99 percent confidence interval:\n 6.11288     Inf\nsample estimates:\nmean of x \n 7.666667 \n\nThe first one-sided t-test shows that your mean performance is definitely not lower (p>0.05 and expected average of 6 is included in 99% CI) then the average. The second one-sided t-test indicates that your performance is significantly greater then the average (p<0.05 and 6 is not included in 99% CI).\nOne-sample Wilcoxon test\nIf our data is not normally distributed, the median would describe our data much better than the average. And that‚Äôs what One-sample Wilcoxon test does. The good news about {gghistostats} function is that, the only thing you need to change is the type argument, from ‚Äúparametric‚Äù to ‚Äúnonparametric.‚Äù By the way, I encourage you to explore the function for yourself by simply asking R about it.\nIf our data would have been not-normally distributed, your productivity would be even better, which you can see from\nthe higher median to-do, namely 8, as compared with the mean of 7.7,\nlower p-value if we consider a p-value a measure of similarity (0.01 vs.¬†0.013) and\nstronger effect size (0.66 vs.¬†0.57).\n\n\n?gghistostats\n\ngghistostats(\n    data       = my_to_dos %>% as_tibble(),\n    x          = value,\n    test.value = 6, ## default value is 0\n    type       = \"np\", \n    normal.curve = T,\n    xlab = \"tasks done\"\n    )\n\n\n\nggsave(\"oswt.jpg\", plot = last_plot(), width = 5.5)\n\n\n\nHere is the interpretation of the R-biserial correlation coefficient, as the effect size for the non-parametric One-sample Wilcoxon test. It shows a very large effect size\n\nWhat‚Äôs next?\nfirst of all, never forget to cite this amazing package!\n\n\ncitation(\"ggstatsplot\")\n\n\n\n  Patil, I. (2021). Visualizations with statistical details:\n  The 'ggstatsplot' approach. Journal of Open Source Software,\n  6(61), 3167, doi:10.21105/joss.03167\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    doi = {10.21105/joss.03167},\n    url = {https://doi.org/10.21105/joss.03167},\n    year = {2021},\n    publisher = {{The Open Journal}},\n    volume = {6},\n    number = {61},\n    pages = {3167},\n    author = {Indrajeet Patil},\n    title = {{Visualizations with statistical details: The {'ggstatsplot'} approach}},\n    journal = {{Journal of Open Source Software}},\n  }\n\nNeed to do Chi-Square test? There is no better way than {ggbarstats} function from {ggstatsplot} package üì¶. Here is [R demo on how to conduct, visualize and interpret Chi-Square test & pairwise post-hoc tests] (https://youtu.be/8Tj0-yMPO64)\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings and watchings\nhttp://www.sthda.com/english/wiki/one-sample-t-test-in-r\nhttp://www.instantr.com/2012/12/29/performing-a-one-sample-t-test-in-r/\nFunder, D. C., & Ozer, D. J. (2019). Evaluating effect size in psychological research: sense and nonsense. Advances in Methods and Practices in Psychological Science.\nGignac, Gilles E, and Eva T Szodorai. 2016. ‚ÄúEffect Size Guidelines for Individual Differences Researchers.‚Äù Personality and Individual Differences 102: 74‚Äì78.\n\n\n\n",
    "preview": "posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/thumbnail.jpg",
    "last_modified": "2022-01-03T11:31:29+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-14-how-to-conduct-chi-square-test-in-r/",
    "title": "R demo | Chi-Square Test | how to conduct, visualize & interpret | + pairwise post-hoc tests",
    "description": "Chi-Square Test checks the independence between two categorical variables, where variables can have two or more categories. Need to do Chi-Square test? It can actually be done with only one line of code. There is no better way than {ggbarstats} function from {ggstatsplot} package üì¶. In this short blog-post you'll learn how to conduct, visualize and interpret Chi-Square test & pairwise post-hoc tests in R.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2021-12-20",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nHow to conduct Chi-Squared test in R\nInterpretation\nPairwise Proportion Tests ‚Ä¶ or post-hoc tests\nWhat‚Äôs next?\nFurther readings and watchings\n\nThis post as a video\nI recommend to watch a video first, because I highlight things I talk about. It‚Äôs ca. 5 minutes long.\n\n\n\n\n\n\n\nPrevious topics\nUnderstanding hypothesis testing and p-values would be very helpful.\nHow to conduct Chi-Squared test in R\nIf you have installed and loaded {ggstatsplot} package, you can use {ggbarstats} function to conduct and visualize Chi-Square test of independence between two categorical variables, where variables can have two or more categories. Within this function we need to specify only four arguments:\nour data, e.g.¬†let‚Äôs take mtcars, which you already have in R, so you don‚Äôt need to look for it\nx - as one of your categorical variables, for example Transmission of the car (with 0 being automatic, and 1 being manual transmission)\ny - would be your second variable, let‚Äôs take the Number of cylinders (4, 6 or 8) and\nthe label argument - which displays both numbers and percentages of observations in each category.\nThis simple command results in a statistically rich and publication ready plot! Now, let‚Äôs interpret the results.\n\n\ninstall.packages(\"ggstatsplot\")\n\n\n\n\n\nlibrary(ggstatsplot)\n\nggbarstats(\n  data  = mtcars, \n  x     = am, \n  y     = cyl, \n  label = \"both\", \n)\n\n\n\n\nInterpretation\nChi-Square statistics was previously used to manually calculate p-value, but nowadays, since p-values are always calculated by computers, we can safely ignore it\nP-value in our test can be seen as the probability of independence between two variables, low p-value (usually p < 0.05), like in our example, indicates that number of cylinders and transmission of cars are dependent on each other. In other words - there is a relationship between them.\nIndeed, the plot shows that the number of cars using automatic transmission (am = 0) increases with increasing number of cylinders. The opposite is true for cars with manual transmission (am = 1), their frequency declines as number of cylinders increases.\nSo, we can conclude, that the relationship between transmission and number of cylinders exists. However, p-value doesn‚Äôt say how strong this relationship is.\nThat‚Äôs why we have V Cramer value with its 95% confidence intervals as the effect size next to p-value. Our effect size of 0.46 indicates a relatively strong relationship, which supports the conclusion made by the p-value. The confidence intervals do not make much sense though, since V Cramer goes from 0 to 1 anyway.\n\nHowever, ggbarstats also provides a second Bayesian V Cramer effect size, which delivers much more useful 95% Highest Density Intervals. The interpretation of the Bayesian effect size is the same, so the relationship between our variables is relatively strong.\nIf that‚Äôs not enough, we can look at the Bayes Factor (Jeffreys, 1961), which tests both null and alternative hypotheses at the same time. Bayes Factor of - 2.82 in our example indicates a strong evidence for the alternative hypothesis - that the relationship exists, which IS in line with the frequentists statistics on the top of the plot.\n\nWe can also see Proportion Tests for transmissions in each cylinder. They show whether proportions inside every cylinder differ. Our Null Hypothesis (\\(H_0\\)) here is that there are equal proportions of different transmissions in a particular category of a cylinder. Which is the case for cylinders 4 and 6. While, our Alternative Hypothesis (\\(H_alt\\)) is that the proportions differ, which is the case for the cylinder 8.\nPairwise Proportion Tests ‚Ä¶ or post-hoc tests\nIf you find a significant relationship between variables and you have more then two categories in any of your variables, like in our example, you might be interested to compare proportions of cylinders with each other, namely 4 with 6, 4 with 8 and 6 with 8. Such simple pairwise comparisons is often called with an unnecessary fancy name - post-hoc tests.\nThe easiest was to make pairwise proportions tests is to use {pairwise_prop_test} function from {rstatix} package. Thus, first, install and load {rstatix} package, then use {table} function for a contingency table of your variables. And finally, simply apply {pairwise_prop_test} function to your contingency table. The results show two kinds of p-values, normal and adjusted for multiple comparisons. Always use the adjusted ones. So, we see that there is a significant association between cylinders 4 and 8, where cylinder 4 has more cars with manual transmission, while cylinder 8 has more cars with automatic transmission.\n\n\ninstall.packages(\"rstatix\")\n\n\n\n\n\nlibrary(rstatix)\n\n\n\n\n\ncontingency_table <- table(mtcars$cyl, mtcars$am)\ncontingency_table\npairwise_prop_test(contingency_table) \n\n\n\n\ngroup1group2pp.adjp.adj.signif460.44000.7300ns480.01080.0324*680.36500.7300ns\n\nWhat‚Äôs next?\nfirst of all, never forget to cite this amazing package!\n\n\ncitation(\"ggstatsplot\")\n\n\n\n  Patil, I. (2021). Visualizations with statistical details:\n  The 'ggstatsplot' approach. Journal of Open Source Software,\n  6(61), 3167, doi:10.21105/joss.03167\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    doi = {10.21105/joss.03167},\n    url = {https://doi.org/10.21105/joss.03167},\n    year = {2021},\n    publisher = {{The Open Journal}},\n    volume = {6},\n    number = {61},\n    pages = {3167},\n    author = {Indrajeet Patil},\n    title = {{Visualizations with statistical details: The {'ggstatsplot'} approach}},\n    journal = {{Journal of Open Source Software}},\n  }\n\nlearn more about Chi-Squared Test\nhave a look at the Chi-Squared Test Goodness of Fit\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings and watchings\nJeffreys, H. 1961. Theory of Probability. 3rd ed.¬†Oxford: Oxford University Press.\n\n\n\n",
    "preview": "posts/2021-12-14-how-to-conduct-chi-square-test-in-r/thumbnail.jpg",
    "last_modified": "2022-01-03T11:32:27+01:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data/",
    "title": "R package reviews {dlookr} diagnose, explore and transform your data",
    "description": "Raw data need to be diagnosed for existing problems, explored for new hypotheses and repaired in order to increase data quality and output. The {dlookr} package makes these steps fast and easy. {dlookr} generates automated reports and performs compex operations, like imputing missing values or outliers, with simple functions. Moreover, {dlookr} collaborates perfectly with {tidyverse} packages, like {dplyr} and {ggplot2} to name just a few!",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-30",
    "categories": [
      "EDA",
      "videos",
      "data wrangling",
      "R package reviews",
      "visualization"
    ],
    "contents": "\n\nContents\nThis post as a video\nChapter 1: DIAGNOSE your data\nGenerall diagnosis\nCategorical variables\nNumeric variables\nOutliers\nMissing Values\nReporting\n\nChapter 2: EXPLORE your data\nDescribtive statistics\nNormality Test\nCorrelation\nRelation\nCategorical to categorical\nCategorical to numeric\nNumeric to categorical\nNumeric to numeric\n\nReporting\n\nChapter 3: TRANSFORM your data\nImputation\n1. numeric\n2. categorical\n3. Imputation of outliers\n\nBinning\nStandardization and Resolving Skewness\n1. Standardization\n2. Resolving Skewness\n\nReporting\n\nConclusion\nUseful ressources\n\nThis post as a video\nIf you are more of a visual person, your can watch this video, which covers ca. 90% of this post.\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse) # for almost everything ;)\nlibrary(flextable) # for beautifying tables\nlibrary(dlookr)    # for the main event of the evening ;)\n\ncitation(\"dlookr\")\n\n\n\nTo cite package 'dlookr' in publications use:\n\n  Choonghyun Ryu (2021). dlookr: Tools for Data Diagnosis,\n  Exploration, Transformation. R package version 0.4.3.\n  https://CRAN.R-project.org/package=dlookr\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {dlookr: Tools for Data Diagnosis, Exploration, Transformation},\n    author = {Choonghyun Ryu},\n    year = {2021},\n    note = {R package version 0.4.3},\n    url = {https://CRAN.R-project.org/package=dlookr},\n  }\n\nChapter 1: DIAGNOSE your data\nGenerall diagnosis\nWe can think of diagnosing our data similarly to diagnosing a disease - we just try to figure out what is wrong. First of all, we can check whether a type of variables is correct. For instance, if we see a text or a factor for a clearly numeric variable - Temperature, we would be alarmed and would know that we need to fix it. Then, we can immediately see which variable has missing values and how many. Lastly, we can see how many unique values do we have in every variable, which is particularly useful for categorical variables, for instance, we could immediately see if some of values were misspelled and are now in several instead of only one category.\n\n\ndiagnose(airquality) %>% flextable()\n\n\nvariablestypesmissing_countmissing_percentunique_countunique_rateOzoneinteger3724.183007680.44444444Solar.Rinteger74.5751631180.77124183Windnumeric00.000000310.20261438Tempinteger00.000000400.26143791Monthinteger00.00000050.03267974Dayinteger00.000000310.20261438\n\nCategorical variables\nSpeaking of categorical variables: using diagnose_category() function we can diagnose all categorical variables from our dataset at once. Such diagnosis reveals names of categories, their counts and percentages and even ranking number from the biggest category to the smallest.\n\n\ndiagnose_category(diamonds) %>% flextable()\n\n\nvariableslevelsNfreqratiorankcutIdeal53,94021,55139.9536521cutPremium53,94013,79125.5672972cutVery Good53,94012,08222.3989623cutGood53,9404,9069.0952914cutFair53,9401,6102.9847985colorG53,94011,29220.9343721colorE53,9409,79718.1627732colorF53,9409,54217.6900263colorH53,9408,30415.3948834colorD53,9406,77512.5602525colorI53,9405,42210.0519106colorJ53,9402,8085.2057847claritySI153,94013,06524.2213571clarityVS253,94012,25822.7252502claritySI253,9409,19417.0448653clarityVS153,9408,17115.1483134clarityVVS253,9405,0669.3919175clarityVVS153,9403,6556.7760476clarityIF53,9401,7903.3185027clarityI153,9407411.3737498\n\nNumeric variables\nDiagnosing all numeric variables at once is similarly easy. diagnose_numeric() function calculates not only most common descriptive statistics, like minimum, first Quartile, average, median, third Quartile and maximum, but also gives you the number of zeros, negative values and even the number of potential outliers for every numeric variable.\n\n\ndiagnose_numeric(diamonds) %>% flextable()\n\n\nvariablesminQ1meanmedianQ3maxzerominusoutliercarat0.20.400.79793970.701.045.01001,889depth43.061.0061.749404961.8062.5079.00002,545table43.056.0057.457183957.0059.0095.0000605price326.0950.003,932.79972192,401.005,324.2518,823.00003,538x0.04.715.73115725.706.5410.748032y0.04.725.73452605.716.5458.907029z0.02.913.53873383.534.0431.8020049\n\nAs mentioned before, {dlookr} happily collaborates with two of the most used {tidyverse} packages: {dplyr} and {ggplot2}, which is simply amazing!\n\n\ndiagnose_numeric(diamonds) %>% \n  filter(minus > 0 | zero > 0) %>% \n  select(variables, median, zero:outlier) %>% \n  flextable()\n\n\nvariablesmedianzerominusoutlierx5.708032y5.717029z3.5320049\n\nOutliers\nBy the way outliers, how would we diagnose them? Of coarse we would count them and get their percentages in every variable. {dlookr} does it too. Moreover, it calculates three different averages: the mean of every variable with outliers, without outliers and the mean of the outliers themselves. In this way we can see how strong the influence of outliers for every variable is. For instance the variable ‚Äúdepth‚Äù in ‚Äúdiamonds‚Äù data has over 2500 outliers. That‚Äôs a lot! However, the means with and without outliers are almost identical. Besides, the average of the outliers themselves is very similar to the original average of the whole data. In contrast, the variable ‚Äúprice‚Äù with over 3500 outliers is heavily influenced by them. The average of the outliers is almost 5 times higher, than the average without them. That‚Äôs deep enough diagnoses of outliers already. And if that weren‚Äôt enough, {dlookr} can visualize the distribution of data with and without outliers. Let‚Äôs plot them!\n\n\ndiagnose_outlier(diamonds) %>% flextable()\n\n\nvariablesoutliers_cntoutliers_ratiooutliers_meanwith_meanwithout_meancarat1,8893.502039302.1536840.79793970.748738depth2,5454.7182054161.20479461.749404961.776373table6051.1216166164.84297557.457183957.373404price3,5386.5591397814,944.7764273,932.79972193,159.807111x320.059325187.2393755.73115725.730262y290.053763449.7734485.73452605.732353z490.090841684.0546943.53873383.538265\n\nFirst of all we‚Äôll quickly find the variables (or columns) with outliers using find_outliers() function. It gives you the index of the column. For example in ‚Äúiris‚Äù dataset the second column contains outliers. Using the argument index = FALSE provides the name of the column, so that you can select only columns with outliers and plot them by a simple plot_outlier() function.\nIf we don‚Äôt specify any columns, plot_outlier() function will plot each numeric variable from our dataset. This plot displays the distribution of data with and without outliers in the form of box plots and histograms, so that we directly see how our data changes if we remove outliers.\n\n\nfind_outliers(iris)\n\n\n[1] 2\n\nfind_outliers(iris, index = F)\n\n\n[1] \"Sepal.Width\"\n\niris %>% \n  select(Sepal.Width) %>% \nplot_outlier()\n\n\n\nairquality %>% \n  select(find_outliers(airquality)) %>% \nplot_outlier()\n\nplot_outlier(ISLR::Default)\n\n\n\n\nIf we remove outliers, they kind of become missing values, despite the fact that they weren‚Äôt missing in the beginning. And if we want to diagnose our data properly, we need to deal with missing values too. And as always, the best way to deal with any data problem is to visualize it.\nMissing Values\nWell amazingly, {dlookr} not only can visualize missing values, but can also do this in three different ways. The first one - is the Pareto chart.\n\n\nplot_na_pareto(airquality)\n\n\n\n\nIn a Pareto Chart counts and proportions of missing values are represented in descending order by bars, and the cumulative total is represented by the line. It even tells you what the amount of missing values means, namely, missing around 24% of observations is still OK, while missing more then 40% of values would be bad. If you have a lot of variables and wanna display only the ones with missing values, use only_na = TRUE argument. The only problem with pareto plot is that we don‚Äôt know whether missing values in different columns belong to the same observation.\n\n\nairquality %>% \n  plot_na_pareto(only_na = TRUE)\n\n\n\nairquality %>% \n  plot_na_pareto(only_na = TRUE, plot = FALSE) %>% flextable()\n\n\nvariablefrequenciesratiogradecumulativeOzone370.24183007OK84.09091Solar.R70.04575163Good100.00000\n\nBut that‚Äôs where the second type of visualization of missing values comes into play - plot_na_hclust() - which shows you how missing values are distributed and whether there is any overlapping of variables between then. The only thing which is troublesome here is that overlapping itself could be difficult to see if some variables have very few and some variable have a lot of missing values.\n\n\nplot_na_hclust(airquality)\n\n\n\n\nThird method of plotting missing values solves this problem. plot_na_intersect() function visualizes the combinations of missing values across columns. The x-axis shows the variables with missing values, while the counts of missing values are shown on the top of the plot as bars. The y-axis represents the combination of variables and their frequencies. For instance, we see, that two of the observations are missing in both variables, Ozone and Solar.\n\n\nplot_na_intersect(airquality)  \n\n\n\n\nReporting\nSo, let‚Äôs close the Diagnosis chapter with the diagnose_report() function, which combines most of what we just learned (but not all!) into one PDF or HTML document in seconds. Just run this line of code and explore the document. But the way - exploring - is the second thing {dlookr} package absolutely nails! So, let‚Äôs get straight to it.\n\n\ndiagnose_report(airquality) # pdf or html\n\n\n\nChapter 2: EXPLORE your data\nDescribtive statistics\n\n\n# all numeric variables\ndescribe(iris) %>% flextable()\n\n\nvariablennameansdse_meanIQRskewnesskurtosisp00p01p05p10p20p25p30p40p50p60p70p75p80p90p95p99p100Sepal.Length15005.8433330.82806610.067611321.30.3149110-0.5520644.34.4004.6004.85.05.15.275.605.806.106.36.46.526.907.2557.7007.9Sepal.Width15003.0573330.43586630.035588330.50.31896570.2282492.02.2002.3452.52.72.82.803.003.003.103.23.33.403.613.8004.1514.4Petal.Length15003.7580001.76529820.144136003.5-0.2748842-1.4021031.01.1491.3001.41.51.61.703.904.354.645.05.15.325.806.1006.7006.9Petal.Width15001.1993330.76223770.062236451.5-0.1029667-1.3406040.10.1000.2000.20.20.30.401.161.301.501.81.81.902.202.3002.5002.5\n\ndescribe() function provides descriptive statistics of ALL numeric variables in your dataset at once, if you don‚Äôt specify any. Among them are average, standard deviation, standard error, interquartile range, skewness and many quantiles. However, describe() function becomes even more useful by collaborating with {dplyr} package. Particularly, we can group the data by any categorical variable and get descriptive stats for each separate category.\n\n\n# per category\niris %>% \n  group_by(Species) %>% \n  select(Sepal.Length) %>% \n  describe() %>% flextable()\n\n\nvariableSpeciesnnameansdse_meanIQRskewnesskurtosisp00p01p05p10p20p25p30p40p50p60p70p75p80p90p95p99p100Sepal.Lengthsetosa5005.0060.35248970.049849570.4000.1200870-0.252688804.34.3494.4004.594.74.8004.84.965.05.105.105.25.325.415.6105.7515.8Sepal.Lengthversicolor5005.9360.51617110.072997620.7000.1053776-0.533009544.94.9495.0455.385.55.6005.65.705.96.046.206.36.406.706.7556.9517.0Sepal.Lengthvirginica5006.5880.63587960.089926950.6750.11801510.032904424.95.2435.7455.806.16.2256.36.406.56.706.836.97.207.617.7007.8027.9\n\nNormality Test\n\n\nnormality(iris) %>% flextable()\n\n\nvarsstatisticp_valuesampleSepal.Length0.97609030.0101811611756293150Sepal.Width0.98491790.1011542684359037150Petal.Length0.87626810.0000000007412263150Petal.Width0.90183490.0000000168046517150\n\nChecking normality of data is an routine task of data scientists before analysing data. normality() function performs a Shapiro-Wilk normality test on ALL numeric variables at once, if non are specified. (When the number of observations is greater than 5000, then 5000 observations are randomly selected.) The ability of this function to work with {dplyr} helps here too, since we often need to check normality of groups before comparing them. The code you see below consists of just 4 words, but conducts 12 normality tests, namely for 3 categories in 4 numeric variables. Now, imagine a dataset with 100 numeric variables and 100 categories you wanna check the normality for. It would still take only 4 words to conduct 10.000 tests!\n\n\niris %>% \n  group_by(Species) %>% \n  normality() %>% \n  flextable()\n\n\nvariableSpeciesstatisticp_valuesampleSepal.Lengthsetosa0.97769850.459513151806050Sepal.Lengthversicolor0.97783570.464737039277750Sepal.Lengthvirginica0.97117940.258314745431550Sepal.Widthsetosa0.97171950.271526393904450Sepal.Widthversicolor0.97413330.337995108261050Sepal.Widthvirginica0.96739050.180896040366850Petal.Lengthsetosa0.95497680.054811467146450Petal.Lengthversicolor0.96600440.158477838348050Petal.Lengthvirginica0.96218640.109775369487250Petal.Widthsetosa0.79976450.000000865857350Petal.Widthversicolor0.94762630.027277804140850Petal.Widthvirginica0.95977150.086954187946950\n\nFor comparison, here is the simplest example of the code I used for the same task before {dlookr}. Here I would need to:\ntransform the data first\n‚Äúnest‚Äù the values\nuse ‚Äúmap‚Äù function two times, which has nothing to do with ‚Äúnormality‚Äù\nuse strange symbols like ‚Äú~‚Äù (tilde) and ‚Äú.x$‚Äù\n‚Äútidy‚Äù up the result\n‚Äúunnest‚Äù the result and even\nget rid of some useless columns to get the same table we have seen above\nDon‚Äôt get me wrong, I am a huge fan of {dplyr}! But some tasks can be simplified even beyond {dplyr} and that‚Äôs just beautiful.\n\n\niris %>% \n  pivot_longer(cols = 1:4) %>% \n  nest(value) %>% \n  mutate(\n    test   = map(data, ~ shapiro.test(.x$value)),\n    tidied = map(test, broom::tidy)\n  ) %>% \n  unnest(tidied, .drop = TRUE) %>% \n  select(-data, -test) %>% \n  flextable() %>% \n  width(j = \"method\", width = 2)\n\n\nSpeciesnamestatisticp.valuemethodsetosaSepal.Length0.97769850.4595131518060Shapiro-Wilk normality testsetosaSepal.Width0.97171950.2715263939044Shapiro-Wilk normality testsetosaPetal.Length0.95497680.0548114671464Shapiro-Wilk normality testsetosaPetal.Width0.79976450.0000008658573Shapiro-Wilk normality testversicolorSepal.Length0.97783570.4647370392777Shapiro-Wilk normality testversicolorSepal.Width0.97413330.3379951082610Shapiro-Wilk normality testversicolorPetal.Length0.96600440.1584778383480Shapiro-Wilk normality testversicolorPetal.Width0.94762630.0272778041408Shapiro-Wilk normality testvirginicaSepal.Length0.97117940.2583147454315Shapiro-Wilk normality testvirginicaSepal.Width0.96739050.1808960403668Shapiro-Wilk normality testvirginicaPetal.Length0.96218640.1097753694872Shapiro-Wilk normality testvirginicaPetal.Width0.95977150.0869541879469Shapiro-Wilk normality test\n\nMoreover, plot_normality() function visualizes the normality of numeric data and two most common transformations of data, namely log transformation & square root, in case the normality assumption wasn‚Äôt met. Particularly, we see:\nHistogram of original data\nQuantile-Quantile plot of original data\nHistogram of log transformed data and finally\nHistogram of square root transformed data\nNow we can even see whether transformation improves something or not.\n\n\nairquality %>%\n  plot_normality(Ozone)\n\n\n\n\nCorrelation\nIn order to quickly check the relationship between numeric variables we can use correlate() function. If we don‚Äôt specify any target variables, Pearson‚Äôs correlation between ALL variables will be calculated pairwisely. But let‚Äôs have a look at only one variable first - Ozone. Nice, right? But plot_correlate() function is even more useful, because it visualizes these relationships. We can of course determine the method of calculations, be it a default ‚Äúpearson‚Äù, or a non-parametric ‚Äúkendall‚Äù or ‚Äúspearman‚Äù correlation. The shape of each subplot shows the strength of the correlation, while the color shows the direction, where blue is positive and red is negative correlation.\n\n\ncorrelate(airquality, Ozone)\n\n\n# A tibble: 5 x 3\n  var1  var2    coef_corr\n  <fct> <fct>       <dbl>\n1 Ozone Solar.R    0.348 \n2 Ozone Wind      -0.602 \n3 Ozone Temp       0.698 \n4 Ozone Month      0.165 \n5 Ozone Day       -0.0132\n\nplot_correlate(iris, method = \"kendall\")\n\n\n\n\nHere again, by using some {dplyr} code, we can quickly check as many correlations as we want.\n\n\ndiamonds %>%\n  filter(cut %in% c(\"Premium\", \"Ideal\")) %>% \n  group_by(cut) %>%\n  plot_correlate(method = \"spearman\")\n\n\n\n\nRelation\n{dlookr} can also check out other kinds of relationships between two particular, for example between two categorical, variables. You just need to specify the response (or target) variable with a function target_by() and the predictor with the function relate().\nCategorical to categorical\nFor instance, the influence of predictor ‚Äúclarity‚Äù on our target ‚Äúcut‚Äù in ‚Äúdiamonds‚Äù dataset, will be analyzed by a Chi-Square Test for independence. Plotting them will produce a mosaic plot with frequencies of both categorical variables.\n\n\ndiamonds %>% \n  target_by(cut) %>%      # similar to \"group_by\" only for one variable\n  relate(clarity) %>% \n  summary() \n\n\nCall: xtabs(formula = formula_str, data = data, addNA = TRUE)\nNumber of cases in table: 53940 \nNumber of factors: 2 \nTest for independence of all factors:\n    Chisq = 4391, df = 28, p-value = 0\n\ndiamonds %>% \n  target_by(cut) %>%      \n  relate(clarity) %>% \n  plot()\n\n\n\n\nCategorical to numeric\nIf our target variable is categorical and our predictor is numeric, simple descriptive stats per category of a target variable and for the numeric variable without categorization and density plots will be displayed.\n\n\n# iris %>% \n#   target_by(Species) %>%      \n#   relate(Sepal.Length) %>% \n#   flextable()\n\niris %>% \n  target_by(Species) %>%      \n  relate(Sepal.Length) %>% \n  plot()\n\n\n\n\nNumeric to categorical\nIf the response variable is numeric, while the predictor is categorical or numeric, simple linear regression will be applied. Using summary() or tab_model() would give you the model output, whith coefficients and p-values. Plotting the categorical predictor would produce a pretty boring box-plot, while visualization of the numeric variable would produce a nice model fit with confidence intervals.\n\n\niris %>% \n  target_by(Sepal.Length) %>%      \n  relate(Species) %>% \n  sjPlot::tab_model()\n\n\n\n¬†\n\n\nSepal Length\n\n\nPredictors\n\n\nEstimates\n\n\nCI\n\n\np\n\n\n(Intercept)\n\n\n5.01\n\n\n4.86¬†‚Äì¬†5.15\n\n\n<0.001\n\n\nSpecies [versicolor]\n\n\n0.93\n\n\n0.73¬†‚Äì¬†1.13\n\n\n<0.001\n\n\nSpecies [virginica]\n\n\n1.58\n\n\n1.38¬†‚Äì¬†1.79\n\n\n<0.001\n\n\nObservations\n\n\n150\n\n\nR2 / R2 adjusted\n\n\n0.619 / 0.614\n\n\niris %>% \n  target_by(Sepal.Length) %>%      \n  relate(Species) %>% \n  plot()\n\n\n\n  #sjPlot::plot_model(type = \"pred\")\n\n\n\nNumeric to numeric\n\n\nairquality %>% \n  target_by(Ozone) %>%      \n  relate(Temp) %>% \n  summary() \n\n\n\nCall:\nlm(formula = formula_str, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.729 -17.409  -0.587  11.306 118.271 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -146.9955    18.2872  -8.038 9.37e-13 ***\nTemp           2.4287     0.2331  10.418  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.71 on 114 degrees of freedom\n  (37 observations deleted due to missingness)\nMultiple R-squared:  0.4877,    Adjusted R-squared:  0.4832 \nF-statistic: 108.5 on 1 and 114 DF,  p-value: < 2.2e-16\n\nairquality %>% \n  target_by(Ozone) %>%      \n  relate(Temp) %>% \n  plot()\n\n\n\n\nReporting\nSimilarly to the diagnose report, eda_report() will produce a report containing most of the exploratory data analysis {dlookr} provides. Moreover, if we here specify the target variable, we‚Äôll get much richer report. Let‚Äôs have a look at the HTML output format this time. In this you‚Äôll see the descriptive stats, the normality checks, correlation analysis and more, so, feel free to explore it by yourself.\n\n\nairquality %>%\n  eda_report(\n    target        = Temp, \n    output_format = \"html\", \n    output_file   = \"EDA_airquality.html\")\n\n\n\nChapter 3: TRANSFORM your data\nIn the last part of this post we‚Äôll see how we can easily fix the problem with missing values and outliers. This part is my favorite! To be honest with you, missing values and outliers just suck! They are like parasites on my data and I just wanna get rid of them. Now, thanks to {dlookr}, I can do it easily and very effective! Check this out.\nImputation\nImputation simply means replacing a missing value with a value which makes sense. We can impute both numeric and categorical values.\n1. numeric\nimputation can be done with a single value, like\nmean\nmedian or\nmode, or\n\nby a machine learning algorithm, like\nknn - K-nearest neighbors\nrpart - Recursive Partitioning and Regression Trees or\nmice - Multivariate Imputation by Chained Equations (random seed must be set)\n\nimputate_na() function needs 4 arguments: - data - name of the variable with missing values - name of the predictor variable and - the method of imputation\nLet‚Äôs use the ‚Äúairquality‚Äù dataset again, and fill out 37 missing values in the variable ‚ÄúOzone‚Äù via the predictor variable ‚ÄúTemperature‚Äù using the average as a method of imputation.\nIf we save the output of the function imputate_na() in some meaningful name, for example ‚Äúbla‚Äù, we can first have a look at the summary. The summary compares some metrix of the Original and New datasets, before and after the imputation accordingly. First, we see that the standard deviation and the standard error got smaller, which is great! Secondly, let‚Äôs plot both datasets and compare them. The plot reveals that distribution of a new dataset changed dramatically, which is not great at all, because we just made up some data which deviates far away from the original data, and the original data is our best bet on reality. So, the distribution before and after imputation suppose to be similar.\n\n\n# mean\nbla <- imputate_na(airquality, xvar = Ozone, yvar = Temp, method = \"mean\")\nsummary(bla)\n\n\nImpute missing values with mean\n\n* Information of Imputation (before vs after)\n           Original Imputation\nn        116.000000 153.000000\nna        37.000000   0.000000\nmean      42.129310  42.129310\nsd        32.987885  28.693372\nse_mean    3.062848   2.319722\nIQR       45.250000  25.000000\nskewness   1.241796   1.421624\nkurtosis   1.290303   2.643199\np00        1.000000   1.000000\np01        4.300000   5.040000\np05        7.750000   9.000000\np10       11.000000  12.200000\np20       14.000000  18.000000\np25       18.000000  21.000000\np30       20.000000  23.000000\np40       23.000000  33.600000\np50       31.500000  42.129310\np60       39.000000  42.129310\np70       49.500000  42.129310\np75       63.250000  46.000000\np80       73.000000  60.200000\np90       87.000000  81.600000\np95      108.500000  97.000000\np99      133.050000 128.240000\np100     168.000000 168.000000\n\nplot(bla)\n\n\n\n\nHmm, but does imputation then makes sense at all? Well, let‚Äôs use one of the machine learning methods, for example K-nearest neighbors, and look at the result. This plot looks much better, because the distribution did not really change, but some aspects of it became a bit more emphasized.\n\n\n# ‚Äúknn‚Äù : K-nearest neighbors\nblap <- imputate_na(airquality, Ozone, Temp, method = \"knn\")\nplot(blap)\n\n\n\n\nBut does this mean, that imputation by a single value is useless? Well I‚Äôve heard this opinion before. But I found that usefulness of imputation strongly depends on the dataset and the best way to impute in my opinion is just to compare all imputation methods with each other and take the best, even if it is a single value method. If we run the whole chunk of code below, we‚Äôll be able to visually choose among methods. I think that ‚Äúrpart‚Äù - Recursive Partitioning and Regression Trees method, does the best job for the variable ‚ÄúOzone‚Äù in ‚Äúairquality‚Äù dataset.\nNeedless to say, the plots can be easily pimped with some ggplot2 syntax, as you can see in the code.\n\n\n# mean\nplot(imputate_na(airquality, xvar = Ozone, yvar = Solar.R, method = \"mean\"))\n\n\n\n# median\nplot(imputate_na(airquality, Ozone, Temp, method = \"median\"))\n\n\n\n# mode\nplot(imputate_na(airquality, Ozone, Temp, method = \"mode\"))\n\n\n\n# ‚Äúknn‚Äù : K-nearest neighbors\nplot(imputate_na(airquality, Ozone, Temp, method = \"knn\"))\n\n\n\n# ‚Äúrpart‚Äù : Recursive Partitioning and Regression Trees\nplot(imputate_na(airquality, Ozone, Temp, method = \"rpart\"))+\n  theme_classic()+\n  theme(legend.position = \"top\")\n\n\n\n# ‚Äúmice‚Äù : Multivariate Imputation by Chained Equations\nplot(imputate_na(airquality, Ozone, Temp, method = \"mice\", seed = 999))+\n  theme_minimal()+\n  theme(legend.position = \"top\")\n\n\n\n iter imp variable\n  1   1  Ozone  Solar.R\n  1   2  Ozone  Solar.R\n  1   3  Ozone  Solar.R\n  1   4  Ozone  Solar.R\n  1   5  Ozone  Solar.R\n  2   1  Ozone  Solar.R\n  2   2  Ozone  Solar.R\n  2   3  Ozone  Solar.R\n  2   4  Ozone  Solar.R\n  2   5  Ozone  Solar.R\n  3   1  Ozone  Solar.R\n  3   2  Ozone  Solar.R\n  3   3  Ozone  Solar.R\n  3   4  Ozone  Solar.R\n  3   5  Ozone  Solar.R\n  4   1  Ozone  Solar.R\n  4   2  Ozone  Solar.R\n  4   3  Ozone  Solar.R\n  4   4  Ozone  Solar.R\n  4   5  Ozone  Solar.R\n  5   1  Ozone  Solar.R\n  5   2  Ozone  Solar.R\n  5   3  Ozone  Solar.R\n  5   4  Ozone  Solar.R\n  5   5  Ozone  Solar.R\n\n\n2. categorical\nOnly three methods are available for imputation of categorical variables:\nmode - mode\nrpart - Recursive Partitioning and Regression Trees and\nmice - Multivariate Imputation by Chained Equations (random seed must be set)\nLet‚Äôs take the diamonds dataset, make it a bit smaller, add some NAs and use the mice imputation. The plot of the results compares the frequencies of each category, so that we can see which values were imputed. The summary command would give you the exact counts and proportions of categories before and after imputation.\nAnd already familiar plot_na_pareto() function can ensure that we have missing values in the old ‚Äúcut‚Äù variable and no missing values in the ‚Äúnew_cut‚Äù variable.\n\n\nd <- diamonds %>% \n  sample_n(1000) \n\nd[sample(seq(NROW(d)), 50), \"cut\"] <- NA\n\nd2 <- imputate_na(d, cut, price, method = \"mice\", seed = 999)\n\n\n\n iter imp variable\n  1   1  cut\n  1   2  cut\n  1   3  cut\n  1   4  cut\n  1   5  cut\n  2   1  cut\n  2   2  cut\n  2   3  cut\n  2   4  cut\n  2   5  cut\n  3   1  cut\n  3   2  cut\n  3   3  cut\n  3   4  cut\n  3   5  cut\n  4   1  cut\n  4   2  cut\n  4   3  cut\n  4   4  cut\n  4   5  cut\n  5   1  cut\n  5   2  cut\n  5   3  cut\n  5   4  cut\n  5   5  cut\n\nplot(d2)\n\n\n\nsummary(d2)\n\n\n* Impute missing values based on Multivariate Imputation by Chained Equations\n - method : mice\n - random seed : 999\n\n* Information of Imputation (before vs after)\n          original imputation original_percent imputation_percent\nFair            31         32              3.1                3.2\nGood            93        102              9.3               10.2\nIdeal          375        394             37.5               39.4\nPremium        243        253             24.3               25.3\nVery Good      208        219             20.8               21.9\n<NA>            50          0              5.0                0.0\n\n\n\nd %>% \n  mutate(new_cut = d2) %>% \n  select(cut, new_cut) %>% \n  plot_na_pareto()\n\n\n\n\n3. Imputation of outliers\nWe have seen that {dlookr} package can effectively find outliers and impute missing values. If the outliers are nasty, they can be treated like missing values. Therefore, if desired, the outliers can be easily replaced by the imputate_outlier() function. This function supports 4 following imputation methods:\nmean\nmedian\nmode and\ncapping - which imputes the upper outliers with 95th percentile, and the bottom outliers with 5th percentile. Here, I would also recommend to compare all 4 methods before deciding which method is the best. And if we look at the plots, I think a simple average wins this time.\n\n\nbla <- imputate_outlier(diamonds, carat, method = \"mean\")\nplot(bla)\n\n\n\nbla <- imputate_outlier(diamonds, carat, method = \"median\")\nplot(bla)\n\n\n\nbla <- imputate_outlier(diamonds, carat, method = \"mode\")\nplot(bla)\n\n\n\nbla <- imputate_outlier(diamonds, carat, method = \"capping\")\nplot(bla)\n\n\n\n\nBinning\nCategorization, or binning(), transforms a numeric variable into a categorical one. The following types of binning are supported:\nquantile, which categorizes using quantiles\nequal - categorizes to have equal length segments\npretty - categorizes into moderately good segments (whatever it it :)\nkmeans - categorization using K-means clustering\nbclust - categorization using clustering technique with bagging, where bagging is an abbreviation for Bootstrap Aggregating - a machine learning meta-algorithm designed to improve predictive performance.\nThe bclust method sounds most fancy, so, let‚Äôs try this one first. The distribution of the Ozone variable provides 3 intuitive groups:\nthe parabola from 0 to 60\nshort flat area from 60 to 80 and\nsteady decline till the end of the plot\n‚Ä¶thus, let‚Äôs set the number of bins (or categories) to 3 and name the categories with the argument ‚Äúlabels‚Äù. Plotting the result shows that our first intuition was correct, the parabola was singled out into a category. However, the third category is kind of small, and has only observation, which we can see using summary() command. And we wanna know the exact numbers were the borders of categories were drawn, we just don‚Äôt use argument ‚Äúlabels‚Äù in the binning() command:\n\n\nDataExplorer::plot_density(airquality$Ozone)\n\n\n\n# with names\nset.seed(4444)\nbin <- binning(airquality$Ozone, type = \"bclust\", nbins = 3,\n              labels = c(\"cat1\", \"cat2\", \"cat3\"))\n\nplot(bin)\n\n\n\nsummary(bin)\n\n\n  levels freq        rate\n1   cat1   83 0.542483660\n2   cat2   32 0.209150327\n3   cat3    1 0.006535948\n4   <NA>   37 0.241830065\n\n# without names\nbinning(airquality$Ozone, type = \"bclust\", nbins = 3)\n\n\nbinned type: bclust\nnumber of bins: 3\nx\n  [1,55.5] (55.5,152]  (152,168]       <NA> \n        83         32          1         37 \n\nIf we check out all the other categorizations, we can choose the one we feel the best about. I would take the kmeans, because it has relatively similar counts of data in every category, while still describing the distribution with three traits of the plot we have seen, namely parabola at the beginning of the plot, plateau in the middle and decline till the end.\n\n\nplot(binning(airquality$Ozone, type = \"quantile\", nbins = 3,\n              labels = c(\"cat1\", \"cat2\", \"cat3\")))\n\n\n\nplot(binning(airquality$Ozone, type = \"equal\", nbins = 3,\n              labels = c(\"cat1\", \"cat2\", \"cat3\")))\n\n\n\nplot(binning(airquality$Ozone, type = \"pretty\", nbins = 3))\n\n\n\nplot(binning(airquality$Ozone, type = \"kmeans\", nbins = 3,\n              labels = c(\"cat1\", \"cat2\", \"cat3\")))\n\n\n\n\nUsing a simple {dplyr} syntax we can easily add a new categorized Ozone variable into our dataset.\n\n\nairquality %>% \n  mutate(\n    binned_Ozone = binning(airquality$Ozone, type = \"quantile\", nbins = 3, labels = c(\"cat1\", \"cat2\", \"cat3\"))\n    ) %>% head() %>% flextable()\n\n\nOzoneSolar.RWindTempMonthDaybinned_Ozone411907.46751cat2361188.07252cat21214912.67453cat11831311.56254cat114.356552814.96656cat2\n\nStandardization and Resolving Skewness\ntransform() function performs both Standardization and Resolving Skewness.\n1. Standardization\nStandardization is the process of putting different variables on the same scale, so that we can better compare and model them. In order to standardize variables, we first calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, we subtract the mean and divide by the standard deviation. {dlookr} provides two methods for standardization, namely:\nzscore - z-score transformation is default: \\[ \\frac{x - mean}{sigma} \\]\nminmax - minmax transformation: \\[ \\frac{x - min}{max - min} \\]\n\n\nbla <- transform(airquality$Solar.R)\n\nplot(bla)\n\n\n\n\n2. Resolving Skewness\n‚Ä¶and 6 methods for resolving skewness:\nlog - transformation, which is great, but has problem with zeros.\nlog+1 - transformation, which is amazing for variables that contain zeros.\nsqrt - square root transformation.\n1/x - transformation\nx^2 - squared transformation and\nx^3 - power of three transformation\nlog+1 is my favorite, because it handles zeros, which otherwise produce ‚ÄúInfinities‚Äù with a simple logarithm.\n\n\nfind_skewness(mtcars, index = F)\n\n\n[1] \"mpg\"  \"hp\"   \"gear\" \"carb\"\n\ntransform(mtcars$hp, method = \"log+1\") %>% \n  plot(ylim=c(0,10))\n\n\n\n\nReporting\nSimilarly to diagnosis and exploratory reporting, we can produce a transformation report using a single intuitive command.\n\n\ntransformation_report(airquality, target = Temp)\n\n\n\nConclusion\nI hope {dlookr} package provides some value for you. It certainly made my life easier, that‚Äôs why I wanted to share it with you. I am super curious what R package you find the most useful? Please, let me know in the comments section below. All right, that‚Äôs it for today. If you liked this post, check out the other ones on my blog, or subscribe to my YouTube channel.\nThanks for learning ;) \nIf you think, I missed something, please comment on it below, and I‚Äôll improve this tutorial.\nUseful ressources\nhttps://github.com/choonghyunryu/dlookr\n\n\n\n",
    "preview": "posts/2021-01-30-r-package-reviews-dlookr-diagnose-explore-and-transform-your-data/dlookr_thumbnail.png",
    "last_modified": "2021-03-21T17:47:05+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress/",
    "title": "Deep Exploratory Data Analysis (EDA) in R",
    "description": "Exploratory Data Analysis is an important first step on the long way to the final result, be it a statistical inference in a scientific paper or a machine learning algorithm in production. This long way is often bumpy, highly iterative and time consuming. However, EDA might be the most important part of data analysis, because it helps to generate hypothesis, which then determine THE final RESULT. Thus, in this post I'll provide the simplest and most effective ways to explore data in R, which will significantly speed up your work. Moreover, we'll go one step beyond EDA by starting to test our hypotheses with simple statistical tests.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-09",
    "categories": [
      "EDA",
      "videos",
      "data wrangling",
      "visualization"
    ],
    "contents": "\n\nContents\nCreating visualised reports of the whole dataset with only one function!\n{DataExplorer}\n{SmartEDA}\n{dlookr}\n\nBig Picture of your data\n{DataExplorer}\n{funModeling}\n\nExplore categorical (discrete) variables\n{DataExplorer}\n{SmartEDA}\n{ggstatsplot}\n\nExplore numeric variables with descriptive statistics\n{dlookr}\n{SmartEDA}\n{summarytools} and {psych}\n\nSummary tools\n{summarytools}\n{skimr}\n{gtsummary}\n\nExplore distribution of numeric variables\n{DataExplorer}\n{moments}\nSkewness\nKurtosis\n\n\nCheck the normality of distribution\n{DataExplorer}\n{dlookr} visualization\n{ggpubr}\n{dlookr} Shapiro-Wilk normality tests\n\nExplore categorical and numeric variables with Box-Plots\n{DataExplorer}\n{ggstatsplot}\n{SmartEDA}\n\nExplore correlations\n{dlookr} - correlation\n{ggstatsplot}\n{PerformanceAnalytics}\n{fastStat}\n{dlookr} - linear models\n\nExploratory modelling\nExplore missing values\n{dlookr}\n\nExplore outliers\n{performance}\n{dlookr}\n\nImpute outliers\nConclusion\nFurther readings and references\n\nYou can run all packages at once, in order to avoid interruptions. You‚Äôd need to install some of them if you still didn‚Äôt, using install.packages(‚Äúname_of_the_package‚Äù) command.\n\n\nlibrary(DataExplorer)       # EDA\nlibrary(tidyverse)          # for everything good in R ;)\nlibrary(SmartEDA)           # EDA\nlibrary(dlookr)             # EDA\nlibrary(funModeling)        # EDA\nlibrary(ISLR)               # for the Wage dataset\nlibrary(ggstatsplot)        # publication ready visualizations with statistical details\nlibrary(flextable)          # beautifying tables\nlibrary(summarytools)       # EDA\nlibrary(psych)              # psychological research: descr. stats, FA, PCA etc.\nlibrary(skimr)              # summary stats\nlibrary(gtsummary)          # publication ready summary tables\nlibrary(moments)            # skewness, kurtosis and related tests\nlibrary(ggpubr)             # publication ready data visualization in R\nlibrary(PerformanceAnalytics) # econometrics for performance and risk analysis\nlibrary(fastStat)           # well :) you've guessed it\nlibrary(performance)        # Assessment of Regression Models Performance (for outliers here)\n\n\n\nI love R, because it is reach and generous. Having around 17.000 packages allows me to solve any data science problem. However, such abundance can be overwhelming, especially because one task can be accomplished by different functions from different packages with different levels of effectiveness. So, looking for the most effective way can be very time consuming! Thus, I hope that this collection of functions will save you some time. And if you know better functions or packages for EDA, please let me know in the comments below and let us together create here the one-stop solution for Deep EDA in R.\nCreating visualised reports of the whole dataset with only one function!\nThe most effective way to explore the data quick is the creation of automated reports. We‚Äôll have a look at three packages which are able to do this. DataExplorer package creates the best report in my opinion. SmartEDA and dlookr packages are also good choices. Three functions you are going to see in a moment will cover all the basics of EDA in a few seconds.\nIf you are more of a visual person, you can watch the R demo of automated data exploration here:\n\n\n\n\n\n\n\n{DataExplorer}\n{DataExplorer} report will deliver basic info about your dataset, like number of rows and columns, number of categorical and numeric variables, number of missing values and number of complete rows. It will also show you a missing data profile, where percentages of missing values in every variable are displayed. It plots the histograms and Quantile-Quantile plots for every numeric variable and bar-plots for every categorical variable. It finally explores the combinations of different variables, by conducting correlation analysis, principal component analysis, box and even scatter plots.\nIf one of the variables is of a particular importance for you, you can specify it and get much richer report. Simply execute the code below and see it for yourself.\n\n\nlibrary(DataExplorer) # for exploratory data analysis\nlibrary(tidyverse)    # for data, data wrangling and visualization\n\n\n\n\n\n# report without a response variable\ncreate_report(diamonds)\n\n# report with a response variable\ncreate_report(diamonds, y = \"price\")\n\n\n\n{SmartEDA}\nIn addition to the similar results, {SmartEDA} report also delivers descriptive statistics for every numeric variable with all important metrics you could need, like number of negative values, number of zeros, mean, median, standard deviation, IQR, bunch of different quantiles and even the number of outliers. {SmartEDA} also displays the density of every numeric variables instead of histograms. And while {DataExplorer} package can visualize density too, density plots are not part of the automated {DataExplorer} report.\nWhat I found particularly useful in {SmartEDA} report is that it provides the code responsible for a particular result. For instance, if you don‚Äôt need the whole report, but wanna see only descriptive statistics, you just copy the code, change the name of your dataset and get the same table you see in the report without looking for such code in the documentation. It saves time! Moreover, in {SmartEDA} package, you can give your report a name and save it in the directory of your choice.\n\n\nlibrary(SmartEDA) # for exploratory data analysis\n\nExpReport(diamonds, op_file = 'smarteda.html')\n\n\n\n{dlookr}\nOne of the most amazing features of {dlookr} package is that {dlookr} perfectly collaborates with {tidyverse} packages, like {dplyr} and {ggplot2}. This elevates the output of {dlookr} on another level. We‚Äôll see some examples here. Another advantage of {dlookr} package is that you can choose the output to be a PDF (by default) or HTML files. Moreover, it also separates between three kinds of reports: diagnose report, EDA report and transformation report.\nThe diagnose report delivers:\nthe number of missing and unique values,\ncounts, proportions and ranks of categorical variables,\ndescriptive stats, number of zeros, negative values and number of outliers of numeric variables and finally\nvisualizes every numeric variables with and without outliers\n\n\nlibrary(dlookr) # for exploratory data analysis\n\n# report without a response variable\ndiagnose_report(diamonds) # pdf by default\n\n\n\nSimilarly to the {DataExplorer} report, we can get much richer EDA report from {dlookr} by specifying the target variable. Let‚Äôs export the EDA report in the HTML format and look at it. This EDA report delivers:\nvisual normality tests of all numerical variables with a histogram and a Quantile-Quantile plot plus two histograms of the most common transformations of data, namely log- and square-root. This is quite useful, because we can immediately see whether we need to transform the data and which type of transformation is more useful;\nthis report also provides correlation coefficients and plots for all possible combinations of numeric variable, then\nsince our target variable is categorical, EDA report provides descriptive statistics for every category of our target variable and every numeric variable, and contingency tables for every category of our target variable and every categorical variables;\nit also visualizes the distribution of every other variable vs.¬†the target variables using box-plots and density plots, with and without outliers.\n\n\n# report with a response variable and some dplyr sintax\ndiamonds %>%\n  eda_report(\n    target        = cut, \n    output_format = \"html\", \n    output_file   = \"EDA_diamonds.html\")\n\n\n\nFinally the transformation report, which is my absolute favorite:\nimputes missing values with multiple methods simultaneously, so that you can compare the distribution of the data after different imputation techniques and choose the best imputation method for every particular variable,\nit also imputes outliers with different methods,\nresolve skewness of the data with different methods and\nis even able to categorize numeric variables, if needed.\n\n\n# example with missing values\ntransformation_report(airquality, target = Temp)\n\n# example with outliers\ntransformation_report(diamonds, target = price)\n\n\n\nBig Picture of your data\nBig reports might be overwhelming though, and we often need only a particular aspect of data exploration. Fortunately, you can get any part of the big report separately. For instance, the basic description for airquality dataset can be reached via functions introduce() and plot_intro() from {DataExplorer} package.\n{DataExplorer}\n\n\nintroduce(airquality) %>% t() \n\n\n                     [,1]\nrows                  153\ncolumns                 6\ndiscrete_columns        0\ncontinuous_columns      6\nall_missing_columns     0\ntotal_missing_values   44\ncomplete_rows         111\ntotal_observations    918\nmemory_usage         6376\n\nplot_intro(airquality)\n\n\n\n\n{funModeling}\n{funModeling} package provides a similar function with some useful metrics, like number of zeros, NAs or unique values for every variable.\n\n\nlibrary(funModeling)    # EDA\nstatus(airquality) %>% flextable()\n\n\nvariableq_zerosp_zerosq_nap_naq_infp_inftypeuniqueOzone00370.2418300700integer67Solar.R0070.0457516300integer117Wind0000.0000000000numeric31Temp0000.0000000000integer40Month0000.0000000000integer5Day0000.0000000000integer31\n\nIf you are tired of reading, you can watch the rest of this post in action as a video:\n\n\n\n\n\n\n\nExplore categorical (discrete) variables\n{DataExplorer}\nSimple bar plots with frequency distribution of all categorical variables are already quite useful, because they provide a quick overview about the meaningfulness of the categorization, and whether there are some typing mistakes in the data. {DataExplorer} package provides a simple plot_bar() function which does just that. However, plotting a target discrete variable by another discrete variable is even more useful. It is some sort of a visual frequency table (see the second plot below). For this use the by =  argument and give it the second categorical variable.\n\n\nplot_bar(diamonds)\n\n\n\nplot_bar(diamonds, by = \"cut\")\n\n\n\n\n{SmartEDA}\nExpCatViz function from {SmartEDA} package also plots each categorical variable with a bar plot, but displays proportions instead of counts.\n\n\nExpCatViz(diamonds, Page = c(1,3))\n\n\n$`0`\n\n\nAnd here we finally come to the DEEP part of EDA. The plot below nearly ‚Äúscrims‚Äù the hypothesis that education level is strongly associated with the job. Namely, the more educated we get, the more likely we‚Äôll end up working with information (e.g.¬†with data ;) and the less likely we‚Äôll end up working in a factory. However, without a proper statistical test and a p-value this hypothesis can not be tested and remains ‚Ä¶ well ‚Ä¶ a speculation.\n\n\nlibrary(ISLR)      # for the Wage dataset\nExpCatViz(\n  Wage %>% \n    select(education, jobclass), \n  target=\"education\")\n\n\n[[1]]\n\n\n{ggstatsplot}\nFortunately, the ggbarstats() function from {ggstatsplot} package does all the above in one line of code and even goes one step further. Namely:\nit counts and calculates percentages for every category,\nit visualizes the ‚Äúfrequency table‚Äù in the form of stacked bars and\nprovides numerous statistical details (including p-value) in addition to visualization, which allows us to make a conclusion or inference already in the exploratory phase of the project!\n\n\nlibrary(ggstatsplot)     # visualization with statistical details\nggbarstats(\n  data  = Wage, \n  x     = jobclass, \n  y     = education, \n  label = \"both\")\n\n\n\n\nExplore numeric variables with descriptive statistics\n{dlookr}\nDescriptive statistics is usually needed for either a whole numeric variable, or for a numeric variable separated in groups of some categorical variable, like control & treatment. Three functions from {dlookr} package, namely describe(), univar_numeric() and diagnose_numeric() do totally nail it. Be careful with the describe() function though, because it also exists in {Hmisc} and {psych} packages too. Thus, in order to avoid the confusion, simply write dlookr:: in front of describe() function, which then provides the most common descriptive stats, like counts, number o missing values, mean, standard deviation, standard error of the mean, IQR, skewness, kurtosis and 17 quantiles.\n\n\nlibrary(flextable)        # beautifying tables\ndlookr::describe(iris) %>% flextable()\n\n\nvariablennameansdse_meanIQRskewnesskurtosisp00p01p05p10p20p25p30p40p50p60p70p75p80p90p95p99p100Sepal.Length15005.8433330.82806610.067611321.30.3149110-0.5520644.34.4004.6004.85.05.15.275.605.806.106.36.46.526.907.2557.7007.9Sepal.Width15003.0573330.43586630.035588330.50.31896570.2282492.02.2002.3452.52.72.82.803.003.003.103.23.33.403.613.8004.1514.4Petal.Length15003.7580001.76529820.144136003.5-0.2748842-1.4021031.01.1491.3001.41.51.61.703.904.354.645.05.15.325.806.1006.7006.9Petal.Width15001.1993330.76223770.062236451.5-0.1029667-1.3406040.10.1000.2000.20.20.30.401.161.301.501.81.81.902.202.3002.5002.5\n\nHere, we can also see how useful can be collaboration of {dlookr} with {tidyverse} packages, like {dplyr} and its group_by() function!, which calculates descriptive statistics per group. And if you don‚Äôt need such a monstrous table, but only want to have the median() instead of 17 quantiles, use univar_numeric() function.\n\n\niris %>% \n  group_by(Species) %>% \n  univar_numeric() %>% \n  knitr::kable()\n\n\n\n\nvariable\n\n\nSpecies\n\n\nn\n\n\nna\n\n\nmean\n\n\nsd\n\n\nse_mean\n\n\nIQR\n\n\nskewness\n\n\nmedian\n\n\nSepal.Length\n\n\nsetosa\n\n\n50\n\n\n0\n\n\n5.006\n\n\n0.3524897\n\n\n0.0498496\n\n\n0.400\n\n\n0.1200870\n\n\n5.00\n\n\nSepal.Length\n\n\nversicolor\n\n\n50\n\n\n0\n\n\n5.936\n\n\n0.5161711\n\n\n0.0729976\n\n\n0.700\n\n\n0.1053776\n\n\n5.90\n\n\nSepal.Length\n\n\nvirginica\n\n\n50\n\n\n0\n\n\n6.588\n\n\n0.6358796\n\n\n0.0899270\n\n\n0.675\n\n\n0.1180151\n\n\n6.50\n\n\nSepal.Width\n\n\nsetosa\n\n\n50\n\n\n0\n\n\n3.428\n\n\n0.3790644\n\n\n0.0536078\n\n\n0.475\n\n\n0.0411665\n\n\n3.40\n\n\nSepal.Width\n\n\nversicolor\n\n\n50\n\n\n0\n\n\n2.770\n\n\n0.3137983\n\n\n0.0443778\n\n\n0.475\n\n\n-0.3628448\n\n\n2.80\n\n\nSepal.Width\n\n\nvirginica\n\n\n50\n\n\n0\n\n\n2.974\n\n\n0.3224966\n\n\n0.0456079\n\n\n0.375\n\n\n0.3659491\n\n\n3.00\n\n\nPetal.Length\n\n\nsetosa\n\n\n50\n\n\n0\n\n\n1.462\n\n\n0.1736640\n\n\n0.0245598\n\n\n0.175\n\n\n0.1063939\n\n\n1.50\n\n\nPetal.Length\n\n\nversicolor\n\n\n50\n\n\n0\n\n\n4.260\n\n\n0.4699110\n\n\n0.0664554\n\n\n0.600\n\n\n-0.6065077\n\n\n4.35\n\n\nPetal.Length\n\n\nvirginica\n\n\n50\n\n\n0\n\n\n5.552\n\n\n0.5518947\n\n\n0.0780497\n\n\n0.775\n\n\n0.5494446\n\n\n5.55\n\n\nPetal.Width\n\n\nsetosa\n\n\n50\n\n\n0\n\n\n0.246\n\n\n0.1053856\n\n\n0.0149038\n\n\n0.100\n\n\n1.2538614\n\n\n0.20\n\n\nPetal.Width\n\n\nversicolor\n\n\n50\n\n\n0\n\n\n1.326\n\n\n0.1977527\n\n\n0.0279665\n\n\n0.300\n\n\n-0.0311796\n\n\n1.30\n\n\nPetal.Width\n\n\nvirginica\n\n\n50\n\n\n0\n\n\n2.026\n\n\n0.2746501\n\n\n0.0388414\n\n\n0.500\n\n\n-0.1294769\n\n\n2.00\n\n\n\ndiagnose_numeric() function reports the usual 5-number-summary (which is actually a box-plot in a table form) and the number of zeros, negative values and outliers.\n\n\niris %>% \n  diagnose_numeric() %>% \n  flextable()\n\n\nvariablesminQ1meanmedianQ3maxzerominusoutlierSepal.Length4.35.15.8433335.806.47.9000Sepal.Width2.02.83.0573333.003.34.4004Petal.Length1.01.63.7580004.355.16.9000Petal.Width0.10.31.1993331.301.82.5000\n\n{SmartEDA}\n{SmartEDA} with its ExpNumStat() function provides, in my opinion, the richest and the most comprehensive descriptive statistics table. Moreover we can choose to describe the whole variables, grouped variables, or even both at the same time. If we call the argument ‚Äúby =‚Äù with a big letter A, we‚Äôll get statistics for every numeric variable in the dataset. The big G delivers descriptive stats per GROUP, but we‚Äôll need to specify a group in the next argument ‚Äúgr =‚Äù. Using GA, would give you both. We can also specify the quantiles we need and identify the lower hinge, upper hinge and number of outliers, if we want to.\n\n\nExpNumStat(iris, by=\"A\", Outlier=TRUE, Qnt = c(.25, .75), round = 2) %>% flextable()\n\nExpNumStat(iris, by=\"G\", gp=\"Species\", Outlier=TRUE, Qnt = c(.25, .75), round = 2) %>% flextable()\n\n\n\n\n\nExpNumStat(iris, by=\"GA\", gp=\"Species\", Outlier=TRUE, Qnt = c(.25, .75), round = 2) %>% flextable()\n\n\nVnameGroupTNnNegnZeronPosNegInfPosInfNA_ValuePer_of_MissingsumminmaxmeanmedianSDCVIQRSkewnessKurtosis25%75%LB.25%UB.75%nOutliersPetal.LengthSpecies:All150001500000563.71.06.93.764.351.770.473.50-0.27-1.401.605.10-3.6510.350Petal.LengthSpecies:setosa500050000073.11.01.91.461.500.170.120.180.100.801.401.581.141.844Petal.LengthSpecies:versicolor5000500000213.03.05.14.264.350.470.110.60-0.59-0.074.004.603.105.501Petal.LengthSpecies:virginica5000500000277.64.56.95.555.550.550.100.780.53-0.265.105.883.947.040Petal.WidthSpecies:All150001500000179.90.12.51.201.300.760.641.50-0.10-1.340.301.80-1.954.050Petal.WidthSpecies:setosa500050000012.30.10.60.250.200.110.430.101.221.430.200.300.050.452Petal.WidthSpecies:versicolor500050000066.31.01.81.331.300.200.150.30-0.03-0.491.201.500.751.950Petal.WidthSpecies:virginica5000500000101.31.42.52.032.000.270.140.50-0.13-0.661.802.301.053.050Sepal.LengthSpecies:All150001500000876.54.37.95.845.800.830.141.300.31-0.575.106.403.158.350Sepal.LengthSpecies:setosa5000500000250.34.35.85.015.000.350.070.400.12-0.354.805.204.205.800Sepal.LengthSpecies:versicolor5000500000296.84.97.05.945.900.520.090.700.10-0.605.606.304.557.350Sepal.LengthSpecies:virginica5000500000329.44.97.96.596.500.640.100.670.11-0.096.236.905.217.911Sepal.WidthSpecies:All150001500000458.62.04.43.063.000.440.140.500.320.182.803.302.054.054Sepal.WidthSpecies:setosa5000500000171.42.34.43.433.400.380.110.480.040.743.203.682.494.392Sepal.WidthSpecies:versicolor5000500000138.52.03.42.772.800.310.110.48-0.35-0.452.523.001.813.710Sepal.WidthSpecies:virginica5000500000148.72.23.82.973.000.320.110.380.350.522.803.182.243.743\n\n{summarytools} and {psych}\n{summarytools} and {psych} packages also provide useful tables with descriptive stats, but since they do not offer anything dramatically new compared to functions presented above, I‚Äôll just provide the code but would not display the results. By the way, summarytools sounds like a good topic for the next chapter‚Ä¶\n\n\nlibrary(summarytools)\niris %>% \n  group_by(Species) %>% \n  descr()\n\nlibrary(psych)\ndescribeBy(iris,\n           iris$Species)\n\n\n\nSummary tools\nThis topic can be singled out because functions presented below give you a quick overview about the whole dataset and some of them also check hypothesis with simple statistical tests.\n{summarytools}\nFor instance, dfSummary() function from {summarytools} package provides some basic descriptive stats for numeric and counts with proportions for categorical variables. It even tries to somehow plot the distribution of both, but I didn‚Äôt find those plots useful. What is useful though, is that dfSummary() provides a number of duplicates and missing values.\n\n\nlibrary(summarytools)\ndfSummary(diamonds)\n\n\nData Frame Summary  \ndiamonds  \nDimensions: 53940 x 10  \nDuplicates: 146  \n\n-----------------------------------------------------------------------------------------------------------------------------\nNo   Variable            Stats / Values                 Freqs (% of Valid)      Graph                    Valid      Missing  \n---- ------------------- ------------------------------ ----------------------- ------------------------ ---------- ---------\n1    carat               Mean (sd) : 0.8 (0.5)          273 distinct values     :                        53940      0        \n     [numeric]           min < med < max:                                       : .                      (100.0%)   (0.0%)   \n                         0.2 < 0.7 < 5                                          : :                                          \n                         IQR (CV) : 0.6 (0.6)                                   : : .                                        \n                                                                                : : : .                                      \n\n2    cut                 1. Fair                         1610 ( 3.0%)                                    53940      0        \n     [ordered, factor]   2. Good                         4906 ( 9.1%)           I                        (100.0%)   (0.0%)   \n                         3. Very Good                   12082 (22.4%)           IIII                                         \n                         4. Premium                     13791 (25.6%)           IIIII                                        \n                         5. Ideal                       21551 (40.0%)           IIIIIII                                      \n\n3    color               1. D                            6775 (12.6%)           II                       53940      0        \n     [ordered, factor]   2. E                            9797 (18.2%)           III                      (100.0%)   (0.0%)   \n                         3. F                            9542 (17.7%)           III                                          \n                         4. G                           11292 (20.9%)           IIII                                         \n                         5. H                            8304 (15.4%)           III                                          \n                         6. I                            5422 (10.1%)           II                                           \n                         7. J                            2808 ( 5.2%)           I                                            \n\n4    clarity             1. I1                            741 ( 1.4%)                                    53940      0        \n     [ordered, factor]   2. SI2                          9194 (17.0%)           III                      (100.0%)   (0.0%)   \n                         3. SI1                         13065 (24.2%)           IIII                                         \n                         4. VS2                         12258 (22.7%)           IIII                                         \n                         5. VS1                          8171 (15.1%)           III                                          \n                         6. VVS2                         5066 ( 9.4%)           I                                            \n                         7. VVS1                         3655 ( 6.8%)           I                                            \n                         8. IF                           1790 ( 3.3%)                                                        \n\n5    depth               Mean (sd) : 61.7 (1.4)         184 distinct values               :              53940      0        \n     [numeric]           min < med < max:                                                 :              (100.0%)   (0.0%)   \n                         43 < 61.8 < 79                                                   :                                  \n                         IQR (CV) : 1.5 (0)                                             . :                                  \n                                                                                        : :                                  \n\n6    table               Mean (sd) : 57.5 (2.2)         127 distinct values         :                    53940      0        \n     [numeric]           min < med < max:                                           :                    (100.0%)   (0.0%)   \n                         43 < 57 < 95                                               :                                        \n                         IQR (CV) : 3 (0)                                           : :                                      \n                                                                                    : :                                      \n\n7    price               Mean (sd) : 3932.8 (3989.4)    11602 distinct values   :                        53940      0        \n     [integer]           min < med < max:                                       :                        (100.0%)   (0.0%)   \n                         326 < 2401 < 18823                                     :                                            \n                         IQR (CV) : 4374.2 (1)                                  : : .                                        \n                                                                                : : : : . . .                                \n\n8    x                   Mean (sd) : 5.7 (1.1)          554 distinct values             :                53940      0        \n     [numeric]           min < med < max:                                               : .              (100.0%)   (0.0%)   \n                         0 < 5.7 < 10.7                                                 : : :                                \n                         IQR (CV) : 1.8 (0.2)                                           : : :                                \n                                                                                      . : : : :                              \n\n9    y                   Mean (sd) : 5.7 (1.1)          552 distinct values     :                        53940      0        \n     [numeric]           min < med < max:                                       : :                      (100.0%)   (0.0%)   \n                         0 < 5.7 < 58.9                                         : :                                          \n                         IQR (CV) : 1.8 (0.2)                                   : :                                          \n                                                                                : :                                          \n\n10   z                   Mean (sd) : 3.5 (0.7)          375 distinct values       :                      53940      0        \n     [numeric]           min < med < max:                                         :                      (100.0%)   (0.0%)   \n                         0 < 3.5 < 31.8                                         : :                                          \n                         IQR (CV) : 1.1 (0.2)                                   : :                                          \n                                                                                : :                                          \n-----------------------------------------------------------------------------------------------------------------------------\n\n{skimr}\n{skimr} is another useful package which provides both some basic descriptive stats of numeric variables and counts for categorical variables. Besides, it is also able to use {dplyr‚Äôs} group_by() function (not shown).\n\n\nlibrary(skimr)\nskim(diamonds)\n\n\n\nTable 1: Data summary\n\n\n\n\n\n\nName\n\n\ndiamonds\n\n\nNumber of rows\n\n\n53940\n\n\nNumber of columns\n\n\n10\n\n\n_______________________\n\n\n\n\nColumn type frequency:\n\n\n\n\nfactor\n\n\n3\n\n\nnumeric\n\n\n7\n\n\n________________________\n\n\n\n\nGroup variables\n\n\nNone\n\nVariable type: factor\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nordered\n\n\nn_unique\n\n\ntop_counts\n\n\ncut\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n5\n\n\nIde: 21551, Pre: 13791, Ver: 12082, Goo: 4906\n\n\ncolor\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n7\n\n\nG: 11292, E: 9797, F: 9542, H: 8304\n\n\nclarity\n\n\n0\n\n\n1\n\n\nTRUE\n\n\n8\n\n\nSI1: 13065, VS2: 12258, SI2: 9194, VS1: 8171\n\nVariable type: numeric\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\ncarat\n\n\n0\n\n\n1\n\n\n0.80\n\n\n0.47\n\n\n0.2\n\n\n0.40\n\n\n0.70\n\n\n1.04\n\n\n5.01\n\n\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\ndepth\n\n\n0\n\n\n1\n\n\n61.75\n\n\n1.43\n\n\n43.0\n\n\n61.00\n\n\n61.80\n\n\n62.50\n\n\n79.00\n\n\n‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ\n\n\ntable\n\n\n0\n\n\n1\n\n\n57.46\n\n\n2.23\n\n\n43.0\n\n\n56.00\n\n\n57.00\n\n\n59.00\n\n\n95.00\n\n\n‚ñÅ‚ñá‚ñÅ‚ñÅ‚ñÅ\n\n\nprice\n\n\n0\n\n\n1\n\n\n3932.80\n\n\n3989.44\n\n\n326.0\n\n\n950.00\n\n\n2401.00\n\n\n5324.25\n\n\n18823.00\n\n\n‚ñá‚ñÇ‚ñÅ‚ñÅ‚ñÅ\n\n\nx\n\n\n0\n\n\n1\n\n\n5.73\n\n\n1.12\n\n\n0.0\n\n\n4.71\n\n\n5.70\n\n\n6.54\n\n\n10.74\n\n\n‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÅ\n\n\ny\n\n\n0\n\n\n1\n\n\n5.73\n\n\n1.14\n\n\n0.0\n\n\n4.72\n\n\n5.71\n\n\n6.54\n\n\n58.90\n\n\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nz\n\n\n0\n\n\n1\n\n\n3.54\n\n\n0.71\n\n\n0.0\n\n\n2.91\n\n\n3.53\n\n\n4.04\n\n\n31.80\n\n\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n{gtsummary}\nFirst of all, tbl_summary() function from {gtsummary} package summarizes all categorical variables by counts and percentages, while all numeric variables by median and IQR. The argument by = inside of tbl_summary() specifies a grouping variable. The add_p() function then conducts statistical tests with all variables and provides p-values. For numeric variables it uses the non-parametric Wilcoxon rank sum test for comparing two groups and the non-parametric Kruskal-Wallis rank sum test for more then two groups. Categorical variables are checked with Fisher‚Äôs exact test, if number of observations in any of the groups is below 5, or with Pearson‚Äôs Chi-squared test for more data.\n\n\nlibrary(gtsummary)\n\nmtcars %>% \n  select(mpg, hp, am, gear, cyl) %>% \n  tbl_summary(by = am) %>% \n  add_p()\n\n\nCharacteristic\n      0, N = 191\n      1, N = 131\n      p-value2\n    mpg\n      17.3 (14.9, 19.2)\n      22.8 (21.0, 30.4)\n      0.002\n    hp\n      175 (116, 192)\n      109 (66, 113)\n      0.046\n    gear\n      \n      \n      \n    3\n      15 (79%)\n      0 (0%)\n      \n    4\n      4 (21%)\n      8 (62%)\n      \n    5\n      0 (0%)\n      5 (38%)\n      \n    cyl\n      \n      \n      0.009\n    4\n      3 (16%)\n      8 (62%)\n      \n    6\n      4 (21%)\n      3 (23%)\n      \n    8\n      12 (63%)\n      2 (15%)\n      \n    \n        \n          1\n          \n           \n          Median (IQR); n (%)\n          \n        \n          2\n          \n           \n          Wilcoxon rank sum test; Fisher's exact test\n          \n      \n    \n\n\n\nWage %>%\n  select(age, wage, education, jobclass) %>% \n  tbl_summary(by = education) %>% \n  add_p()\n\n\nCharacteristic\n      1. < HS Grad, N = 2681\n      2. HS Grad, N = 9711\n      3. Some College, N = 6501\n      4. College Grad, N = 6851\n      5. Advanced Degree, N = 4261\n      p-value2\n    age\n      42 (33, 50)\n      42 (33, 50)\n      40 (32, 49)\n      43 (34, 51)\n      44 (38, 53)\n      \n    wage\n      81 (70, 97)\n      94 (78, 110)\n      105 (89, 121)\n      119 (100, 143)\n      142 (117, 171)\n      \n    jobclass\n      \n      \n      \n      \n      \n      \n    1. Industrial\n      190 (71%)\n      636 (65%)\n      342 (53%)\n      274 (40%)\n      102 (24%)\n      \n    2. Information\n      78 (29%)\n      335 (35%)\n      308 (47%)\n      411 (60%)\n      324 (76%)\n      \n    \n        \n          1\n          \n           \n          Median (IQR); n (%)\n          \n        \n          2\n          \n           \n          Kruskal-Wallis rank sum test; Pearson's Chi-squared test\n          \n      \n    \n\nExplore distribution of numeric variables\n{DataExplorer}\nWhy do we need to explore the distribution? Well, many statistical tests depend on symmetric and normally distributed data. Histograms and density plots allow us the first glimpse on the data. For example, {DataExplorer} package provides very intuitive functions for getting histogram and density plots of all continuous variables at once, namely plot_histogram() and plot_density(). Moreover, they both collaborate perfectly with {dplyr} package, which is always a good think! So, looking at two variables displayed here, we can see that Wind is distributed kind of symmetric while Ozone is not. But how can we measure the symmetry of data? And when is data symmetric enough?\n\n\nplot_histogram(Wage)\n\n\n\nplot_density(Wage)\n\n\n\n# works perfectly with dplyr!\nairquality %>% \n  select(Ozone, Wind) %>% \n  plot_density()\n\n\n\n\n{moments}\nSkewness\nThe symmetry can be described by two measures: skewness and kurtosis. They are useful, because significant skewness and kurtosis clearly indicate not-normally distributed data.\nSkewness measures the lack of symmetry. A data is symmetric if it looks the same to the left and to the right of the central point. The skewness for a perfectly normal distribution is zero, so that any symmetric data should have a skewness near zero. Positive values for the skewness indicate data that are skewed to the right, which means that most of the data is actually on the left side of the plot, like on our Ozone plot. Negative values would then indicate skewness to the left, with most of data being on the right side of the plot. Using skewness() function from {moments} package shows that the skewness of Ozone is indeed positive and is far away from the zero, which suggests that Ozone is not-normally distributed.\nGeneral guidelines for the measure of skewness are following:\nif skewness is less than -1 or greater than 1, the distribution is highly skewed,\nif skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed and\nif skewness is between -0.5 and 0.5, the distribution is approximately symmetric.\nBut here again, how far from zero would be far enough in order to say that data is significantly skewed and therefore not-normally distributed? Well, D‚ÄôAgostino skewness test from {moments} package provides a p-value for that. For instance, a p-value for Ozone is small, which rejects the Null Hypothesis about not-skewed data, saying that Ozone data is actually significantly skewed. In contrast the p-value for Wind is above the usual significance threshold of 0.05, so that we can treat Wind data as not-skewed, and therefore - normal.\n\n\nlibrary(moments)\nskewness(airquality$Ozone, na.rm = T)   \n\n\n[1] 1.225681\n\nskewness(airquality$Wind, na.rm = T)  \n\n\n[1] 0.3443985\n\nagostino.test(airquality$Ozone)\n\n\n\n    D'Agostino skewness test\n\ndata:  airquality$Ozone\nskew = 1.2257, z = 4.6564, p-value = 3.219e-06\nalternative hypothesis: data have a skewness\n\nagostino.test(airquality$Wind)\n\n\n\n    D'Agostino skewness test\n\ndata:  airquality$Wind\nskew = 0.3444, z = 1.7720, p-value = 0.07639\nalternative hypothesis: data have a skewness\n\nKurtosis\nKurtosis is a measure of heavy tails, or outliers present in the distribution. The kurtosis value for a normal distribution is around three. Here again, we‚Äôd need to do a proper statistical test which will give us a p-value saying whether kurtosis result is significantly far away from three. {moments} package provides Anscombe-Glynn kurtosis test for that. For instance, Ozone has a Kurtosis value of 4.1 which is significantly far away from 3, indicating a not normally distributed data and probable presence of outliers. In contrast, the Kurtosis for Wind is around 3 and the p-value tells us that Wind distribution is fine.\n\n\nanscombe.test(airquality$Ozone)\n\n\n\n    Anscombe-Glynn kurtosis test\n\ndata:  airquality$Ozone\nkurt = 4.1841, z = 2.2027, p-value = 0.02762\nalternative hypothesis: kurtosis is not equal to 3\n\nanscombe.test(airquality$Wind)\n\n\n\n    Anscombe-Glynn kurtosis test\n\ndata:  airquality$Wind\nkurt = 3.0688, z = 0.4478, p-value = 0.6543\nalternative hypothesis: kurtosis is not equal to 3\n\nCheck the normality of distribution\nNow, finally, the normality of the distribution itself can, and should be always checked. It‚Äôs useful, because it helps us to determine a correct statistical test. For instance, if the data is normally distributed, we should use parametric tests, like t-test or ANOVA. If, however, the data is not-normally distributed, we should use non-parametric tests, like Mann-Whitney or Kruskal-Wallis. So, the normality check is not just another strange statistical concept we need to learn, but it‚Äôs actually helpful.\nThere are two main ways to check the normality: using a Quantile-Quantile plot and using a proper statistical test. And, we need them both!\n{DataExplorer}\n{DataExplorer} package provides a simple and elegant plot_qq() function, which produces Quantile-Quantile plots either for all continuous variables in the dataset, or, even for every group of a categorical variable, if the argument by =  is specified.\n\n\nplot_qq(iris)\n\n\n\nplot_qq(iris, by = \"Species\")\n\n\n\n\n{dlookr} visualization\nCool, right? But plot_normality() function from {dlookr} package visualizes not only Quantile-Quantile plot, but also the histogram of the original data and histograms of two of the most common transformations of data, namely log & square root transformations, in case the normality assumption wasn‚Äôt met. This allows us to see, whether transformation actually improves something or not, because its not always the case. Here we could also use {dplyr} syntax in order to quickly visualize several groups.\n\n\niris %>%\n  group_by(Species) %>%\n  plot_normality(Petal.Length)\n\n\n\n\nHowever, we still don‚Äôt know, when our data is normally distributed. The QQ-plot can be interpreted in following way: if points are situated close to the diagonal line, the data is probably normally distributed. But here we go again! How close is close enough? It‚Äôs actually very subjective! That is why, I like to explore QQ-plots using {ggpubr} package‚Ä¶(read on the next chapter)\n{ggpubr}\n‚Ä¶ which goes one step further and shows confidence intervals, which help to decide whether the deviation from normality is big or not. For example, if all or most of the data fall into these confidence intervals, we can conclude that data is normally distributed. However, in order to to be sure, we‚Äôd need to actually do a statistical test, which is in most cases a Shapiro-Wilk Normality test.\n\n\nlibrary(ggpubr)\nggqqplot(iris, \"Sepal.Length\", facet.by = \"Species\")\n\n\n\n\n{dlookr} Shapiro-Wilk normality tests\nVery intuitive normality() function from {dlookr} package performs Shapiro-Wilk Normality test with every numeric variable in the dataset. For example, we have seen that variable Wind in airquality dataset has a nice skewness and kurtosis, so, it suppose to be normally distributed, while variable Ozone suppose to be not-normally distributed, right? And indeed, normality() function totally confirms that.\n\n\nnormality(airquality) %>%\n  mutate_if(is.numeric, ~round(., 3)) %>% \n  flextable()\n\n\nvarsstatisticp_valuesampleOzone0.8790.000153Solar.R0.9420.000153Wind0.9860.118153Temp0.9760.009153Month0.8880.000153Day0.9530.000153\n\nMoreover, via the collaboration with {dplyr} package and it‚Äôs group_by() function we can conduct around 2000 normality tests in seconds and only few lines of code:\n\n\ndiamonds %>%\n  group_by(cut, color, clarity) %>%\n  normality()\n\n\n# A tibble: 1,932 x 7\n   variable cut   color clarity statistic     p_value sample\n   <chr>    <ord> <ord> <ord>       <dbl>       <dbl>  <dbl>\n 1 carat    Fair  D     I1          0.888 0.373            4\n 2 carat    Fair  D     SI2         0.867 0.0000183       56\n 3 carat    Fair  D     SI1         0.849 0.00000379      58\n 4 carat    Fair  D     VS2         0.931 0.0920          25\n 5 carat    Fair  D     VS1         0.973 0.893            5\n 6 carat    Fair  D     VVS2        0.871 0.127            9\n 7 carat    Fair  D     VVS1        0.931 0.493            3\n 8 carat    Fair  D     IF          0.990 0.806            3\n 9 carat    Fair  E     I1          0.941 0.596            9\n10 carat    Fair  E     SI2         0.867 0.000000807     78\n# ‚Ä¶ with 1,922 more rows\n\nSo, why don‚Äôt we just do our Shapiro-Wilk tests all the time and forget all those skewnesses and visualizations? Well, because given enough data, Shapiro-Wilk test will always find some non-normality even in perfectly symmetric bell-shaped data. Here is an example of a vector with less than 300 values, where Shapiro-Wilk test shows highly significant deviation from normality, while a density plot shows a bell curved data distribution. Moreover, tests or skewness, kurtosis and Quantile-Quantile plot all indicate normally distributed data. Thus, it‚Äôs always better to check several options before making a conclusion about normality of the data.\n\n\nbla <- Wage %>%\n  filter(education == \"1. < HS Grad\") %>% \n  select(age)\n\nnormality(bla) %>% flextable()\n\n\nvarsstatisticp_valuesampleage0.98551880.008259363268\n\nplot_density(bla)\n\n\n\nagostino.test(bla$age)\n\n\n\n    D'Agostino skewness test\n\ndata:  bla$age\nskew = 0.24361, z = 1.64889, p-value = 0.09917\nalternative hypothesis: data have a skewness\n\nanscombe.test(bla$age)\n\n\n\n    Anscombe-Glynn kurtosis test\n\ndata:  bla$age\nkurt = 2.6282, z = -1.3503, p-value = 0.1769\nalternative hypothesis: kurtosis is not equal to 3\n\nggqqplot(bla$age)\n\n\n\n\nExplore categorical and numeric variables with Box-Plots\nBox-plots help us to explore a combination of numeric and categorical variables. Put near each other, box-plots show whether distribution of several groups differ.\n{DataExplorer}\nFor example, using the intuitive plot_boxplot() function from {DataExplorer} package with an argument by =  which specifies a grouping, variable, will put all groups of all numeric variables into the boxes. Such exploration however immediately creates the next question - do these groups differ significantly? We can not tell that from just staring at the picture‚Ä¶\n\n\nplot_boxplot(iris, by = \"Species\")\n\n\n\n\n{ggstatsplot}\n‚Ä¶but if we use a ggbetweenstats() function from {ggstatsplot} package, we‚Äôd check tons of hypothesis using only a few intuitive arguments. For instance:\ndata\nx axis, where we determine the grouping categorical variable\ny axis, where we have our numeric variable of interest and\nthe type of the test, which I would always set to non-parametric for exploratory analysis.\nThis simple code not only provides you with a p-value which tells you whether there are significant differences between groups, but also conducts a correct multiple pairwise comparisons to see between which groups exactly these differences are. ggbetweenstats() even adjusts the p-values for multiple comparisons with Holm method automatically and produces bunch of other statistical details on top of the amazing visualization. If fact, I found ggbetweenstats() function sooo useful, that I did two separate videos on it already.\n\n\nggbetweenstats(\n  data = iris, \n  x    = Species, \n  y    = Sepal.Length, \n  type = \"np\")\n\n\n\n\n{SmartEDA}\nThe only useful thing here, compared to function provided above, is plotting of the whole variable near the the same variables splitted into groups.\n\n\nExpNumViz(iris, target = \"Species\", Page = c(2,2))\n\n\n$`0`\n\n\nExplore correlations\n{dlookr} - correlation\nIn order to quickly check the relationship between numeric variables we can use correlate() function from {dlookr} package, which delivers correlation coefficients. If we don‚Äôt specify any target variable or the method, Pearson‚Äôs correlation between ALL variables will be calculated pairwisely.\n{dlookr‚Äôs} plot_correlate() function is a bit more useful, because it visualizes these relationships. We can of course determine the method of calculations if we need to, be it a default ‚Äúpearson‚Äù, or a non-parametric ‚Äúkendall‚Äù or ‚Äúspearman‚Äù correlations, which are more appropriate for not-normally or non-very-linearly distributed values with some outliers. The shape of each subplot shows the strength of the correlation, while the color shows the direction, where blue is positive and red is negative correlation. It‚Äôs fine for the pure exploratory analysis, but, as always, the next logical step would be to test hypothesis and figure out which correlations are significant.\n\n\ncorrelate(airquality, Ozone)\n\n\n# A tibble: 5 x 3\n  var1  var2    coef_corr\n  <fct> <fct>       <dbl>\n1 Ozone Solar.R    0.348 \n2 Ozone Wind      -0.602 \n3 Ozone Temp       0.698 \n4 Ozone Month      0.165 \n5 Ozone Day       -0.0132\n\nplot_correlate(airquality, method = \"kendall\")\n\n\n\ndiamonds %>%\n  filter(cut %in% c(\"Premium\", \"Ideal\")) %>% \n  group_by(cut) %>%\n  plot_correlate()\n\n\n\n\n{ggstatsplot}\nAnd that‚Äôs exactly what ggcorrmat() function from {ggstatsplot} package does! Namely, it displays:\ncorrelation coefficients,\na colored heatmap showing positive or negative correlations, and, finally shows\nwhether a particular correlation is significant or not, where not-significant correlations are simply crossed out.\nMoreover, we can get the results in a table form with p-values and confidence intervals for correlation coefficients, if we want to, by simply using output = ‚Äúdataframe‚Äù argument.\n\n\nggcorrmat(data = iris)\n\n\n\nggcorrmat(\n  data   = iris,\n  type   = \"np\",\n  output = \"dataframe\"\n) %>% \n  mutate_if(is.numeric, ~round(., 2)) %>% \n  flextable()\n\n\nparameter1parameter2estimateconf.levelconf.lowconf.highstatisticp.valuemethodn.obsSepal.LengthSepal.Width-0.170.95-0.320.00656,283.260.04Spearman correlation150Sepal.LengthPetal.Length0.880.950.840.9166,429.350.00Spearman correlation150Sepal.LengthPetal.Width0.830.950.780.8893,208.420.00Spearman correlation150Sepal.WidthPetal.Length-0.310.95-0.45-0.15736,637.000.00Spearman correlation150Sepal.WidthPetal.Width-0.290.95-0.43-0.13725,048.130.00Spearman correlation150Petal.LengthPetal.Width0.940.950.910.9535,060.850.00Spearman correlation150\n\nIf any particular correlation catches your attention during the Exploratory Data Analysis, and you want to display it professionally, use the ggscatterstats() function from {ggstatsplot} package, which delivers statistical details, that matter, namely:\nthe statterplot, which helps you to decide to go for a parametric, non-parametric or even robust correlation analysis, if needed,\nthe correlation coefficient itself with the name of the method and 95% confidence intervals,\nthe p-value of the correlation, and it even displays ‚Ä¶\nthe distribution of both numeric variables in different ways, for example densigram which combines a density plot and histogram, as you see on the current plot, or boxplot as you can see in the outcommented line.\nSo, you see, in just a few seconds we went from the exploration of our data to the publication ready plot with a tested hypothesis, namely: growing ozone concentration leads to a significant increase in temperature. Nice, right?\n\n\nggscatterstats(\n  data = airquality,\n  x = Ozone,\n  y = Temp,\n  type = \"np\" # try the \"robust\" correlation too! It might be even better here\n  #, marginal.type = \"boxplot\"\n)\n\n\n\n\n{PerformanceAnalytics}\nAnother effective way to conduct multiple correlation analysis is supported by the chart.Correlation() function from {PerformanceAnalytics} package. It displays not only\ncorrelation coefficients, but also\nhistograms for every particular numeric variable, and\nscatterplots for every combination of numeric variables.\nBesides, significance stars are particularly helpful, because they describe the strength of correlation.\nHere we can of coarse also specify the method, we measure the correlation by.\n\n\nlibrary(PerformanceAnalytics)\nchart.Correlation(iris %>% select(-Species), method = \"kendall\") \n\n\n\n\n{fastStat}\nIf you don‚Äôt care about the distribution and the spread of data, but only need correlation coefficients and p-values, you can use cor_sig_star() function from {fastStat} package.\n\n\nlibrary(fastStat)\niris %>% select_if(is.numeric) %>% cor_sig_star(method = \"kendall\")\n\n\n                    Sepal.Width     Petal.Length      Petal.Width\n1 Sepal.Length -0.118(0.183)         0.872(0)***      0.818(0)***\n2                   Sepal.Width -0.428(0.001)**  -0.366(0.008)** \n3                                   Petal.Length      0.963(0)***\n4                                                     Petal.Width\n\n{dlookr} - linear models\nThe compare_numeric()* function from {dlookr} package examines the relationship between numerical variables with the help of (Pearson‚Äôs) correlation and simple linear models. The correlation results are a little boring because they only provide correlation coefficients (therefore not shown). However, the results of pairwise linear regressions are interesting, because they not only produce p-values, but also other useful metrics, like \\(R^2\\), AIC etc. On top of this, we could plot all the results of compare_numeric() function, which would display:\nthe strength of the correlation with circles,\nthe spread of data with box-plots and\nthe linear regression itself\n\n\nbla <- compare_numeric(iris) \n\nbla$linear %>% \n  mutate_if(is.numeric, ~round(.,2)) %>% \n  flextable()\n\n\nvar1var2r.squaredadj.r.squaredsigmastatisticp.valuedflogLikAICBICdeviancedf.residualnobsSepal.LengthSepal.Width0.010.010.832.070.151-183.00371.99381.02100.76148150Sepal.LengthPetal.Length0.760.760.41468.550.001-77.02160.04169.0724.53148150Sepal.LengthPetal.Width0.670.670.48299.170.001-101.11208.22217.2533.81148150Sepal.WidthPetal.Length0.180.180.4033.280.001-72.57151.13160.1623.11148150Sepal.WidthPetal.Width0.130.130.4122.910.001-76.98159.96169.0024.51148150Petal.LengthPetal.Width0.930.930.481,882.450.001-101.18208.35217.3933.84148150\n\nplot(bla)\n\n\n\n\nExploratory modelling\nHowever, how can we explore whether linear model makes any sense? Well, I think the easiest way is to plot the data with {ggplot2} package and use geom_smooth() function which always fits the data no matter what shape. Such exploration may point out the necessity to use non-linear models, like GAM or LOESS:\n\n\nggplot(airquality, aes(Solar.R, Temp))+\n  geom_point()+\n  geom_smooth()+\n  facet_wrap(~Month)\n\n\n\n\nExplore missing values\nI found this topic actually so cool, that I produced a small separate video of 7 minutes (and article):\n\n\n\n\n\n\n\n{dlookr}\nHere I‚Äôll just give you two useful functions from {dlookr} package. The first one - plot_na_intersect() shows you which variables have missing values and how many. And the second function imputate_na() imputes missing values with different machine learning methods. For instance, using ‚ÄúK nearest neighbors‚Äù algorithm, we could impute 37 missing values in Ozone variable, and even visually check the quality of our imputation in only one line of code. Using the imputate_na() function, we only need to specify 4 arguments:\nthe dataset\nthe variable with missing values, that would Ozone\nthe variable which will predict the missing values, for example Temperature and\nthe imputation method\n\n\nplot_na_intersect(airquality)\n\n\n\nplot(imputate_na(airquality, Ozone, Temp, method = \"knn\"))\n\n\n\n\nExplore outliers\n{performance}\ncheck_outliers() function from {performance} package provides an easy way to identify and visualize outliers with different methods. If you want to have an aggressive method and clean out a lot of outliers, go with the zscore method, but if you don‚Äôt have much data, go with less conservative method, for example interquartile range.\n\n\nlibrary(performance)\nplot(check_outliers(airquality$Wind, method = \"zscore\"))\n\n\n\ncheck_outliers(airquality$Wind, method = \"iqr\")\n\n\nWarning: 3 outliers detected (cases 9, 18, 48).\n\n{dlookr}\ndiagnose_outlier() function from {dlookr} not only counts outliers in every variable using interquartile range method, but also gets their percentages. Moreover, it calculates three different averages: the mean of every variable with outliers, without outliers and the mean of the outliers themselves. In this way we can see how strong the influence of outliers for every variable actually is. For instance the variable ‚Äúdepth‚Äù in ‚Äúdiamonds‚Äù data has over 2500 outliers. That‚Äôs a lot! However, the means with and without outliers are almost identical. Besides, the average of the outliers themselves is very similar to the original average of the whole data. In contrast, the variable ‚Äúprice‚Äù with over 3500 outliers is heavily influenced by them. The average of the outliers is almost 5 times higher, than the average without them.\n\n\ndiagnose_outlier(diamonds) %>% flextable()\n\n\nvariablesoutliers_cntoutliers_ratiooutliers_meanwith_meanwithout_meancarat1,8893.502039302.1536840.79793970.748738depth2,5454.7182054161.20479461.749404961.776373table6051.1216166164.84297557.457183957.373404price3,5386.5591397814,944.7764273,932.79972193,159.807111x320.059325187.2393755.73115725.730262y290.053763449.7734485.73452605.732353z490.090841684.0546943.53873383.538265\n\nBesides, {dlookr} can visualize the distribution of data with and without outliers, and, thank to collaboration with {dplyr}, we could choose to visualize only variables with over 5% of values being outliers:\n\n\nairquality %>% \n  dplyr::select(Ozone, Wind) %>% \n  plot_outlier()\n\n\n\n# Visualize variables with a ratio of outliers greater than 5%\ndiamonds %>%\n  plot_outlier(diamonds %>%\n      diagnose_outlier() %>%\n      filter(outliers_ratio > 5) %>%\n      select(variables) %>%\n      pull())\n\n\n\n\nImpute outliers\nSimilarly to imputate_na() function, {dlookr} package provides the imputate_outlier() function too, which allows us to impute outliers with several methods: mean, median, mode and cupping. The last one, ‚Äúcapping‚Äù, is the fanciest, and it imputes the upper outliers with 95th percentile, and the bottom outliers with 5th percentile. Wrapping a simple plot() command around our result, would give us the opportunity to check the quality of imputation.\n\n\nbla <- imputate_outlier(diamonds, carat, method = \"capping\")\nplot(bla)\n\n\n\n# summary(bla)  # if needed\n\n\n\nConclusion\nSo, the usual Exploratory Data Analysis allows us to have a look at the data and form the hypothesis. In this post we not only did that, but also went one step further and started to explore (or test) the hypotheses themselves with simple statistical tests. This can help us to decide which complex statistics we need to use next, in order to produce the final result, for example inference and predictions from a multivariate model, which would be a completely new story‚Ä¶\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings and references\nhere is an amazing paper, which describes the most used EDA packages in R: https://www.groundai.com/project/the-landscape-of-r-packages-for-automated-exploratory-data-analysis/1\n\n\n\n",
    "preview": "posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress/DEDA_thumbnail.jpg",
    "last_modified": "2021-06-04T17:13:07+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-04-how-to-impute-missing-values-in-r/",
    "title": "How to impute missing values with Machine Learning in R",
    "description": "Imputation simply means - replacing a missing value with a value that makes sense. But how can we get such values? Well, we'll use Machine Learning algorithms, because they have a high prediction power. So, in this post we'll learn how to impute missing values easily and effectively.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yuzar-blog.netlify.app/"
      }
    ],
    "date": "2021-01-09",
    "categories": [
      "videos",
      "data wrangling",
      "visualization",
      "machine learning"
    ],
    "contents": "\n\nContents\nThis post as a video\nVisualize missing values\ndlookr\nvisdat\n\nUnivariate\nmissing value imputation by {dlookr}\nUnivariate\nimputation of numeric variables\nUnivariate\nimputation of categorical variables\n\nMultivariate\nmissing value imputation by {missRanger}\nMultivariate\nimputation of numeric variables\nMultivariate\nimputation of categorical variables\n\nUseful ressources\n\n\n\nknitr::opts_chunk$set(echo = T, warning=F, message=F)\nlibrary(grid)\n\n\n\nThis post as a video\nIf you are more of a visual learner, your can watch this post.\n\n\n\n\n\n\n\nYou can load all the packages at once to avoid interruptions.\n\n\nlibrary(dlookr)      # for exploratory data analysis and imputation\nlibrary(visdat)      # for visualizing NAs\nlibrary(plotly)      # for interactive visualization\nlibrary(missRanger)  # to generate and impute NAs\nlibrary(tidyverse)   # for almost everything ;) it's a must!\n\n\n\nVisualize missing values\ndlookr\nFirst of all we have to make sure we have missing values in our\ndataset. Using plot_na_pareto() function from {dlookr} package\nwe can produce a Pareto chart, which shows\ncounts and proportions of missing values in every\nvariable. It even tells you what the amount of missing values\nmeans, namely, missing around 24% of observations is\nstill OK, while missing more then 40% of values would\nbe bad. If you have a lot of variables and wanna\ndisplay only the ones with missing values, use only_na =\nTRUE argument. The only problem with Pareto\nchart is that we don‚Äôt know whether missing values in different\ncolumns belong to the same observation.\n\n\nlibrary(dlookr)      # for exploratory data analysis and imputation\nplot_na_pareto(airquality, only_na = TRUE)\n\n\n\n\nplot_na_intersect() function form {dlookr} package\nvisualizes the combinations of missing values across\ncolumns. The x-axis shows the variables with\nmissing values, while the counts of missing values are shown on the top\nof the plot as bars. The y-axis represents the\ncombination of variables and their frequencies. For\ninstance, we see, that two of the observations are missing in both\nvariables, Ozone and Solar.\n\n\nplot_na_intersect(airquality)\n\n\n\n\nThe only thing here is that we don‚Äôt exactly know which two\nobservation are the same. To get this problem solved we can use\nvis_miss() function from {visdat} package and wrap it up into\nggplotly() command to get an interactive plot, which\nwill show us the information of any point on which we hover the cursor.\nWe can even zoom in a little bit if want to ;)\nvisdat\n\n\nlibrary(visdat)      # for visualizing NAs\nlibrary(plotly)      # for interactive visualization\nvis_miss(airquality) %>% ggplotly()\n\n\n\n\nNow, since we know we have missing values in two variables, we can\nimpute them in every particular variable separately using\n{dlookr} package, or, impute missing values in the whole dataset at\nthe same time with the {missRanger} package. Let‚Äôs do both and\ncheck the quality of our imputation by visualizing imputed values.\nUnivariate\nmissing value imputation by {dlookr}\nUnivariate\nimputation of numeric variables\nDlookr package provides numerous methods for imputation.\nHowever, here I would only present two of them, which are particularly\nuseful, because they can be applied for both numeric and categorical\nvariables. The first method is rpart, or Recursive\nPartitioning and Regression Trees, and the second is\nmice, or Multivariate Imputation by Chained\nEquations.\nLet‚Äôs use both of them, and the imputation by the mean, just for sake\nof comparison, to see which method does the best job. And to make this\neven harder let‚Äôs produce more missing values in our dataset and make\nsure that at least one variable is categorical.\nNow every variable in our dataset misses at least 10% of\nobservations. Using the imputate_na() function, we only need to\nspecify 4 arguments:\nthe dataset\nthe variable with missing values, that would Ozone\nthe variable which will predict the missing values, for example\nTemperature and\nthe imputation method\n\n\nlibrary(grid)\n# produce more NAs with missRanger package\nlibrary(missRanger)\nset.seed(111)\nairquality_NA <- generateNA(airquality) %>% \n  mutate(Month = factor(Month))\n\n# check out NAs\nplot_na_pareto(airquality_NA)\n\n\n\nplot_na_intersect(airquality_NA)\n\n\n\nblip <- imputate_na(airquality_NA, Ozone, Temp, method = \"mean\")\nplot(blip)\n\n\n\nbla <- imputate_na(airquality_NA, Ozone, Temp, method = \"rpart\")\nsummary(bla)\n\n\n* Impute missing values based on Recursive Partitioning and Regression Trees\n - method : rpart\n\n* Information of Imputation (before vs after)\n                    Original   Imputation\ndescribed_variables \"value\"    \"value\"   \nn                   \"106\"      \"153\"     \nna                  \"47\"       \" 0\"      \nmean                \"42.80189\" \"40.95070\"\nsd                  \"33.74228\" \"31.46152\"\nse_mean             \"3.277340\" \"2.543513\"\nIQR                 \"48.00000\" \"45.33333\"\nskewness            \"1.182420\" \"1.249498\"\nkurtosis            \"1.127915\" \"1.255872\"\np00                 \"1\"        \"1\"       \np01                 \"4.10\"     \"5.04\"    \np05                 \"7.25\"     \"9.00\"    \np10                 \"10.5\"     \"12.2\"    \np20                 \"14.00000\" \"17.66667\"\np25                 \"16.00000\" \"17.66667\"\np30                 \"18.50000\" \"17.66667\"\np40                 \"23\"       \"23\"      \np50                 \"32.00000\" \"31.71429\"\np60                 \"41.0\"     \"34.2\"    \np70                 \"55.5\"     \"46.4\"    \np75                 \"64\"       \"63\"      \np80                 \"73.0\"     \"74.8\"    \np90                 \"87.00000\" \"83.44444\"\np95                 \"109.5\"    \" 96.4\"   \np99                 \"134.35\"   \"128.24\"  \np100                \"168\"      \"168\"     \n\nplot(bla)\n\n\n\n\n\n\nblup <- imputate_na(airquality_NA, Ozone, Temp, method = \"mice\", seed = 111)\n\n\n\n\n\nplot(blup)\n\n\n\n\nComparing several methods would gives us a choice to use the\nmethod which does not change the distribution too much or too\nweirdly. For example, imputation by a simple average shown\nhere is a strange one; it produces a bump in the middle of the plot\nwhich simply does not make any sense. In contrast, both machine learning\nmethods did a great job, although I‚Äôd prefer to use mice\ninstead of rpart for this particular dataset.\nUnivariate\nimputation of categorical variables\nPredicting a categorical variable, Month, is similarly\neasy.\n\n\nblah <- imputate_na(airquality_NA, Month, Temp, method = \"mice\", seed = 111)\n\n\n\n\n\nplot(blah)\n\n\n\n\nMultivariate\nmissing value imputation by {missRanger}\nMultivariate\nimputation of numeric variables\nSo, the dlookr package is sweet for a quick\nand controllable imputation! And I do talk about it more in the separate\ndlookr article.\nBut we can go one step further using the missRanger package,\nwhich is a real heavyweight in data imputation!\nThe missRanger approach, is a non-parametric\nmultivariate imputation by the chained random forest (Mayer 2019). Compared to dlookr\napproach, which used only one variable with missing values,\nmissRanger predicts multiple variables at the same time by\nusing all other variables in a dataset as predictors. This method\ncombines the random forest imputation, which is cool on\nit‚Äôs own, with the mice method (Multivariate\nImputation by Chained Equations) using the predictive mean\nmatching which we just saw in action. So, {missRanger} iterates\nimputation for every missing value multiple times until the average\n(out-of-bag) prediction error stops to improve. ‚Ä¶you know‚Ä¶machine\nlearning stuff‚Ä¶ This allows for a realistic imputation which avoids ‚Äúnew\nstrange‚Äù values like 0.3 in a 0-1 coded variable and thus contains the\noriginal data structure. On top of this, {missRanger} is much quicker\nthen {dlookr}, because it‚Äôs written in C++!, and thus, can be used for a\nbid data.\nSo, let‚Äôs impute all the missing values in our dataset at\nonce. For this we just need to specify our dataset and provide the\nformula. The formula is interesting: a point on the left side of the\ntilde will find all the variables with missing and fill them\nup, and the point on the right side of the tilde will use all the\npredictors in a dataset for imputation, even those with missing\nvalues ;). How amazing is that!?\nSo, if we then plot the imputed dataset in red, and the original in\nblack, we‚Äôll not only see that missRanger did not produce any\nweird values outside of the original distribution, but also that most of\nthe new red points are placed in the area where the most of the old\nblack points are. So, in a few lines of code and a few seconds we filled\nup all variables, numeric and categorical, without skewing our data.\nBesides, the arguments verbose = 2 and\nreturnOOB = T show you the OOB (out of bag error) for every\nvariable and every iteration - so we can see how the result improves!\nThat‚Äôs just madness!\n\n\nairquality_imputet <- missRanger(\n  airquality_NA, \n  formula = . ~ . ,\n  num.trees = 1000, \n  verbose = 2, \n  seed = 111,  \n  returnOOB = T)\n\n\n\nMissing value imputation by random forests\n\n  Variables to impute:      Ozone, Solar.R, Wind, Temp, Month, Day\n  Variables used to impute: Ozone, Solar.R, Wind, Temp, Month, Day\n    Wind    Temp    Month   Day Solr.R  Ozone\niter 1: 1.0000  0.9711  0.6812  1.0257  0.9786  0.4319  \niter 2: 0.1009  0.0311  0.0362  0.0712  0.0752  0.1230  \niter 3: 0.1062  0.0303  0.0290  0.0674  0.0707  0.1231  \niter 4: 0.1074  0.0308  0.0290  0.0709  0.0704  0.1144  \niter 5: 0.1092  0.0347  0.0290  0.0733  0.0720  0.1023  \niter 6: 0.1053  0.0313  0.0362  0.0702  0.0701  0.0912  \niter 7: 0.0974  0.0319  0.0362  0.0744  0.0775  0.0891  \n\n# numeric imputation\nggplot()+\n  geom_point(data = airquality_imputet, aes(Ozone, Solar.R), \n             color = \"red\")+\n  geom_point(data = airquality_NA, aes(Ozone, Solar.R))+\n  theme_minimal()\n\n\n\nggplot()+\n  geom_point(data = airquality_imputet, aes(Wind, Temp), \n             color = \"red\")+\n  geom_point(data = airquality_NA, aes(Wind, Temp))+\n  theme_minimal()\n\n\n\n\nMultivariate\nimputation of categorical variables\n\n\n# caterogical imputation\nggplot()+\n  geom_bar(data = airquality_NA, aes(Month), width = 0.3)+\n  geom_bar(data = airquality_imputet, aes(Month), fill = \"red\",\n           position = position_nudge(x = 0.25), width = 0.3)+\n  \n  theme_minimal()\n\n\n\n\nThat‚Äôs it for today. If you found this useful and want to see more,\nfeel free to subscribe to my YouTube channel.\nThanks for learning ;) \nIf you think, I missed something, please comment on it below, and\nI‚Äôll improve this tutorial.\nUseful ressources\nChoonghyun Ryu (2021). dlookr: Tools for Data Diagnosis,\nExploration, Transformation. R package version 0.4.0. https://CRAN.R-project.org/package=dlookr\nMichael Mayer (2019). missRanger: Fast Imputation of Missing\nValues. R package version 2.1.0. https://CRAN.R-project.org/package=missRanger\nWright, M. N. & Ziegler, A. (2016). ranger: A Fast\nImplementation of Random Forests for High Dimensional Data in C++ and R.\nJournal of Statistical Software, in press. http://arxiv.org/abs/1508.04409.\nStekhoven, D.J. and Buehlmann, P. (2012). ‚ÄòMissForest -\nnonparametric missing value imputation for mixed-type data‚Äô,\nBioinformatics, 28(1) 2012, 112-118. https://doi.org/10.1093/bioinformatics/btr597.\nVan Buuren, S., Groothuis-Oudshoorn, K. (2011). mice:\nMultivariate Imputation by Chained Equations in R. Journal of\nStatistical Software, 45(3), 1-67. http://www.jstatsoft.org/v45/i03/\n\n\n\nMayer, Michael. 2019. ‚ÄúmissRanger: Fast\nImputation of Missing Values.‚Äù https://cran.r-project.org/package=missRanger.\n\n\n\n\n",
    "preview": "posts/2021-03-04-how-to-impute-missing-values-in-r/thumbnail_missing_values.png",
    "last_modified": "2022-05-11T18:11:05+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/",
    "title": "Null Hypothesis, Alternative Hypothesis and Hypothesis Testing",
    "description": "Hypothesis testing is one of the most important concepts in (frequentiest) statistics and science. However, most people who test hypotheses are scientists, but not statisticians. That's why scientists often do not test hypotheses properly, without any bad intension—Å. So, in this blog-post we'll break down hypothesis testing in small parts and try to properly understand every of them.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-09",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nThe essence of Hypothesis Testing\nWhy do we actually need hypothesis testing?\nNull Hypothesis and Alternative Hypothesis\n3 theoretical examples\nSummary for \\(H_0\\) and \\(H_A\\)\n\nNever accept null hypothesis! Only try to refect!\nHow far is far enough?\nStop trying to learn something!\nWhat‚Äôs next?\n\nThis post as a video\n\n\n\n\n\n\n\nThe essence of Hypothesis Testing\nHypothesis testing can be summarized in only 5 points:\nCollect data\nClearly define Null (\\(H_0\\)) and Alternate (\\(H_A\\)) Hypotheses. That‚Äôs the most important point!\nGet p-value through something called a statistical test. Every modern software will calculate it for you.\nDefine your rejection threshold, e.g.¬†p-value < 0.05 (or <5%) and finally\nMake a conclusion, e.g.¬†if p = 0.03, we reject the \\(H_0\\) in favor of \\(H_A\\)\nThat‚Äôs actually enough to get you started with Hypothesis Testing. But if you want to know what exactly Null (\\(H_0\\)) and Alternate (\\(H_A\\)) Hypotheses are, why we only reject the null but never accept it, what is p-value or why do we need hypothesis testing at all, continue reading. So‚Ä¶\nWhy do we actually need hypothesis testing?\nThe short answer: TO MAKE SENSE OF THE DATA. The long answer ‚Ä¶ where should I start?\nI‚Äôll start with the first time I have got my own data. I was super motivated, because this data seemed to be a golden ticket to writing a thesis and finally getting a degree. The only thing I needed to do is to analyze this data. I couldn‚Äôt wait to start working, so I opened my Excel table and started to look at it. However, the more I looked at it, the more puzzled I became. How the heck does one actually analyzes data? I asked my boss, and she said:\n‚ÄúJust test some hypothesis.‚Äù\n‚ÄúCool! Thanks! That helps!‚Äù - I said and slowly started to panic üò±, because I had no idea how hypothesis testing works. So, I had no choice but to figure it out! What a pain!\nHowever, after a bit of research I realized that the hypothesis testing is not a pain at all, but is actually a huge help for analyzing data! And here is why. Having a data - we want to make a claim or a statement. For example: sport reduces weight. This claim IS our hypothesis. Having several claims creates a compelling story for a thesis or a scientific paper. There is only one problem with that: why should anyone believe our story? This could be a science fiction story. Thus, in order to make it a real science, not science fiction, we need to make it believable and solid. How? Well, we test every of our hypothesis against the null hypothesis. Wait, what the hell is the null hypothesis?\nNull Hypothesis and Alternative Hypothesis\nThe null hypothesis is ‚Ä¶ absolutely, nothing! Empty, boring, zero, null!. For instance:\nwhen nothing happens,\nwhen there is no effect,\nwhen there is no difference,\nwhen nothing new was learned\nFor instance, if our research hypothesis is - sport reduces weight, then our null hypothesis is - sport DOES NOT reduce weight. So, we actually need ONLY these TWO hypothesis to make any of our claims solid! And since our research hypothesis is the only Alternative to the null hypothesis, it is often called the Alternative Hypothesis. Let‚Äôs have 3 examples and clearly define both Null and Alternative hypotheses.\n3 theoretical examples\nif you measure the effect of sports on muscles:\nmuscle gain (or measurable effect) is your research or alternative hypothesis \\(H_A\\)\nNO muscle gain (NO effect) is your null hypothesis \\(H_0\\)\n\nif you don‚Äôt believe previous studies, or accepted value, for example that the average weight loss from a fancy diet is 3 kilos per week, you make an experiment in order to test this accepted value. Then:\naverage weight loss ‚â† 3 kg/week is our \\(H_A\\)\naverage weight loss = 3 kg/week is our \\(H_0\\)\n\nif you study any difference between anything (groups, treatments etc.):\nNO difference is our \\(H_0\\)\nTHERE IS a difference is our \\(H_A\\)\n\nNO difference expressed mathematically means - the difference is equal to zero. Writing this null hypothesis down in a simple formula makes it even easier to understand:\n\\[ average \\ 1 - average \\ 2 = 0\\]\nand if we add \\(average \\ 2\\) to both sides of the formula, we‚Äôll get:\n\\[ average \\ 1 = average \\ 2\\] ‚Ä¶which, again, means that if the difference between samples \\(= 0\\), than the averages of both groups are the same.\nFor the \\(H_A\\) it would be:\n\\[ average \\ 1 - average \\ 2 ‚â† 0\\] \\[ average \\ 1 ‚â† average \\ 2\\]\n‚Ä¶which is saying that samples (or their averages) differ. So, the \\(H_0\\) and \\(H_A\\) are always mathematical opposites! Which makes the hypothesis testing really simple:\nSummary for \\(H_0\\) and \\(H_A\\)\nin order to answer a question or make a solid claim from our data we only need two hypotheses, \\(H_0\\) and its only alternative - \\(H_A\\)\nyour question (claim or statement) is in fact your \\(H_A\\)\nthe best part of the \\(H_A\\) is that we can create tons of \\(H_A\\), because we can ask thousands of different questions, or make thousands of different claims.\nbut we always have only one \\(H_0\\)\nthe beauty of the \\(H_0\\) is that it is always the mathematical opposite of the \\(H_A\\), doesn‚Äôt matter what the alternative is. The \\(H_A\\) is always what \\(H_0\\) is NOT\nand the best part of the \\(H_0\\) is that we do not need any preliminary data for it. It‚Äôs always there, e.g.¬†NO difference or NO effect\nOk, we just learned about what hypotheses are, but how do we test them? And what‚Äôs the point of testing anyway? Well‚Ä¶\nThe only goal of hypothesis testing is to (1) reject or (2) fail to reject the null hypothesis. There are really only these two possible outcomes. We can never accept the null hypothesis. Why? Let‚Äôs figure out on some examples?\nNever accept null hypothesis! Only try to refect!\nImagine you travel to a completely new country for hiking. You discover a beautiful lake and start to fish. After 3 hours of fishing you didn‚Äôt catch anything and you even failed to have a single bite. Can you accept the null hypothesis that there is NO fish in that lake? Of course not! You only failed to reject it during 3 hours. May be you just need to fish a little longer. So, you fish for another hour and finally catch your first fish. In that case the null hypothesis that there is NO fish in that lake is destroyed by the ‚Äúfishy‚Äù evidence. That‚Äôs why we cannot accept the null hypothesis until we reject it; we can only fail to reject it.\nNow imagine you explore life on an alien planet. Your null hypothesis (\\(H_0\\)) is that there is NO life on this planet. You drive around a few hours and look for life. If you see any alien, you can happily reject the null hypothesis in favor of the alternative, because further believing the null hypotheses that there is NO life on the planet after you just saw one - seems ridiculous. But if you don‚Äôt see any aliens, can you definitively say that there is no alien life on this planet, or accept the null hypotheses? Again, no! Because if we would have explored longer we might have found an alien life. And since we are not sure about the absence of life on this new planet we cannot accept the null hypothesis; we can only fail to reject it.\nThe last intuitive example comes from trial courts. If you are accused of a crime, you presumed to be NOT-guilty. So, the null hypothesis is that - you are NOT guilty. The court tries to reject it. If there is a clear evidence against you, like a videotape, it would be ridiculous to still believe the null hypothesis, that you are NOT guilty, because the evidence is recorded, so, the judges can reject the null hypothesis that you are NOT guilty in favor of the alternative hypothesis that you are actually guilty. But if there is not enough evidence against you, for example no tape or no fingerprints, the judges cannot say you are guilty. But they also can NOT say you are NOT guilty! Imagine, you did actually commit a crime. But nobody can prove it, because there is absolutely no evidence. May be if Sherlock Holmes looked for evidence, he would have found some. But until then, judges can not accept that - you are NOT guilty, they only can fail to reject the presumption that - you are NOT guilty. In other words, judges can‚Äôt say you are innocent, but rather you are STILL NOT guilty.\nEverything is clear in the case of aliens on a new planet or fish in the lake. If you found any evidence against the null hypothesis, you simply reject it. But comparing averages of two groups is a bit more complex? Remember our very first hypothesis - sport reduces weight? Let‚Äôs test that one.\nHow far is far enough?\nWe first collect some data from people who did not exercise for one year. Some people in this group gained a bit of weight, because they eat a lot, while some people lost a bit of weight, because they were sick or on a diet. Despite this mild random variable ON AVERAGE the weight change in this group was zero. And that‚Äôs our Null Hypothesis - NO change in weight. We‚Äôll call this group - a control group.\nThen we‚Äôll find another group or people who has exercised for one year and measure their weight before and after one year of exercise. Well, imagine we have an average loss in weight of only 1 kilo. It‚Äôs not 0 change, but it‚Äôs too close to 0 for being taken seriously. It could be just due to the same random variation we have had in the control group. Then, imagine we sampled a different group where people lost 3 kilos on average. Here I am starting to feel uncomfortable to think that the change is close to 0, and I start to think whether I should reject the null hypothesis. Finally, the last group of people who exercised for one year lost 10 kilos on average. Now, it‚Äôs soo far away from 0 that the null hypothesis - that there is NO change in weight seems ridiculous. Therefore we‚Äôll reject it in favor of the alternative hypothesis - sport reduces weight. Interestingly, for the weight loss of 10 kilograms everyone would agree that sport works, while for a loss of only 1 kilo people would agree that sport does not work. But for 3 kilos some people would say yes while some would say no. And all their opinions would be highly subjective and not concrete, making us not confident about our conclusion any more. Well, hypothesis testing offers a concrete way to decide when we reject and when we fail to reject the null hypothesis. And that is where p-value comes into play and help us solve this problem numerically.\n\nThe p stands for the probability. And it refers to the probability that we would have gotten the results we did just by chance.\n\nIn order to understand it better, let‚Äôs test the last hypothesis (comming from on an amazing YouTuber - MrNystrom):\nIf you throw a fair coin, you have only 2 possible outcomes: tales or heads. The chances to get tails are 50%. Clear and boring result. The chances to get 2 tails in a row are 25%, because we have 4 possible outcomes: 1/4 = 25%. It‚Äôs pretty likely to get 2 tails in a row. So, nothing unusual here. The coin must be fair. However, 3 tails in a row starts to feel strange. The chances to get 3 tails in a row are low, 1/8 = 12.5%, but still possible. But the chances to get 4 tails in a row are only 6.25%. And if we get 4 tails in a row we start to doubt whether the coin is actually fair. And if you get 6 tails in a row despite only 1.5% chances (probability) to get them, the null hypothesis that this is a fair coin would seem ridiculous and you‚Äôll reject it, because it‚Äôs simply too unlikely to happen randomly (or by chance).\nA widely accepted, but by no means the best, cut off for a p-value is 5%. That means, that if your p-value (your probability that would have gotten the results you did just by chance) is below 0.05, you can reject the null hypothesis, while if a p-value is above 0.05, you fail to reject the null. So, p-values provide concrete boundaries for making a decision about hypothesis testing and are therefore useful. However, p-values are one of the most misunderstood and misused concepts in statistics. Thus, p-values deserve a separate video. Here I would like to point out only one, but the most frequent misuse of p-values, which, if eliminated could dramatically increase the quality of science.\nResearchers always want to reject the \\(H_0\\) in favour of the research hypothesis, because that would mean - that they found something interesting or new. But if the \\(H_0\\) can not be rejected, it seems like a tragedy and failure. All the efforts of making experiments, collecting and analyzing data seem useless and there is nothing to be published. However, it‚Äôs not true! We always can say - we learned nothing new.\nThe goal of science is NOT to find an effect or a difference, but to figure out whether there is an effect or difference. If there is NO effect, it is a perfectly valid and equally important result. But some scientists insist on finding something significant and continue looking for it till they found it, a phenomenon known as - p-hacking. Well, p-hacking also deserves a separate video. Until then I‚Äôd love to finish with the quote of Cassie Kozyrkov:\nStop trying to learn something!\n\n‚ÄúYou should get into the habit of learning nothing more often, because if you insist on learning something beyond the data every time you test hypotheses, you will learn something stupid.‚Äù\n\nWhat‚Äôs next?\nYou should definitely get an intuitive understanding of p-values!\nReferences:\nhttps://www.youtube.com/watch?v=VK-rnA3-41c&t=1s&ab_channel=MathandScience\nhttps://www.youtube.com/watch?v=0oc49DyA3hU&ab_channel=StatQuestwithJoshStarmer\nhttps://medium.com/hackernoon/statistical-inference-in-one-sentence-33a4683a6424\nhttps://medium.com/@kozyrkov?source=post_page-----33a4683a6424-------------------\nhttps://www.youtube.com/watch?v=-MKT3yLDkqk\nhttps://www.youtube.com/watch?v=5koKb5B_YWo&ab_channel=StatQuestwithJoshStarmer\nhttps://www.youtube.com/watch?v=zR2QLacylqQ&ab_channel=zedstatistics\nhttps://www.youtube.com/watch?v=tLM7xS6t4FE&ab_channel=SciShow\n\n\n\n",
    "preview": "posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/thumbnail.jpg",
    "last_modified": "2021-07-31T11:46:43+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-07-31-p-value-intuitive-explanation/",
    "title": "What is p-value and why we need it",
    "description": "Why do we need p-values? Well, they help to **make decisions** and **answer the question whether we found something new or not**. But despite the fact that **p-values are** actually **useful**, they are **far from perfect**! And while everyone uses p-values, understanding them (and using them correctly) is very hard. The definition of the p-value from the book is often correct but rarely intuitive. Intuitive explanations are often not entirely correct. So, in this blog-post (and video) we‚Äôll start with an intuitive (and not entirely correct) definition and will gradually build up the understanding of the p-value step by step. Thus, I don‚Äôt recommend to skip any part of this blog (or video). We‚Äôll also talk about how to use and interpret p-values correctly in order to **make better decisions and better science**.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-09",
    "categories": [
      "videos",
      "statistics"
    ],
    "contents": "\n\nContents\nThis post as a video\nPrevious topics\nP-value definition N¬∞1: it‚Äôs a probability\nP-value definition N¬∞2: three probabilities (learned from Josh Starmer!)\nP-value definition N¬∞3: cumulative probability (area under the curve)\nP-value definition N¬∞4: original Fisher‚Äôs definition\nDon‚Äôt follow any cut-off, the cut-off should follow you!\nA single article you need to read: ‚ÄúMoving to a World Beyond ‚Äúp<0.05‚Äù \"\nWhat to do:\nWhat not to do:\n\nWhat about Bayesian statistical inference?\nConclusion\nWhat‚Äôs next to learn?\nFurther readings and watchings\n\nThis post as a video\n\n\n\n\n\n\n\nPrevious topics\nSince p-values are mostly used for testing hypothesis, you should definitely check out the previous post - hypothesis testing, where I superficially introduced the p-value already.\nP-value definition N¬∞1: it‚Äôs a probability\nThe most intuitive explanation of p-values I have found came from Andrew Vickers‚Äô book ‚ÄúWhat is a p-value anyway? 34 Stories to Help You Actually Understand Statistics‚Äù: ‚ÄúP-value is the probability of the toothbrush being dry if you‚Äôve just cleaned your teeth.‚Äù Let‚Äôs start with that.\nThe P stands for probability. And it refers to the probability to observe the results we did just by chance. In order to understand this definition a little better, let‚Äôs have an example:\n\nIf we throw a fair coin, we have only 2 possible outcomes: heads or tales. The chances to get tails are obviously 50%. Then we throw our coin a second time. The chances to get 2 tails in a row are 25%, because we have 4 possible outcomes after two throws, with only one of those four outcomes having two tails in a row, so 1/4 = 0.25 = 25%. It‚Äôs actually pretty likely to get 2 tails in a row. So, nothing unusual here. The coin must be fair. But 3 tails in a row starts to feel strange. The chances to get 3 tails in a row are low, only 12.5%, but still possible. However, if we get 3 tails in a row we start to doubt whether the coin is actually fair. And if we get 6 tails in a row despite only 1.5% probability to get them, the null hypothesis - that this is a fair coin - would seem ridiculous and we‚Äôll reject it, because it‚Äôs simply too unlikely to happen randomly (or by chance).\n\nThis example, which defines the p-value as ONE particular probability of having only tales, is clear, intuitive, but only ONE sided. It isn‚Äôt wrong, but ONE-sided p-values coming from only ONE probability is not what people use. The ‚Äúreal p-values‚Äù are TWO sided. Then why did we learned about one-sided p-values at all, you might ask??? Well, it was totally necessary for a better understanding of the second definition of p-values. And this is actually one of those definitions from the book, where people start to run away from statistics :) namely:\n\nP.S.: One-sided p-values are rare and dangerous, so, please don‚Äôt use them to avoid confusion, especially if you just started to learn about p-values.\n\nP-value definition N¬∞2: three probabilities (learned from Josh Starmer!)\nP-value is the probability to get (1) the data we have collected, (2) as extreme or (3) more extreme data just by chance, assuming our null hypothesis is true.\nTwo tailed p-values are determined by adding up three probabilities.\n\nRemember our coin example? The probability of having two tails in a row is 25% (or 0.25), because there are 4 equally possible outcomes after flipping a coin 2 times. These 4 possible outcomes is our distribution. And the data we have got - two tails - is only one side of this distribution.\nLikewise, the probability of getting two heads is also 0.25, and since two heads are as extreme as two tales, we have to add the probability of two tails to the probability of two heads: 1/4 + 1/4 = 2/4 = 0.5. Now we have a two sided probability but the p-values still need one last part.\nThe third and the last part of the p-value is - the probability of observing something rarer or more extreme. In this case it is 0, because no other outcomes are rarer then two tails or two heads. Thus, if we add this 0 to the probability of having either two tales or two heads, we‚Äôll get a final p-value of 0.5. And since our p-value is way above the significance threshold of 0.05, we failed to reject the null hypothesis, so that our null hypothesis is still TRUE - our coin must be fare.\nThus: the p-value is the probability of getting the (1) data we collected (two tails = 0.25), as extreme (two heads = 0.25) or more extreme data (0) is: 0.25 + 0.25 + 0 = 0.5.\nNow let‚Äôs calculate the p-value for a slightly more complicated outcome: 3 heads and 1 tail. This is one of the 16 possible outcomes, when we flip the coin 4 times. The null hypothesis would be that our data - 3 heads and 1 tail - is nothing special, but belongs to this exact distribution.\n\nThe probability we randomly get 3 heads and 1 tail is 4/16 = 0.25. That would be our data and with that the first part of the p-value.\nThe probability that we randomly get something equally rare, namely 3 tails and 1 head, is also 4/16 = 0.25, which will be the second part of the p-values.\nThe probability that we randomly get something rarer or more extreme, which in our case are 4 heads and 4 tails is 2/16 = 0.125, and are the last part of our p-value.\nAdding them all together would result into a p-value of 0.625, so we would fail to reject the null hypothesis - that our data (3 heads and 1 tail) is nothing special, and would need to conclude that 3 heads and 1 tail belong to this distribution.\nP-value definition N¬∞3: cumulative probability (area under the curve)\nCalculating a p-value from discrete numbers is kind of easy, but how do we do it for continuous numbers? Plotting our discrete data will help to understand that. If we plot our values from the last example, we would find the most probable outcomes (green) in the middle, a bit less probable (black and blue) on both sides of the middle, and finally the least probable values (red) very far away from the middle. This will become the probability distribution of our discrete values. We can also cover plotted values with a curve to better visualize this distribution. Now, if we mark our discrete values with discrete numbers on the x-axis, we‚Äôll see that we‚Äôll be able to add any continuous value in between those discrete numbers, e.g.¬†0.3 or 0.333. And for every of these continuous value we can calculate probability in the exact same way as we just did for 2 tails, or 3 heads and 1 tail. That‚Äôs how we arrive at the continuous probability. The p-values for such continuous probability are also calculated in the same way. But instead of adding up only 3 discrete probabilities (our data, as extreme and more extreme data), we add up 2 discrete parts (our data and similarly extreme data) and 1 continuous part (more extreme data from both tails of the distribution):\nand since we calculate probabilities of many points under the curve and\nadd all of them together, or ACCUMULATE those probabilities under the curve,\nwe‚Äôll get the AREA under the curve - which then IS our p-value as a cumulative probability, including all the values which are as extreme or more extreme than our data.\n\nAn example of such curve could be the weight of people. For instance, the average weight of German males is 80 kilos with 95% of them weighting between 70 and 90 kilos (these numbers are totally made up ;). 2.5% of them will weight <70 and 2.5% will weight >90 kilos. If we get a new data point - namely a weight of a new person, we can calculate the p-value for that point. Our null hypothesis is - that the persons weight is NOT different from our distribution. If this person weights 75 kilos, the area under the curve (or a p-value!), which includes all the values as extreme or more extreme than 75, will be ca. 0.6 (or 60%), which is fairly big. So, we would fail to reject the null hypothesis, concluding that the person could be a German male, or is at least not different from German males. However, if the weight is 65 kilos, the area under the curve is quite small, let‚Äôs say 2%. Here, we can reject the null hypothesis and conclude that this person must belong to a different population, e.g.¬†women, or a man from a different country.\nSo you see the p-value is not a simple probability, but 3 probabilities (parts) added together! A simple probability is the number of outcomes of interest, divided by the total number of outcomes (it‚Äôs not even the one-sided p-value). While a p-value is the probability that random chance generated the\ndata we observed, or the data that is\n\nequally rare or 3. rarer for discrete values, or\n\nequally extreme or 3. more extreme for continuous values,\n\nif the null hypothesis is true.\nP-value definition N¬∞4: original Fisher‚Äôs definition\nAll right, p-value is a cumulative probability. And since a probability is a continues measure from 0 to 1 (or from 0% to 100%), the p-value also IS any number between 0 & 1. If we look for a difference between two groups, we can see a p-value as a measure of similarity.\n\nFor examples if two samples are identical, there is 100% similarity and 0% difference. So, our p-value is equal to 1. If similarity is only 60%, then the difference is 40%, which is much bigger then 0, but is still small. But if the similarity drops to 5% or below, then the difference of 95% is often considered significant, and we can confidently reject the null hypothesis - that there is NO difference between samples, in favour of the alternative hypothesis - that such difference exists.\nBut wait!!!, does it make any sense to dichotomize a continuous number (from 0 to 1) into only two categories - significant and not-significant? Of coarse, not! The p-value is not black and white, but everything in between ‚Ä¶ like 50 shades of gray ‚Ä¶\n\n‚Ä¶ ups, sorry, not that 50 shades of gray ‚Ä¶ that one :) (Sterne, Smith, and Cox 2001)\n\nHmm, but if dichotomizing continuous p-values into two categories - significant and not-significant - doesn‚Äôt make much sense, why is the whole world doing exactly that? ‚ÄúThe basic explanation is neither philosophical nor scientific, but sociologic - everyone uses them.‚Äù (Goodman 2019). But in order to answer this question more thoroughly, we‚Äôd need to briefly explore the history of p-values ‚Ä¶\nRonald Fisher, the father of modern statistics, saw the P value as an index measuring the strength of evidence against the null hypothesis (see the picture above). Fisher himself proposed: ‚Äúif P-value is between 0.1 and 0.9 there is certainly no reason to suspect the hypothesis tested. If it‚Äôs below 0.02 it is strongly indicated that the hypothesis fails to account for the whole of the facts.‚Äù He sometimes used the threshold for the p-value of 0.05 (< 5%) himself to reject the null hypothesis, but without any clear reason. Moreover, he recommended to treat a p-value around 0.05 as inconclusive, where we‚Äôd need to repeat the experiment. The reason Fisher used p-values at all was - to reduce the probability of the type I error - the probability to find something what is not there - false positive, e.g.¬†that somebody has cancer, while being totally healthy. The most typical type I error happens when we measure a difference between two groups and find this difference to be significant, while both samples came from the same population:\n\nHaving a stiff threshold of 0.05 can be:\nGood: because from all H0 you test, in the long run you will falsely reject at most 5% of the correct ones, but it‚Äôs also\nBAD: because it statistically guarantees that 1 in 20 healthy people, will ‚Äúhave‚Äù cancer just by chance, or in 1 out of 20 statistical tests we‚Äôll find nonsense just by chance.\nThus, lowering the p-value to, let‚Äôs say 0.01, reduces the probability of finding nonsense and ensures we find something what is really there.\nAlong the type I error, Neyman and Pearson also advocated the type II error, that could be made in interpreting the results of an experiment. The type II error is the probability to miss something what is there - false negative, e.g.¬†failing to detect cancer, when cancer is already in the body.\n\nNeyman and Pearson‚Äôs idea of hypothesis testing was actually really good, because it supposed to reduce the number of mistakes by keeping the rates of both type I and type II errors low. However, only the easy part of their approach ‚Äî that the null hypothesis can be rejected if P < 0.05 (type I error rate of 5%) ‚Äî has been widely adopted and stuck in the medical research, causing current dichotomy of results into significant or not significant.\nTwo serious consequences of this are that\npotentially clinically important differences observed in small studies are denoted as non-significant and ignored, while\nall significant findings are assumed to result from real treatment effects.\nWell, how do we deal with type II errors? Increasing the sample size will increase the power of the experiment and therefore decrease the probability of type II error.\nSo, now we know that hypothesis testing can produce two types of mistakes:\nThe type I error IS an ERROR, because p-values help to maintain nonsense. While\nThe type II error IS an ERROR, because p-values help to miss something important.\nTherefore, we should stop dichotomising our results into significant and not significant! But, if after reading this you still want to continue dichotomising your results into significant or not significant, please, answer a following question: is the difference between a P-value of 0.051 and 0.049 statistically significant?\nThe answer is - a clear NO! However, if using a p-value continuously totally blows your mind, and you still need some pragmatic decision making tool, consider using a hybrid approach, which singles out 5 instead of only 2 categories of the strength of the evidence against the null hypothesis in favor of the alternative:\nP > 0.10 The data shows NO evidence against the null hypothesis\n0.05 < P < 0.10 Weak evidence against the null hypothesis in favor of the alternative. It is sometimes reported as a dot: ‚Äò.‚Äô\n0.01 < P < 0.05 Moderate evidence against the null hypothesis in favor of the alternative. It is sometimes reported as one star: ‚Äò*‚Äô\n0.001 < P < 0.01 Strong evidence against the null hypothesis in favor of the alternative. It is sometimes reported as two stars: ‚Äò**‚Äô\nP < 0.001 Very strong evidence against the null hypothesis in favor of the alternative. It is sometimes reported as three stars: ‚Äò***‚Äô\nDon‚Äôt follow any cut-off, the cut-off should follow you!\nAny cut-off splits the probability into two parts:\nLevel of confidence - beta (Œ≤), e.g.¬†95%, tells how confident are we in our decision and\nLevel of significance - alpha (‚ç∫), e.g.¬†5%, tells us when to reject or fail to reject the \\(H_0\\). Namely, (1) if p-value <= ‚ç∫, we reject the \\(H_0\\); (2) if p-value > ‚ç∫, we fail to reject the \\(H_0\\).\nBoth ‚ç∫ & Œ≤ add up to 1 and tell you the same thing - how sure are you about your decision:\nfor 95% confidence: ‚ç∫ = 1 - Œ≤ = 1 - 0.95 = 0.05\nfor 99% confidence: ‚ç∫ = 1 - Œ≤ = 1 - 0.99 = 0.01\nThe cut-off itself is up to you and is highly dependent on the experiment. It might sound fuzzy, but it actually gives you a freedom to adjust the decision-making to reality. On the other hand, clinging to the cut-off of 0.05 may actually increase the rates of both type I and type II errors. Let‚Äôs have a look at two examples:\nImagine you study a new treatment for a deadly disease and your p-value for the difference between control and treatment groups is 0.15. If you strictly follow the 0.05 cut-off, you‚Äôll fail to reject the \\(H_0\\) and would conclude that the treatment does not work. The p-value of 0.15 makes you ‚Äúonly‚Äù 85% sure that such difference exists, making this difference ‚Äúnot significant enough.‚Äù However, if you don‚Äôt blindly follow the 0.05, you could look at your experiment in a completely new way by saying: I am 85% confident that new treatment works and we found a new way of treating a deadly disease.\nSimilarly, if I was 85% confident of winning the lottery I would definitely play! However, if I‚Äôd strictly follow the cut-off of 0.05, I would ‚Äúbe not confident enough‚Äù that I will win (which is ridiculous) and I won‚Äôt play.\nOn the opposite side, if an airline company welcomes you aboard with a catchy phrase: ‚ÄúWe are 95% confident that you‚Äôll survive, so your probability of crashing with us is below 5% (p < 0.05),‚Äù would you go aboard? I personally would run away! The airline company would chase me though and scrim: \"Stop running away! We successfully rejected the Null Hypothesis that you will die!!! But I would only be willing to fly .. if my level of confidence in survival increases to 99.9999% or if my chances to die would become 1 to a million (‚ç∫ = 0,000001) flights.\nSo, you see?, the freedom to set up your own level of significance (the cut-off) depending on the situation makes you a better scientist, while blindly following the cut-off of 0.05 would either keep you pure or just kill you.\n\nP.S.: the dramatic effect of the last sentence serves solely educational purposes ‚Ä¶ while hoping to significantly (p < 0.05 üòâ) increase the probability that you‚Äôll remember the material :)\n\nDoes it mean that any threshold is evil? Well, no! The threshold of 0.05 means that if there is no difference between Group 1 and Group 2, and if we did this exact experiment 100 times, then only 5 of these experiments would result in a wrong decision. These would be 5 false positives (type I error). Lowering the threshold to let‚Äôs say 0.001 would reduce the number of false positives to 1 out of 1000 experiments. But the costs of doing 1000 experiments are often much higher then it is worth.\nA single article you need to read: ‚ÄúMoving to a World Beyond ‚Äúp<0.05‚Äù \"\nIf you want to know what statisticians themselves think about p-value, read the following article: Moving to a World Beyond ‚Äúp<0.05‚Äù (Wasserstein, Schirm, and Lazar 2019). Here I would just sum up some recommendations the authors provided. These recommendations would enable you to find fewer false alarms (false positives, type I error), and to overlook fewer discoveries (false negatives, type II error).\nWhat to do:\nreport precise continuous p-values without reference to arbitrary thresholds. For example, P = 0.023 rather than P < 0.05.\nlower the 0.05 ‚Äústatistical significance‚Äù threshold for claims of novel discoveries to a 0.005 threshold and refer to p-values between 0.05 and 0.005 as ‚Äúsuggestive.‚Äù\nremember, the p-value as only one aspect of a complete data analysis. Thus, supplement the p-value with visualizations of confidence intervals on effect sizes or likelihood ratios, justify the adequacy of the sample size and the reasons various statistical methods were employed.\nclarify objectives, invest into careful planning & design, invest into producing solid data, use more descriptive stats, use more of the regularized, robust, nonlinear and nonparametric methods for exploratory research, check the robustness of your methods and use several different data analysis techniques for testing the same hypothesis.\nlook for both (1) a small p-value and (2) a large effect size before declaring a result ‚Äúsignificant,‚Äù instead of dichotomized p-values alone.\ninterpret the p-value in the context of sample size. A large study can detect a small, clinically unimportant finding. The larger the sample the more likely a difference to be detected. Lower the p-value threshold (e.g.¬†to 0.001) for large studies.\nreduce unplanned and uncontrolled modeling/testing (p-hacking). Examining 20 associations will produce one result that is ‚Äúsignificant at P = 0.05‚Äù by chance alone. Thus, testing multiple variables, using long questionnaires with hundreds of variables and measuring a wide range of potential outcomes, would guarantee several false positive findings.\n‚ÄúAccept uncertainty and embrace variation in effects‚Äù (McShane et al. 2019)\nWhat not to do:\nDon‚Äôt make decisions based solely on some arbitrary threshold such as p < 0.05. Dichotomizing p-values into ‚Äúsignificant‚Äù and ‚Äúnon significant‚Äù one loses information. A P-value is not black and white, but ‚âà 50 shades of grey üòÇ\nDon‚Äôt treat ‚Äúp = 0.051‚Äù and ‚Äúp = 0.049‚Äù as being categorically different. üôÇ As Gelman and Stern (2006) famously observed, the difference between ‚Äúsignificant‚Äù and ‚Äúnot significant‚Äù is not itself statistically significant. Don‚Äôt discard a well-designed, excellently conducted, thoughtfully analyzed, and scientifically important experiment only because it failed to cross the threshold of 0.05 (e.g.¬†p-value = 0.09).\nDon‚Äôt believe that an association or effect exists just because it was ‚Äústatistically significant.‚Äù\nDon‚Äôt believe that an association or effect is absent just because it was not ‚Äústatistically significant.‚Äù\nDon‚Äôt conclude anything about practical importance based on ‚Äústatistical significance‚Äù (or lack thereof).\nIn fact - don‚Äôt say ‚Äústatistically significant‚Äù at all ‚Ä¶ whether expressed in words or by asterisks in a table.\nDon‚Äôt believe that your p-value gives the probability that your test hypothesis is TRUE. P-value is the probability of the data, not of (any) hypothesis!\nWhat about Bayesian statistical inference?\nThe switch to the use of Bayesian statistical inference in medical research was proposed for several decades, but is barely possible. A major reason is that prior knowledge can be difficult to quantify. The major reason for that is that science suppose to produce NEW knowledge. Thus, repeated experiments are necessary, but few scientist want to do them. Moreover, if the priors are flat (a wide range of values is considered to be equally likely), then the Frequentist and Bayesian results are similar, while computing power required for Bayesian analysis is ‚Äúsignificantly‚Äù ;) higher. Two approaches will give different results, however, if our prior opinion is not vague. Besides, we can‚Äôt ignore the fact that Bayesian statistics is even less intuitive to the majority of scientists then Frequentists statistics. Otherwise everybody would have already used it. Thus, until an alternative, simple and widely accepted approach to the p-values exists, banning p-values is of no help to anyone.\nConclusion\nMost of the scientist can‚Äôt tell you what the p-value is. Most of the statisticians can, but can‚Äôt explain it to intuitively. So, don‚Äôt give yourself a hard time if you still a bit confused. It‚Äôs normal. Instead, I‚Äôd suggest we stop caring about understanding the p-value, but start caring to use it properly. And remember, you don‚Äôt need to understand how the car works, you only need to know how to drive.\n\nWhat‚Äôs next to learn?\nmisinterpretations of p-values (only if you are interested)\neffect size and\npower analysis\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings and watchings\nthat‚Äôs an amazing video: https://www.youtube.com/watch?v=tLM7xS6t4FE&ab_channel=SciShow\nhttp://www.stat.ualberta.ca/~hooper/teaching/misc/Pvalue.pdf\n\n\n\nGelman, Andrew, and Hal Stern. 2006. ‚ÄúThe difference between \"significant\" and \"not significant\" is not itself statistically significant.‚Äù American Statistician 60 (4). https://doi.org/10.1198/000313006X152649.\n\n\nGoodman, Steven N. 2019. ‚ÄúWhy is Getting Rid of P-Values So Hard? Musings on Science and Statistics.‚Äù American Statistician 73 (sup1). https://doi.org/10.1080/00031305.2018.1558111.\n\n\nMcShane, Blakeley B., David Gal, Andrew Gelman, Christian Robert, and Jennifer L. Tackett. 2019. ‚ÄúAbandon Statistical Significance.‚Äù American Statistician 73 (sup1). https://doi.org/10.1080/00031305.2018.1527253.\n\n\nSterne, Jonathan A. C., George Davey Smith, and D. R. Cox. 2001. ‚ÄúSifting the evidence‚Äîwhat‚Äôs wrong with significance tests?‚Äù BMJ 322 (7280). https://doi.org/10.1136/bmj.322.7280.226.\n\n\nWasserstein, Ronald L, Allen L Schirm, and Nicole A Lazar. 2019. ‚ÄúMoving to a World Beyond \"p < 0.05\".‚Äù https://doi.org/10.1080/00031305.2019.1583913.\n\n\n\n\n",
    "preview": "posts/2021-07-31-p-value-intuitive-explanation/thumbnail.jpg",
    "last_modified": "2021-09-28T10:53:03+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/",
    "title": "R package reviews {DataExplorer} explore your data!",
    "description": "What is the best way to explore the data quick? I think it's visualization. And what it the best way to visualize the data quick? I think it's - {DataExplorer} package, because it can visualize all your data in seconds using only one function! Check this out...",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-06",
    "categories": [
      "R package reviews",
      "EDA",
      "videos"
    ],
    "contents": "\n\nContents\nR demo (ca. 9 min) about DataExplorer package\nAutomated EDA: visualise everything with only one function!\nTable of contents\nBig Picture of your data\nView missing value distribution\nVisualise distribution of categorical (discrete) variables\nVisualize correlation\nVisualise distribution of numeric variables\nVisualize boxplot\nVisualize scatterplots\nVisualize principal component analysis\nData transformation or Feature EngineeringSum up small categories\nDummification\nSet all missing values to a particular value.\nTransform or drop columns\n\nConclusion\nWhat‚Äôs next?\nFurther readings and references\n\n\n\n# install.packages(\"tidyverse\")\n# install.packages(\"DataExplorer\")\n# install.packages(\"datasets\")\n\nlibrary(tidyverse)        # for data wrangling and visualization\nlibrary(DataExplorer)     # for exploratory data analysis\nlibrary(datasets)         # for getting the data \n\n\n\nR demo (ca. 9 min) about DataExplorer package\n\n\n\n\n\n\n\nAutomated EDA: visualise everything with only one function!\ncreate_report() function will produce, save and open an HTML document in your web-browser with visualization of all your data in seconds! Moreover, we can determine a single most important variable in our dataset and let the report display the data in the light of this variable. Just run create_report() function with two different datasets, as shown below and see the difference for yourself.\n\n\nairquality <- datasets::airquality %>% \n  mutate(Month = factor(Month))\n\n\n\n\n\ncreate_report(airquality)\n\ncreate_report(diamonds, y = \"price\")\n\n\n\nEvery report consists of several parts and always starts with ‚ÄúTable of contents‚Äù.\nTable of contents\n\nTable of contents shows what exactly the exploratory analysis visualized. Namely, it first generally describes the dataset with raw numbers and percentages, then visualizes missing values and every single (univariate) numeric or categorical variable, while checking the distribution of numeric variables with histograms and Quantile-Quantile plots. The report then explores multiple variables simultaneously by displaying correlation between then and conducting a principal component analyses. It finishes with box and scatter plots, which I find especially informative.\nNot bad for a single function, if you ask me ;)\nBut, should we then always produced the whole report? Of coarse not. Conveniently, every part of the report can be produced separately, which not only allows you to explore every aspect of our data even deeper than in the big report, but also gives you the opportunity to adjust the appearance of plots as you wish. Let‚Äôs have a look at this!\nBig Picture of your data\nBelow we can see the basic overview of diamonds data in absolute numbers. First, we see the number of rows and columns (variables). Then we see how many numeric and categorical variables do we have in our dataset and whether there are missing values or even missing columns.\n Such quick introduction into our dataset can be reached by the function introduce() for absolute numbers (‚Äút‚Äù simply transposes the result) or by the plot_intro() for percentages.\n\n\nintroduce(diamonds) %>% t()\n\n\n                        [,1]\nrows                   53940\ncolumns                   10\ndiscrete_columns           3\ncontinuous_columns         7\nall_missing_columns        0\ntotal_missing_values       0\ncomplete_rows          53940\ntotal_observations    539400\nmemory_usage         3457760\n\nplot_intro(airquality)\n\n\n\n\nView missing value distribution\nOne of the most useful functions from DataExplorer package, which I use almost daily is plot_missing(). It not only shows - how many percent of data is missing in every columns, but also explains what the amount of missing values means for us, namely that missing around 25% of data is still kind of OK and we can continue working. This legend helps me to better communicate with my collaborators, which often want me to continue analysis, even when I tell them there is not enough data in some columns. Yes, :) some people prefer to believe statistical software then a human being. Besides, it helps to quickly identify variables with missing values, if some models refuse to run because missing values are present.\nMoreover, all plots in DataExplorer can be pimped with the syntax from ggplot2 package:\n\n\n# pimp your plot with ggplot2\nplot_missing(airquality, ggtheme = theme_minimal())\n\n\n\n\nVisualise distribution of categorical (discrete) variables\nThe best way to visualize categorical variables are bar-plots. And a simple plot_bar() function visualizes all categorical variables from our dataset at once. It even sorts the categories for us putting the most frequent categories on the top. It‚Äôs very useful to see how many data-points every category has and usually, in my job, I see way too many small categories which are then supposed to be thrown together. By the way DataExplorer package can do this too, and easily, but more about this later. Until then, we can go one step further as compared to the create_report() function and display the frequency distribution of our categorical variable by any other variable, be it numeric (e.g.¬†price) or discrete (e.g.¬†cut). So, the long time of using table() function to count categorical variables is over for me :)\n\n\nplot_bar(diamonds)\n\n\n\n## View frequency distribution by a numeric variable\nplot_bar(diamonds, with = \"price\", ggtheme = theme_classic())\n\n\n\n## View frequency distribution by a discrete variable\nplot_bar(diamonds, by = \"cut\", ggtheme = theme_bw())\n\n\n\n\nVisualize correlation\nOne of the most powerful tools to summarize a large dataset and to identify patterns and connections between variables is a correlation matrix. A correlation matrix is simply a cross table with correlation coefficients between all combinations of variables pairwisely. This table is at the same time a heatmap, which colorcoded correlation from the negative (in blue) to the positive (in red). In practice, high correlation can be desirable, for example where it may allow to predict one variable by the other. However, it also may be unwanted, for example two highly correlated variables in a statistical model would screw up the results by providing the same information, like weight and BMI. Thus, knowing correlation is always useful for developing better hypotheses.\n\n\n## View overall correlation heatmap\nplot_correlation(na.omit(airquality), type = \"continuous\")\n\n\n\n\nVisualise distribution of numeric variables\nThe distribution of numeric variables is usually explored by the means of histograms, density plots, Quantile-Quantile plots and scatterplots. Well, DataExplorer package provides easy functions for each of them. Moreover, it allows to produce the Quantile-Quantile plots for every category of a discrete variable, which I personally find very useful, since usual groups comparison (be it a t-test or ANOVA) requires checking the normality of every group, and not of the whole numeric variable.\nAs you probably remember, the big-picture from the create_report() function does not provide density and categorical Quantile-Quantile plots, showing that it‚Äôs worth to go beyond the general report. For instance, you immediately see that Wind and Temperature can be analyzed using parametric methods (e.g.¬†ANOVA), while Ozone would either need a non-parametric approach (e.g.¬†Kruskal-Wallis) or would need to be transformed before modeling.\n\n\n## View histogram of all continuous variables\nplot_histogram(airquality)\n\n\n\n## View estimated density distribution of all continuous variables\nplot_density(airquality, ncol = 3)\n\n\n\n## View quantile-quantile plots of all continuous variables\nplot_qq(airquality, ncol = 3, ggtheme = theme_linedraw())\n\n\n\n## View quantile-quantile plots of all continuous variables of all categories \nplot_qq(iris, by = \"Species\", \n        ncol = 2,\n        ggtheme = theme_bw(), \n        theme_config = list(legend.position = c(\"top\")))\n\n\n\n\nVisualize boxplot\nBoxplots beautifully show where the most of the data is, namely inside of the box, and whether variables have many outliers. Boxplots are also useful for a quick comparison of distributions of several groups, samples or categories, because they visualize center, spread and range of every group. Thus, similarly to correlation analysis above, boxplots also help to develop hypotheses. I love boxplots and have already three videos on them on my YouTube channel.\n\n\n## View bivariate continuous distribution based on `Month`\nplot_boxplot(iris, by = \"Species\", \n             ncol = 2, \n             ggtheme = theme_bw())\n\n\n\n\nVisualize scatterplots\nScatterplots are kind of self-explanatory.\n\n\nplot_scatterplot(airquality, by = \"Temp\", ncol = 2, ggtheme = theme_classic())\n\n\n\n\nVisualize principal component analysis\nThe goal of PCA is to reduce the number of variables in a dataset, while preserving as much information from the original dataset as possible. PCA transforms larger datasets with a lot of variables, some of which might be highly correlated, into a smaller dataset with a few new variables - (which are) the principal components and are uncorrelated with each other.\n\n\nplot_prcomp(diamonds, maxcat = 5L)\n\n\n\n\nData transformation or Feature Engineering\nSum up small categories\nTransforming and cleaning your data is the next step after EDA. Being able to make it quick and simple frees your time and brain capacity for the important creative work instead of solving problems. The best package for data manipulation is definitely - {dplyr}! However, some other worth to know packages also provide very simple and useful ways, but fade in the bright light of dplyr.\nDataExplorer is a good example. For instance, if your have a lot of small categories in a variable, you can aggregate them into a single category - ‚ÄúOTHER‚Äù, you just have to call the variable you are interested in and set the threshold for how many percent of your data suppose to go into this ‚ÄúOTHER‚Äù category. For instance, the variable clarity has 4 categories, all of which have less then 10% of observations. Cumulatively (together) they add up to 20%. Thus, we can use 20% as a threshold and end up with only 5 big categories:\n\n\nplot_bar(diamonds)\n\n\n\njanitor::tabyl(diamonds$clarity) %>% \n  mutate(percent = round(percent*100)) %>% \n  arrange(-percent)\n\n\n diamonds$clarity     n percent\n              SI1 13065      24\n              VS2 12258      23\n              SI2  9194      17\n              VS1  8171      15\n             VVS2  5066       9\n             VVS1  3655       7\n               IF  1790       3\n               I1   741       1\n\n## Group bottom 20% `clarity` by frequency\nbla <- group_category(\n  diamonds, \n  feature = \"clarity\", \n  threshold = 0.2, \n  update = T)\n\njanitor::tabyl(bla$clarity) %>% \n  mutate(percent = round(percent*100)) %>% \n  arrange(-percent)\n\n\n bla$clarity     n percent\n         SI1 13065      24\n         VS2 12258      23\n       OTHER 11252      21\n         SI2  9194      17\n         VS1  8171      15\n\nplot_bar(bla)\n\n\n\n\nDummification\nSometimes, especially in the machine learning field, it is useful to produce several dummy variables (0s & 1s) from one categorical variable. We can easily do this by calling a dummify() function and specify the variable which needs to be dummified. Interestingly, if we don‚Äôt specify any, all categorical variables will be transferred.\n\n\n## Dummify diamonds dataset\nglimpse(dummify(diamonds, select = \"cut\"))\n\n\nRows: 53,940\nColumns: 14\n$ carat         <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.2‚Ä¶\n$ depth         <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.‚Ä¶\n$ table         <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 5‚Ä¶\n$ price         <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, ‚Ä¶\n$ x             <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.0‚Ä¶\n$ y             <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.1‚Ä¶\n$ z             <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.5‚Ä¶\n$ color         <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, ‚Ä¶\n$ clarity       <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2‚Ä¶\n$ cut_Fair      <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ cut_Good      <int> 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶\n$ cut_Ideal     <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ‚Ä¶\n$ cut_Premium   <int> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ‚Ä¶\n$ cut_Very.Good <int> 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, ‚Ä¶\n\nglimpse(dummify(diamonds))\n\n\nRows: 53,940\nColumns: 27\n$ carat         <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.2‚Ä¶\n$ depth         <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.‚Ä¶\n$ table         <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 5‚Ä¶\n$ price         <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, ‚Ä¶\n$ x             <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.0‚Ä¶\n$ y             <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.1‚Ä¶\n$ z             <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.5‚Ä¶\n$ cut_Fair      <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ cut_Good      <int> 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ‚Ä¶\n$ cut_Ideal     <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, ‚Ä¶\n$ cut_Premium   <int> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, ‚Ä¶\n$ cut_Very.Good <int> 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ color_D       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ color_E       <int> 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, ‚Ä¶\n$ color_F       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ‚Ä¶\n$ color_G       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ color_H       <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, ‚Ä¶\n$ color_I       <int> 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ color_J       <int> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, ‚Ä¶\n$ clarity_I1    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ clarity_IF    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ clarity_SI1   <int> 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, ‚Ä¶\n$ clarity_SI2   <int> 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ‚Ä¶\n$ clarity_VS1   <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, ‚Ä¶\n$ clarity_VS2   <int> 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ clarity_VVS1  <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n$ clarity_VVS2  <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ‚Ä¶\n\nSet all missing values to a particular value.\n\n\ndf <- airquality %>% \n  slice(1:10)\n\ndf\n\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    NA     194  8.6   69     5  10\n\nset_missing(df, list(0L, \"unknown\"))\n\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5      0       0 14.3   56     5   5\n6     28       0 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10     0     194  8.6   69     5  10\n\nTransform or drop columns\nAgain, dplyr is much better for data manipulation. But if you still don‚Äôt know dplyr, first, your should, and secondly, you can use functions below to quickly update or drop columns.\n\n\n## Update columns\nupdate_columns(airquality, c(\"Month\", \"Temp\"), as.factor) %>% head()\n\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nupdate_columns(airquality, c(\"Temp\", \"Wind\"), function(x) x^2) %>% head()\n\n\n  Ozone Solar.R   Wind Temp Month Day\n1    41     190  54.76 4489     5   1\n2    36     118  64.00 5184     5   2\n3    12     149 158.76 5476     5   3\n4    18     313 132.25 3844     5   4\n5    NA      NA 204.49 3136     5   5\n6    28      NA 222.01 4356     5   6\n\n## Drop columns\nmtcars %>% drop_columns(2:10) %>% head()\n\n\n   mpg carb\n1 21.0    4\n2 21.0    4\n3 22.8    1\n4 21.4    1\n5 18.7    2\n6 18.1    1\n\ndiamonds %>% drop_columns(c(\"x\", \"y\", \"z\")) %>% head() \n\n\n# A tibble: 6 x 7\n  carat cut       color clarity depth table price\n  <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int>\n1 0.23  Ideal     E     SI2      61.5    55   326\n2 0.21  Premium   E     SI1      59.8    61   326\n3 0.23  Good      E     VS1      56.9    65   327\n4 0.290 Premium   I     VS2      62.4    58   334\n5 0.31  Good      J     SI2      63.3    58   335\n6 0.24  Very Good J     VVS2     62.8    57   336\n\nConclusion\nDataExplorer allows us not only to quickly explore our data, but also helps to get initial insights and to generate new hypotheses. Moreover, it greatly simplifies the workflow, which we need to do anyway, saves time, reduces the probability of mistakes by coding everything manually and accelerates delivering final results. So, DataExplorer alone would be enough for a reasonably good exploration of data. But, there are of coarse many other useful packages for EDA. Thus, in the next post I‚Äôll present the best functions from different packages which will take your EDA to the next level.\nWhat‚Äôs next?\nThe next steps in your journey to the final results are cleaning and further pre-processing your data.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings and references\nhttp://boxuancui.github.io/DataExplorer/\nOne of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com\n\n\n\n",
    "preview": "posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/2.png",
    "last_modified": "2021-01-21T23:07:35+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-06-survival-analysis-2-parametric-survival-models/",
    "title": "Survival analysis 2: parametric survival models",
    "description": "The non-parametric Kaplan-Meier method (KM) can not describe survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g. Exponential, Weibull etc.) can! Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post we‚Äôll try to close this gap.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-06",
    "categories": [
      "survival analysis",
      "videos"
    ],
    "contents": "\n\nContents\nPrevious topics\nWhy do we need parametric survival models\nIs Time a Variable or a Constant?\nSteady change in hazard and survival\nPositive exponential change in hazard and survival\nNegative exponential change in hazard and survival\nB(u)ilding Exponential model ‚Ä¶ finally ü•≥\n\nHow to compute parametric models\nFinal thoughts\nFurther readings and references\n\n\n\nlibrary(tidyverse)  # data wrangling and visualization\nlibrary(sjPlot)     # for plotting results of log.regr.\nlibrary(effects)    # for probability output and plots\nlibrary(survival)   # for core survival analysis routines\nlibrary(survminer)  # for drawing survival curves\nlibrary(flexsurv)   # for Parametric Survival Modelling\nlibrary(knitr)      # for a wonderful-looking tables \n\n\n\nPrevious topics\nA good understanding of Kaplan-Meier method (KM) is a prerequisite for this post, but since you are here, I suppose you are already familiar with it üòâ\nIf you are more of a visual person, you can watch the video below first, but for more details and R-code have a look at the article.\n\n\n\n\n\n\n\nWhy do we need parametric survival models\n\n\n\nThe main disadvantage of the non-parametric Kaplan-Meier method (KM) shown in the picture above is that it can not be described survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g.¬†Exponential, Weibull etc.) can! Moreover, parametric models are the logical step on the way from the KM to the semi-parametric Cox models, because they beautifully connect the dots between KM and Cox models and thus greatly improve understanding of survival analysis. Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post we‚Äôll try to close this gap.\nIs Time a Variable or a Constant?\nSo, how can we describe the survival with a smooth function? To answer this question, let‚Äôs first describe a NOT-survival function, which happens to be a Hazard to die.\n\nImage by Leonardo Yip on unsplash.\nI‚Äôd describe death with two things: the event of death itself and a particular time point at which death happens. These two things always describe a single event, because one only dies üíÄ once. However, if several people die üíÄüíÄüíÄ, they will not die in the exact same moment, right? Nop. Thus, the time of death would vary and the number of death would grow over time, which would make the time itself a variable. That‚Äôs how several events of death at different time points allow us to express death in two ways:\nvia a different number of events per fixed unit of time, which is often called a Risk (Hazard) to die, or simply Hazard (\\(\\lambda\\) - lambda). This makes the number of events a variable, and Hazard a rate of death, e.g, per day. Or,\nvia different stretches of passing time per fixed number of events. The time interval is usually measured until the next event occurs. This makes the time a variable.\nBut whatever changes, the number of events per unite of time, or time per unit of events, the change itself is a key here, and there are different kinds of changes.\nSteady change in hazard and survival\nImagine that every day exactly 3 out of 10 people die in the cold ocean water after Titanic accident. It‚Äôs a pretty stable rate of death, or Hazard, of 30%. If the hazard to die would steadily grow with the same rate, then the probability of survival would steadily decrease at the same rate. Thus, Hazard and Survival can be expressed in terms of each other. Particularly, the Hazard of dying over time can be seen as a Failed survival (\\(F(t)\\) in the left formula below). Or, the Survival over time (\\(S(t)\\) the right formula below) can be seen as the Hazard of NOT-dying, or simply a negative Hazard, which mathematically can be expressed as a ‚Äú-‚Äù minus sign, or ‚Äú1 -‚Äù, in front of the Hazard. Both functions result in straight lines, where Hazard steadily increases and survival steadily decreases (plots below):\n\n\n\\[ F(t) = Hazard * t \\]\n\n\n\\[ S(t) = 1 - Hazard * t \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuch steady increase or decrease in hazard or survival are rather not natural, not realistic. It‚Äôs kind of hard to plan and monitor death üòâ, except you are a serial killer. The hazard usually either exponentially increases, e.g.¬†in the case of hunger, or exponentially declines, e.g.¬†in the case of a pandemic after vaccine was found. Thus, let‚Äôs have a look at both exponential changes.\nPositive exponential change in hazard and survival\nThink about hunger for a moment. It accumulates, right? And the longer we stay hungry, the higher is the probability (risk) that we die. The left plot below shows such a development, where a Hazard of dying \\(F(t)\\) expressed in probabilities is small in the beginning (few deaths), but grows exponentially with time (more and more deaths). I like to call such a trend a positive, or accelerating, exponential change. The ‚Äúexp‚Äù in the left formula below is all we need to add in order to plot such a trend.\nAgain, since survival can be seen as a negative hazard, we can express the survival \\(S(t)\\) by simply using a minus sign (or ‚Äú1-‚Äù) in front of the Hazard. The plot on the right displays the results of such survival function. It‚Äôs kind of obvious, that if the hazard of dying is low in the beginning of ‚Äútime‚Äù, then the probability of survival is high. At the end of ‚Äútime‚Äù Survival exponentially drops due to a exponential increase of the Hazard.\n\n\n\\[ F(t)= exp^{Hazard * t} \\]\n\n\n\\[ S(t) = 1 - exp^{Hazard * t} \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegative exponential change in hazard and survival\nHowever, in some cases, more people die at the beginning of ‚Äútime‚Äù, where after rate of death declines over time. Think about a pandemic, or a titanic crash. The change is still exponential, and goes in the same direction: up, in the case of the Hazard, and down, in the case of survival. However, the exponential change is kind of turned (bend) inside out. I like to call it a negative, or decelerating, exponential change.\nSuch ‚Äúinside out bending‚Äù can be achieved by a ‚Äú-‚Äù minus sing in front of both a Hazard and the exponential function itself (left equation and picture below). This minus does not describe the negative hazard, as in the example above. It only changes the curve from accelerating to decelerating. In order to get survival for this function, we also, as in the example above, have to use a ‚Äú-‚Äù (or ‚Äú1-‚Äù) in front of the whole exponential hazard expression. Interestingly, two minus signs in front of the ‚Äúexp‚Äù in the Survival formula neutralize each other and become a plus, which leaves us with the \\(exp^{-Hazard * t}\\) (right equation and picture below).\n\n\n\\[ F(t)= - exp^{-Hazard * t} \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ \\]\n\n\n\n\n\n\\[ S(t) = 1 - ( - exp^{-Hazard * t}) = exp^{-Hazard * t} \\]\n\n\n\n\n\nB(u)ilding Exponential model ‚Ä¶ finally ü•≥\n\nImage by Mih√•ly K√∂les on unsplash.\nNow we finally have our smooth survival function we wanted to describe in the beginning. Let‚Äôs look at it one more time and plot it on top of the not-smooth Kaplan-Meier step function.\nSince survival function \\(S(t)\\) shows the probability of survival passed the certain time point,\n\\[ S(t) = P(T > t) \\]\nand the exponential function \\(F(t)\\) shows the probability of Failed-survival passed the certain time point,\n\\[ F(t) = P(T \\leq t) = 1 - exp^{-Hazard * t} \\]\na survival function can then be expressed in terms of not-survival (exponential) function:\n\\[ S(t) = P(T > t) = 1 - P(T \\leq t) = 1 - F(t) = 1 - [1 - exp^{-Hazard * t}] = exp^{-Hazard * t} \\]\nSo that the survival function, which shows a rate of decrease, is a flipped hazard function, which showed a rate of increase. Thus, we can rewrite the \\(S(t)\\) as:\n\\[S(t) = exp^{- Hazard * t} = exp^{- \\lambda* t} \\]\nwhere -Hazard and time are two parameters which describe our exponential change in survival probability, which is why such models do have their name - parametric exponential models. And since the Hazard is negative, and exponential function is not-linear (aka. curvy) - out model produces a negative exponential curve (see below). Such smooth rate of decrease describes survival probability much better then a Kaplan-Meier method, which abruptly (step-wisely) drops probability only after an event, while keeping the probability constant between the events.\nHow to compute parametric models\n\n\nlibrary(flexsurv)    # for Parametric Survival Modelling\n\nex <- flexsurvreg(Surv(time, status) ~ 1, data = d2, dist=\"exponential\")\nwe <- flexsurvreg(Surv(time, status) ~ 1, data = d2, dist=\"weibull\")\n\nggsurvplot(\n  ex, \n  conf.int = FALSE, \n  surv.median.line = \"hv\",\n  xlab = \"Days\", \n  ylab = \"Survival probability\", \n  break.time.by = 1,\n  risk.table = F\n)\n\n\n\n\nThe parameters of the curve (\\(-\\lambda * t\\)) allow us to model and predict survival and hazard over time, which is the main advantage of exponential models over the Kaplan-Meier method, which is not-parametric and therefore not ‚Äúmodellable‚Äù. However, the non-linearity is often troublesome, and we‚Äôd rather use the linear regression concept, which summarizes (regresses) a lot of numbers into a few numbers, like the intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)). Fortunately, a non-linear curve can be easily ‚Äúlinearised‚Äù via a natural logarithm. For this, we don‚Äôt even have to understand how logarithm or the exponential function work, we only need to know that they neutralize each other. Moreover, using ‚Äúlog‚Äù (logarithmazing both sides of the equation below) produces three positive side effects:\nfirst, on the right side of the equation, this would transfer our curve (\\(-\\lambda * t\\)) into a line (\\(b_0 + b_1x_1 + ... + b_kx_k\\)), where we will be able to have an intercept and \\(\\beta\\) coefficients as in a usual linear logistic regression \\[Hazard = exp^{b_0 + b_1x_1 + ... + b_kx_k}\\]\nor\n\\[ log(Hazard) = b_0 + b_1x_1 + ... + b_kx_k \\]\nSuch survival model is in fact the Poisson model. Thus, it might help if you already know Poisson distribution. If not, that‚Äôs OK, you don‚Äôt have to understand Poisson before (citation from Prof.¬†Marin‚Äôs video, see references).\nsecondly, it will help us to connect to further models, like Weibull and Cox models, because the difference between them lie mainly in the intercept ‚Äú\\(b_0\\)‚Äù;\nand finally, it will greatly increase the interpretability, because the Hazard-Ratios (HRs) (that‚Äôs what exponential model delivers) can be interpreted exactly like the Odds-Ratios (ORs) from the logistic regression (as described in my posts on logistic regression). Similarly to the Odds in the logistic regression, the Hazard itself, which it the probability of dying NOW, is less useful then the Hazard-Ratios. A Hazard-Ratio is the ratio of hazard of somebody who is exposed (sick) to somebody who is not exposed (healthy). For instance if HR = 2, the risk of dying of somebody who is exposed is double compared to somebody who is not exposed.\nFinal thoughts\nAre parametric models useful? Of coarse!\n3 curves above which described different kind of changes in survival and hazard over time meant to say that distribution can be very different. If a suitable distribution can be found, parametric model is more informative than the KM or Cox model\nthey can predict survival probabilities at any given point in time, event hazard, mean and median survival times are readily available\nthey are also slightly more efficient and yield more precise estimates due a better fit (see the picture above)\nAre parametric models perfect? Of coarse not!\nparametric models need to specify the distribution, which may be difficult to identify\nthey are also mathematically more complex then, e.g.¬†KM, and are therefore rarer, which greatly decrease comparability of results among studies. Due to their lower popularity I will not go deeper into any particular parametric model: Weibull, Gompertz, Accelerated Failure Time models etc.\nThus, despite the fact that parametric models are a good alternative to the KM and Cox‚Äôs regression model (which do not need to specify any distribution), KM and Cox remain the most popular methods for analyzing survival data. And that is why the next logical step in your statistical journey would be learning about Cox Proportional Hazard Models (in progress).\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings and references\nM J Bradburn, T G Clark, S B Love, & D G Altman. (2003). Survival Analysis Part II: Multivariate data analysis ‚Äì an introduction to concepts and methods. British Journal of Cancer, 89(3), 431-436.\nExamples of focused model comparison: parametric survival models: https://cran.r-project.org/web/packages/fic/vignettes/survival.pdf\nsurvminer cheat sheet: https://rpkgs.datanovia.com/survminer/survminer_cheatsheet.pdf\nthe whole playlist of videos from MarinStatsLectures! They are amazing! https://www.youtube.com/watch?v=vX3l36ptrTU&list=PLqzoL9-eJTNDdnKvep_YHIwk2AMqHhuJ0 especially the second video of marinstatslecture: https://www.youtube.com/watch?v=MdmWdIV5k-I&list=PLqzoL9-eJTNDdnKvep_YHIwk2AMqHhuJ0&index=2\nOne of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com\n\n\n\n",
    "preview": "posts/2021-01-06-survival-analysis-2-parametric-survival-models/thumbnail_survival_2.png",
    "last_modified": "2021-06-04T18:34:10+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/",
    "title": "R package reviews {performance} check how good your model is! ",
    "description": "There are several indicators of model quality, e.g. $R^2$ or AIC, and several assumption for every model which supposed to be checked, e.g. normality of residuals, multicollinearity etc.. R provides solutions for every indicator or assumption you can imagine. However, they are usually spread around different packages and functions. {performance} package brings all of quality indicators and all of the assumption under one roof. Thus, for me it became the one-stop solution for modelling.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-03",
    "categories": [
      "R package reviews",
      "videos",
      "visualization"
    ],
    "contents": "\n\nContents\nR demo for how check model performance and model assumptions\nCheck model performance, or model quality\nUsual linear models\nComplex mixed-effects models\nFancy Bayesian mixed-effects models\nCompare models\nUse individual quality indicators\n\nCheck assumptions: or modern (Jan 2020) model diagnostics\nAll assumptions at once!\nIndividual assumptions with reports of statistical tests!\n\nUseful references\n\n\n\nlibrary(tidyverse)          # data manipulation\nlibrary(performance)        # model performance\n\n\n\nR demo for how check model performance and model assumptions\nA short (ca. 15 min) video below shows how performance package works and the code you‚Äôll see in the video is provided below.\n\n\n\n\n\n\n\nCheck model performance, or model quality\nUsual linear models\nIn order to see how model performs, use the intuitive function - model_performance. It provide several quality indicators, which differ depending on the model. For instance the first model below provides the most ‚Äúclassical‚Äù quality indicators:\nAIC - Akaike‚Äôs Information Criterion, the lower the better. AIC is an estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data.\nBIC - Bayesian Information Criterion, the lower the better\n\\(R^2\\) - the proportion of the variance explained, the higher the better. It is sometimes referred to as a goodness of fit of the model\n\\(R^2 adjusted\\) - the proportion of the variance explained for multiple (several predictors) models, the higher the better\nRMSE - Root Mean Square Error is a measure of spread of the residuals around predictions (prediction errors), the lower the better\nSigma - standard deviation is a measure of spread of the data around the mean, the lower the better\n\n\nm <- lm(mpg ~ hp + cyl, data = mtcars)\n\nmodel_performance(m)\n\n\n# Indices of model performance\n\nAIC     |     BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n-----------------------------------------------------\n169.562 | 175.425 | 0.741 |     0.723 | 3.021 | 3.173\n\nComplex mixed-effects models\nMixed effects model provide two different \\(R^2\\)s and ICC:\nconditional \\(R^2\\) shows model‚Äôs total explanatory power and\nmarginal \\(R^2\\) show the part related to the fixed effects (predictors) alone\nICC - intraclass correlation coefficient, is similar to \\(R^2\\) and also shows the goodness of fit or, in other words, quantifies the proportion of variance explained by a grouping (random) factor in mixed-effects (multilevel/hierarchical) models.\n\n\nlibrary(lme4)\n\nm1 <- lmer(mpg ~  hp * cyl + wt + (1 | am), data = mtcars)\n\nmodel_performance(m1)\n\n\n# Indices of model performance\n\nAIC     |     BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma\n-------------------------------------------------------------------\n167.074 | 177.334 |      0.864 |      0.862 | 0.009 | 2.055 | 2.240\n\nFancy Bayesian mixed-effects models\nELPD - expected log pointwise predictive density is a measure the prediction accuracy of Bayesian models, the closer to 0 the better. The out-of-sample predictive fit can either be estimated by Bayesian leave-one-out cross-validation (LOO) or by widely applicable information criterion (WAIC). Thus, the two next indicators are:\nLOOIC - leave-one-out cross-validation information criterion, the lower the better\nWAIC - widely applicable information criterion, the lower the better\n\n\nlibrary(rstanarm)\n\nm2 <- stan_glmer(mpg ~ hp * cyl + wt  + (cyl | am), data = mtcars, refresh=0)\n\nmodel_performance(m2)\n\n\n# Indices of model performance\n\nELPD    | ELPD_SE |   LOOIC | LOOIC_SE |    WAIC |    R2 | R2 (marg.) | R2 (adj.) |  RMSE | Sigma\n-------------------------------------------------------------------------------------------------\n-75.612 |   3.996 | 151.223 |    7.991 | 150.534 | 0.862 |      0.856 |     0.826 | 2.065 | 2.273\n\nCompare models\nUnfortunately we can‚Äôt compare not similar models, for instance different types of models (e.g.¬†linear vs.¬†mixed effects) or models done with different amounts of data, because one of them will be better for reasons other then model quality. Even more unfortunately, such comparisons are often conducted anyway. I was also guilty of it in the past. One of the common R functions allowing such comparisons is anova(model1, model2). It smartly does not allow the comparison of linear vs.¬†mixed effects models (see the red warning below), but stupidly can be out-tricked by placing mixed effects model first.\n\n\nanova(m, m1)\n\n\n\nError: $ operator not defined for this S4 class\n\n\nanova(m1, m)\n\n\nData: mtcars\nModels:\nm: mpg ~ hp + cyl\nm1: mpg ~ hp * cyl + wt + (1 | am)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm     4 169.56 175.43 -80.781   161.56                         \nm1    7 151.05 161.31 -68.524   137.05 24.513  3  1.952e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIn contrast compare_performance() compares them immediately and provides a useful warning (see below). It also provides an somewhat superficial (according to the package author), but still useful ranking of models (see Performance-Score).\n\n\ncompare_performance(m, m1, rank = T)\n\n\n# Comparison of Model Performance Indices\n\nName |   Model |     AIC |     BIC |  RMSE | Sigma | Performance-Score\n----------------------------------------------------------------------\nm1   | lmerMod | 167.074 | 177.334 | 2.055 | 2.240 |            75.00%\nm    |      lm | 169.562 | 175.425 | 3.021 | 3.173 |            25.00%\n\nWarning: Models are not of same type. Comparison of indices might be not meaningful.\nHowever, if we compare models which are comparable, no warning will be displayed.\n\n\nm1.1 <- lmer(mpg ~  hp + cyl + wt + (1 | am), data = mtcars)\ncompare_performance(m1, m1.1, rank = T)\n\n\n# Comparison of Model Performance Indices\n\nName |   Model |     AIC |     BIC | R2 (cond.) | R2 (marg.) |   ICC |  RMSE | Sigma | Performance-Score\n--------------------------------------------------------------------------------------------------------\nm1   | lmerMod | 167.074 | 177.334 |      0.864 |      0.862 | 0.009 | 2.055 | 2.240 |            71.43%\nm1.1 | lmerMod | 164.240 | 173.035 |      0.829 |      0.827 | 0.008 | 2.345 | 2.509 |            28.57%\n\nMoreover, performance package can also easily plot this comparison, simply by wrapping up the compare_performance() function into a plot() function. It could not be easier or more elegant than that!\n\n\nplot( compare_performance(m1, m1.1) )\n\n\n\n\nUse individual quality indicators\n\n\nr2(m1)    # model fit - the proportion of the variance explained\n\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.864\n     Marginal R2: 0.862\n\nr2(m2)    # Bayesian R2 even with Credible Intervals\n\n\n# Bayesian R2 with Standard Error\n\n  Conditional R2: 0.862 (89% CI [0.799, 0.917])\n     Marginal R2: 0.856 (89% CI [0.761, 0.925])\n\nicc(m2)   # Intraclass Correlation Coefficient (ICC)\n\n\n# Intraclass Correlation Coefficient\n\n     Adjusted ICC: 0.858\n  Conditional ICC: 0.476\n\n# not part of the \"performance\" package, but fits well here\nAIC(m1)   # Akaike's Information Criterion\n\n\n[1] 167.0742\n\nBIC(m1)   # Bayesian Information Criterion\n\n\n[1] 177.3343\n\nCheck assumptions: or modern (Jan 2020) model diagnostics\nAll assumptions at once!\nThe first time I discovered check_model() function, I have got an intensive intellectual ‚Äúnerdgasm‚Äù. I could not believe how simple and at the same time sophisticated this function is! I then checked out other work of the authors of {performance} package and was stunned, how many useful things they already produced. And I don‚Äôt think they‚Äôll stop any time soon. Thus, if you just began to learn R and stats, check out their work to quickly step up your data science game!\n\n\ncitation(\"performance\")\n\n\n\n  L√ºdecke et al., (2021). performance: An R Package for\n  Assessment, Comparison and Testing of Statistical Models.\n  Journal of Open Source Software, 6(60), 3139.\n  https://doi.org/10.21105/joss.03139\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {{performance}: An {R} Package for Assessment, Comparison and Testing of Statistical Models},\n    author = {Daniel L√ºdecke and Mattan S. Ben-Shachar and Indrajeet Patil and Philip Waggoner and Dominique Makowski},\n    year = {2021},\n    journal = {Journal of Open Source Software},\n    volume = {6},\n    number = {60},\n    pages = {3139},\n    doi = {10.21105/joss.03139},\n  }\n\nNow the function itself: it visually checks all the assumptions you need to check and gives you a big-picture overview of assumptions for almost any model you can have (at least for all the common ones). Two examples below display such a big-picture of a usual linear model and a mixed-effects model with random effects. The subplots of this picture even explain what you should look for! For instance: ‚ÄúDots should be plotted along the line‚Äù in the residuals diagnostics plots. Have a look at the big-picture first and go to the next chapter for some more details on the particular assumption. Why? Because the package provides the opportunity to check individual assumptions too and even goes one step deeper into it!\n\n\ncheck_model(m)\n\n\n\n\n\n\ncheck_model(m1)\n\n\n\n\nIndividual assumptions with reports of statistical tests!\nChecking individual assumption is also very intuitive. For instance, for checking the normality of the residuals, use check_normality() function. It will conduct the Shapiro-Wilk Normality test and report the result of it. Put check_normality(m) inside of the plot() function to visualize the result. It is even preferable to visually inspect the residuals, because Shapiro-Wilk Test will often produce significant results for large sample sizes (and in the age of big data we always have large samples) even if data is perfectly normally distributed.\n\n\ncheck_normality(m)     # shapiro.test, however, visual inspection (e.g. Q-Q plots) are preferable\n\n\nOK: residuals appear as normally distributed (p = 0.212).\n\ncheck_normality(m1)\n\n\nWarning: Non-normality of residuals detected (p = 0.010).\n\nplot( check_normality(m1) )\n\n\nWarning: Non-normality of residuals detected (p = 0.010).\n\n\nThe collinearity is measured by the variance inflation factor (VIF). VIF<5 is acceptable for individual predictors, while VIF<10 is moderate, so, that it gives you an idea that some variables in the model might be redundant. If you model interactions, VIF would naturally increase, and the unwritten (I forgot where did I learned it from üôà) rule is that interactions with VIF<20 are still acceptable.\n\n\ncheck_collinearity(m)  # by variance inflation factor (VIF)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Term  VIF Increased SE Tolerance\n   hp 3.26         1.80      0.31\n  cyl 3.26         1.80      0.31\n\ncheck_collinearity(m1)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Term  VIF Increased SE Tolerance\n   wt 2.50         1.58      0.40\n\nHigh Correlation\n\n   Term    VIF Increased SE Tolerance\n     hp  78.22         8.84      0.01\n    cyl  12.22         3.50      0.08\n hp:cyl 122.72        11.08      0.01\n\nplot( check_collinearity(m1) )\n\n\n\n\nIf you wanna check the heteroscedasticity, use function check_heteroscedasticity(). The heteroscedasticity assumption itself is kind of self-explanatory. But what I can‚Äôt explain is that why nobody in the history of R programming language came up with such intuitive functions before {performance}?\n\n\ncheck_heteroscedasticity(m)\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.041).\n\ncheck_heteroscedasticity(m1)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.111).\n\nplot( check_heteroscedasticity(m) )\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.041).\n\n\nOne particular function somehow didn‚Äôt finish up in the ‚Äúbig-picture‚Äù. Namely check_outliers(). May be because there are too many methods for identifying outliers (‚Äúzscore‚Äù, ‚Äúiqr‚Äù, ‚Äúcook‚Äù, ‚Äúpareto‚Äù, ‚Äúmahalanobis‚Äù, ‚Äúrobust‚Äù, ‚Äúmcd‚Äù, ‚Äúics‚Äù, ‚Äúoptics‚Äù, ‚Äúlof‚Äù)? I don‚Äôt know. Z-Scores is the default method though, however, the method can be easily specified as an argument of the function:\n\n\ncheck_outliers(mtcars$mpg)\n\n\nWarning: 4 outliers detected (cases 18, 19, 20, 28).\n\nplot(check_outliers(mtcars$mpg))\n\n\n\nplot(check_outliers(mtcars$mpg, method = \"iqr\"))\n\n\n\n\nThere are many more useful functions in this package. And there is no need to describe them all here. If you liked what you have seen so far, just type ‚Äú?performance‚Äù in the RStudio console, go to Help, scroll down to the bottom of the Help page, click Index and enjoy the package üòâ.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nCheers guys, have a good one!\nUseful references\nhttps://easystats.github.io/performance/\nhttps://cran.r-project.org/web/packages/performance/performance.pdf\nhttps://www.rdocumentation.org/packages/performance/versions/0.6.1\nOne of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com\n\n\n\n",
    "preview": "posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/14.png",
    "last_modified": "2021-06-04T18:54:32+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/",
    "title": "Survival analysis 1: a gentle introduction into Kaplan-Meier Curves",
    "description": "Survival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every ‚Äúevent‚Äù is fatal üòÉ, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-03",
    "categories": [
      "survival analysis",
      "videos"
    ],
    "contents": "\n\nContents\nPrevious topics\nWhy do we need survival analysis?\nDeath is not the only option! Or: ‚ÄúWhat is an event?‚Äù\nCensoring\n\nHow to calculate Kaplan-Meier survival curve ‚Äúmanually‚Äù step by step\nSurvival probability\n\nHow to compute Kaplan-Meier survival curve\nInterpretation of Kaplan-Meier Curve\n\nComparing survival of groups\n2 groups\nInterpretation of groups comparison using 4 benchmarks\nLog-Rank test\n> 2 groups and multiple pairwise (post-hoc) Log-Rank test\nMultiple survival curves\n\nConclusions\nWhat‚Äôs next?\nFurther readings, videos and references\n\n\n\nlibrary(tidyverse)  # data wrangling and visualization\nlibrary(sjPlot)     # for plotting results of log.regr.\nlibrary(effects)    # for probability output and plots\nlibrary(survival)   # for core survival analysis routines\nlibrary(survminer)  # for drawing survival curves\nlibrary(knitr)      # for a wonderful-looking tables\n\n\n\nIf you are more of a visual person, you can watch the video below first, but for more details and R-code have a look at the article.\n\n\n\n\n\n\n\nCan there be something more horrifying, then a Titanic crash with lots of immediate deaths? Well, unfortunately, yes! Namely, not immediate deaths. Imagine survived people in the middle of the cold dark ocean, some in life jackets, some without, clinging on ship remains, with a huge panic and little hope for rescue. There will be no helicopter to save them (it‚Äôs 1912). There might not be a single ship in those waters for months. How long will they survive? Will time simply prolong their suffering or increase the probability of survival, because eventually they will be found by some random ship? Do women have higher chances of survival then men? What about rich vs.¬†pure passengers? Well, in this post we‚Äôll find answers to all of these questions and along the way learn how survival analysis works. Particularly, we will:\nlearn about the most important concepts of survival analysis: survival curve, censoring and log-rank test,\nmanually calculate, then compute and interpret survival curves, and\ntest survival differences between two or more groups, e.g.¬†females vs.¬†males\n\nSource\nPrevious topics\nA good understanding of linear and logistic regressions would improve the digestion of this post.\nWhy do we need survival analysis?\n\n\nlibrary(tidyverse)\n\nd <- tibble(\n  time   = c(1,1,1,2,2,3,4,5,7,10), \n  status = c(1,1,1,1,0,1,0,0,1,0)\n)\n\nd %>% kable()\n\n\ntime\nstatus\n1\n1\n1\n1\n1\n1\n2\n1\n2\n0\n3\n1\n4\n0\n5\n0\n7\n1\n10\n0\n\nTo answer this question, let‚Äôs start with a small example of Titanic survivors: only 10 people and only 10 days on the sea after the crash and analyse it with the classic statistical methods, linear and logistic regressions. We are interested in how many people and how long survived and how many died on a particular day. Thus, two main parameters of interest for us are (1) time (in days) and (2) ‚Äúlife-status‚Äù of people, where ‚Äústatus = 1‚Äù means death and ‚Äústatus = 0‚Äù means not-death. And having the status of people we could just have counted survivors and not-survivors at each particular day to understand what was going on, wright?\n\n\nggplot(d, aes(time, fill = factor(status)))+\n  geom_bar(position = position_dodge())+\n  theme_bw()\n\n\n\n\nWell, not quite. Besides, the fact that the plot with our counts is not particularly revealing, we have two problems with that:\nThe first one is that ‚Äústatus = 0‚Äù does not always mean a survival. First, the person might be carried away by the waves and disappear. We lost the person and we don‚Äôt know it‚Äôs status. It might have died, it might have been saved by a fisher boat or is still alive somewhere in the ocean. We are not sure. The only thing what we are sure about is that this person can not be counted as dead or survived. Secondly, if some people survived 10 days, they might die at day 11. So, the time plays a role here and brings us to the second problem.\nWe do not have the data for every of these 10 days. Only for 7. Moreover, if I did not eat anything for 5 days, my survival probability at day 5, where I certainly survived, is not 100%. It is lower, because hunger accumulates over time. Similarly, a decrease of my survival probability also accumulates over time, which can not be determined by a simple counting of survivors vs.¬†not-survivors at a particular day.\nDespite this problems we still can analyse these data. We only need to find the right method for it.\nLooking at the numeric variable ‚Äútime‚Äù, we might be tempted to use a linear regression to model the survival time for two different status groups, 0 & 1:\n\n\nm <- lm(time ~ status, d %>% mutate(status = factor(status)))\n\nlibrary(effects)\nplot(allEffects(m))\n\n\n\n\nBut when we plot the model results, we realize that something isn‚Äôt quite right there. All the information we have to model the survival time are zeros and ones. And the average time of dying (status = 1) of 2.5 days misses lot‚Äôs of information. For instance, half of the people (3 persons) died on the very first day! And the further we go in time, the less additional people die, which we can see on the very first counts-plot above. So, the survival time is not really linear and can certainly not be described with only zeros and ones! Thus, a linear model does not seem to be useful here. Then what is?\nSince we have zeros and ones in the status column we can use a logistic regression, right?! Yes! Especially because probability curve is not linear and will catch the trend of survival over time. Cool! Let‚Äôs go with it:\n\n\nm <- glm(status ~ time, d, family = binomial)\n\nlibrary(sjPlot)\nplot_model(m, type = \"pred\", ci.lvl = NA)\n\n\n$time\n\n\nNow we have the non-linear survival probabilities :), which is a great improvement as compared to a linear regression. However, the fifth observation has a status of 0 at day 2. This means that the person survived for 2 days for sure, but was lost after it. ‚ÄúSo what?‚Äù, - you might ask. The probabilities in logistic regression would count this person as a 100% survivor after the second day simply because it‚Äôs status is not ‚Äú=1‚Äù, which is wrong (or biased), since we don‚Äôt know whether this person survived. Thus, logistic regression overestimates survival probability and is therefore also an inappropriate tool to analyze survival data.\nSo, it looks like we can‚Äôt analyse survival data with classic methods. And that is exactly why we need survival analysis method.\nWhile linear regression models describe the time, but miss a non-linear survival probability, logistic regression catches a non-linear trend, but overestimates survival probability. In contrast, survival analysis solves both issues, since it models non-linear survival probabilities over time while accounting for lost subjects of the study which are either dead nor survived.\nDeath is not the only option! Or: ‚ÄúWhat is an event?‚Äù\n\nImage by Luke Southern on unsplash.\nSurvival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every ‚Äúevent‚Äù is fatal üòÉ, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.\nThe event, as a final fixed point in time is needed because we can‚Äôt observe or experiment forever. If we study lung cancer, we can‚Äôt wait until patients die from other causes, e.g.¬†being old. Only when some of patients survive cancer, or some cars don‚Äôt break by a certain time (event), we can get valuable insights. For instance, what is the time from the start of the treatment to progression, or what is the probability to survive lung cancer after exactly 1 year? Moreover, it opens the possibility to compare survival or failure times among different groups, e.g.¬†lung cancer between smokers and non-smokers, or breakdown time between German and Korean cars.\nSo, the survival time has a start and a finish point - the event. But what if a patient withdraw from the study due to a personal reasons, or a car got stolen 1 day before the event? Will they ‚Äúsurvive‚Äù by the time of the event? We don‚Äôt know! But they certainly survived from the start of the study until the point we lost them. And this is a valuable information we surely want to include in our analysis! But how do we differentiate ‚Äúlost‚Äù survivors from the ‚Äúreal‚Äù survivors? Well, we just call them a new name - censored. Str–∞nge word to apply to a person, right? But as I thought about the inappropriate information in a book or swearing on TV, which is often getting censored, the concept of censoring a patient became more digestible to me. See, since we can not say that person has died or is still alive, simply because we DO NOT HAVE ENOUGH INFORMATION, both of conclusions would be inappropriate, and thus, we censor this person. The concept of censoring is so important, that it deserves an extra chapter.\nCensoring\n\nThe need for censoring arises from the fact that the actual survival times will be unknown for some individuals. There are lots of ways to loose a subject of study:\nwe can loose an individual for no reason, when it simply does not appear anymore\nthe individual may experience another event, e.g.¬†death or accident\neven patients who survived till the very end of the study can be treated as censored due the unknown survival time (I personally prefer to see these patients as survived)\nAll examples above are considered to be right censoring due to their direction from left to right on the time-axes. Most of survival data are right censored. There are two other kinds of censoring. Left censoring appears if we do not know where the sickness began, while interval censoring happens when the exact time of patient loss is not known, but only a time window. Both left and interval censorings are rare, difficult to analyse, are often a result of a bad study design, and thus, will not be covered here.\nImportant to remember is that censored observations still provide useful information! That is why they need to be included into analysis.\n\nHow to calculate Kaplan-Meier survival curve ‚Äúmanually‚Äù step by step\nImagine a peaceful sunny day right before the Titanic crash. No one has died, but all 10 people are at high risk (hazard) of death, they just don‚Äôt know it yet. Guess, how many people would probably die the day before the crash? The answer is - probably zero. Literally, the probability of dying is zero, because we know the crash will not happen the day before the actual crush. To answer this question more properly, we need to remember the definition of probability: Probabilities are ratios of something happening, to everything what can happen. Thus, if nobody out of all 10 people died the day before the crash, the probability of dying is 0%, while the probability of surviving is 100%:\n\\[ probability \\ of \\ dying = \\frac{0}{10} = 0\\]\n\\[ probability \\ of \\ surviving = \\frac{10}{10} = 1  = 100\\%\\]\nFor a better representation, let‚Äôs put all the numbers in one single table:\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\", \"10/10 = 1\"\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n\nThen, at the day of the crash, where all 10 people are now know that they are at high risk of dying, 3 of them have actually died. The probability of dying at the first day is then \\(3/10 = 0.3 \\approx 30\\%\\) and the probability of surviving is \\(7/10 = 0.7 \\approx 70\\%\\). As you can see, the survival can be also calculated from the probability of dying \\(1-0.3 = 0.7\\), which is sometimes very useful.\nSurvival probability\n\nOne important moment here is that the probability of surviving is cumulative, which means it accumulates day by day. This accumulation happens via multiplying the new probability of surviving day 1 \\((1 - \\frac{d_i}{n_i}) = (1 - \\frac{3}{10}) = 70\\%\\) by the old probability of surviving all the time before, which in our case is the day before the accident, or - day zero \\(S(t_{i-1}) = 100\\%\\):\n\\[ S(t_i) = S(t_{i-1})*(1 - \\frac{d_i}{n_i}) = 1 * (1 - \\frac{3}{10}) = 0.7 \\]\nWhere,\n\\(S(t_{i‚àí1})\\) = the probability of being alive at \\(t_{i‚àí1}\\)\n\\(n_i\\) = the number of patients alive just before \\(t_i\\)\n\\(d_i\\) = the number of events at \\(t_i\\)\n\\(t_0\\) = 0, \\(S(0)\\) = 1\nThe survival probability at a certain time, \\(S(t)\\), is conditional because the person needs to have survived beyond that certain time, e.g.¬†the zero day (that‚Äôs the condition) in order to remain in the experiment for the first day.\n\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\",   \"10/10      = 1\",\n  1, 10, 3, \"3/10 = 0.3\", \"1 * (7/10) = 0.7\",\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n1\n10\n3\n3/10 = 0.3\n1 * (7/10) = 0.7\n\nThe second day is a little more interesting, because one person died and one simply disappeared (status = 0). This lost person was most likely carried away by the waves and hopefully was rescued. The hope is big, because the further people are scattered in the ocean, the higher the chances are that one of them will be found alive. And this one will let the world know that others are still out there. So, despite the fact that missing person seems bad, it might turn out to be a good thing. So, we certainly can not count a missed person as - dead. Besides, as a German proverb says: ‚ÄúHope is the last to die‚Äù üòâ.\nBut since we are also not sure whether this person is still alive, we can‚Äôt say - the person survived. That produces a dilemma: despite not being dead, the person is not part of our experiment anymore and we have to remove it from the number of people at risk. So, while the second day would have 7 people at risk left (since 3 have died at the very first day), the third day would be left with only 5 people, because 1 person died and one disappeared (was censored) as compared to day two. Similarly to the first day, the survival probability is cumulative and always includes the probability of the day before.\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\",    \"10/10       = 1\",\n  1, 10, 3, \"3/10 = 0.3\",  \"1 * (7/10)  = 0.7\",\n  2, 7,  1, \"1/7  = 0.14\", \"0.7 * (6/7) = 0.6\",\n  3, 5,  1, \"1/5  = 0.20\", \"0.6 * (4/5) = 0.48\"\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n1\n10\n3\n3/10 = 0.3\n1 * (7/10) = 0.7\n2\n7\n1\n1/7 = 0.14\n0.7 * (6/7) = 0.6\n3\n5\n1\n1/5 = 0.20\n0.6 * (4/5) = 0.48\n\nThe next two days two people disappeared, so, we censor them. There is no need to calculate something for censored data in survival analysis, because we don‚Äôt know whether they survived or not. And since we are interested in either survival or death, the final table below contains only dead cases. At day 6 nobody died or disappeared, that is why we do not calculate anything for that day either. The last person in our experiment died at day 7. So, ‚ÄúN_died‚Äù will be 1, and ‚ÄúN_at_risk‚Äù will be 2, since 1 out of remaining 5 has died and 2 were censored. Thus, the full ‚Äúmanually‚Äù calculated table will look like this:\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\",    \"10/10        = 1\",\n  1, 10, 3, \"3/10 = 0.3\",  \"1 * (7/10)   = 0.7\",\n  2, 7,  1, \"1/7  = 0.14\", \"0.7 * (6/7)  = 0.6\",\n  3, 5,  1, \"1/5  = 0.20\", \"0.6 * (4/5)  = 0.48\",\n  7, 2,  1, \"1/2  = 0.50\", \"0.48 * (1/2) = 0.24\"\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n1\n10\n3\n3/10 = 0.3\n1 * (7/10) = 0.7\n2\n7\n1\n1/7 = 0.14\n0.7 * (6/7) = 0.6\n3\n5\n1\n1/5 = 0.20\n0.6 * (4/5) = 0.48\n7\n2\n1\n1/2 = 0.50\n0.48 * (1/2) = 0.24\n\nBut what happens if we ignore censoring and simply calculate the probability of being not dead? Well, since 4 people did not die on day 7, the probability of survival would be \\(\\frac{4}{10} = 40\\%\\) which is almost twice as high, and thus we‚Äôll massively overestimate survival probability, as compared to the actual probability of 0.24 which accounts for censoring!\n\nStory time:\n\nSource\nOnly paying attention to survivors even has a name - survivorship bias, and there is a small story to it. During the Second World War some planes came back from the battle field with a lot of damage from bullets. They barely could fly, but they still came back. So, the military decided to protect the aircraft with more armor at the places where the most bullet-holes were, like wings, and reduce the armor at the places with no bullet-holes. Surprisingly, the percentage of planes which came back did not increase. The engineers were puzzled! Until one mathematician, Abraham Walt, which were invited to solve this problem said: ‚ÄúPut more armor on places with no bullet-holes, because if these place are shot, the plane won‚Äôt come back.‚Äù And as the others thought about it, they realized that all the planes which came back did not have any bullet-holes on the cockpit or the engines. The bullet-holes shows all the places where the aircraft can be shot but still come back, or survive! That is the survivorship bias. So, it seems to me, that logistic regression has a survivorship bias as compared to the survival analysis if we wanna analyse survival data.\n\nHow to compute Kaplan-Meier survival curve\nLoad all needed packages at once to avoid interruptions.\n\n\nlibrary(tidyverse)  # data wrangling and visualization\nlibrary(knitr)      # beautifying tables\nlibrary(car)        # for checking assumptions, e.g. vif etc.\nlibrary(broom)      # for tidy model output\nlibrary(sjPlot)     # for plotting results of log.regr.\nlibrary(sjmisc)     # for plotting results of log.regr.\nlibrary(effects)    # for probability output and plots\n\n\n\nFirst of all, please, be sure you know exactly what 0s and 1s in your data mean! Because in a logistic regression 1 is the survival, while in a survival analysis 1 is death! Secondly, we have to differentiate censored cases somehow, e.g.¬†we can mark them with a plus sign in the data or on the plot. The data below shows that people were censored on days 2+, 4+, 5+ and 10+. Install and load a survival package to be able to execute the code below:\n\n\n# install.packages(\"survival\")\nlibrary(survival)\n\nSurv(time = d$time, event = d$status)\n\n\n [1]  1   1   1   2   2+  3   4+  5+  7  10+\n\nThe Surv function unites ‚Äútime‚Äù and ‚Äústatus‚Äù data into a single ‚Äúsurvival‚Äù object, which allows to account for censored observations. This object can then be used to model survival probability by the survfit function. We model the survival by adding ‚Äú~ 1‚Äù to the object, where 1 means no variables which could have influenced the survival. Our survival object on the left side of the tilde is then the response variable and on the right site of the ~ (tilde) are predictors, or in our case of ‚Äú1‚Äù - nothing. Let‚Äôs produce our fist survival model and have a look at the model output:\n\n\nsurvival_model <- survfit(Surv(time, status) ~ 1, data = d)\n\n# small summary\nsurvival_model\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n      n  events  median 0.95LCL 0.95UCL \n     10       6       3       1      NA \n\nwhere we have:\nn - number of participants,\nevents - number of deaths, which are generically called event since you can study other types of ‚Äúdeadlines‚Äù üòÇ, like relapse of the sickness, or first break of the car,\nthe median survival time estimated instead of the mean due to a non-parametric (explained later) nature of survival analysis and\n95% confidence intervals for the median survival time\nMuch more information can be assessed by the summary function, which, surprisingly üòâ, delivers the same table which we just calculated above manually:\n\n\n# big summary\nsummary(survival_model)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     10       3     0.70   0.145       0.4665        1.000\n    2      7       1     0.60   0.155       0.3617        0.995\n    3      5       1     0.48   0.164       0.2458        0.938\n    7      2       1     0.24   0.188       0.0515        1.000\n\nNow you can be sure, our calculations were correct and you have successfully learned how to do survival analysis. Congrats! So that from now on you can start using software. It has a lot of advantages! For instance, a summary function, which only displays the result of dead cases, can be filled with an additional argument censored = TRUE, which displays all, dead and censored cases:\n\n\nsummary(survival_model, censored = T)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     10       3     0.70   0.145       0.4665        1.000\n    2      7       1     0.60   0.155       0.3617        0.995\n    3      5       1     0.48   0.164       0.2458        0.938\n    4      4       0     0.48   0.164       0.2458        0.938\n    5      3       0     0.48   0.164       0.2458        0.938\n    7      2       1     0.24   0.188       0.0515        1.000\n   10      1       0     0.24   0.188       0.0515        1.000\n\nGreat! Right? But what if you have thousands of days (your table would be huge!), but you are interested in only a few of them? Well, you can specify the results of which day you want to see by adding a times = ... argument to the summary function. A cool üòé thing about it is that you can even ask for the days, where no data were available for, e.g.¬†day 8 or 9 in our simple example. For instance, the probability of survival beyond day 8 is 24%:\n\n\nsummary(survival_model, times = c(8, 9))\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    8      1       6     0.24   0.188       0.0515            1\n    9      1       0     0.24   0.188       0.0515            1\n\nNow, since we know the exact probability of survival on each day, even on days we did not have the data for, we can visualize the model results. Install and load a survminer package to be able to execute the code below:\n\n\n# install.packages(\"survminer\")\nlibrary(survminer)\n\nggsurvplot(\n  survival_model, \n  conf.int = FALSE, \n  surv.median.line = \"hv\",\n  xlab = \"Days\", \n  ylab = \"Survival probability\", \n  break.time.by = 1,\n  risk.table = T\n)\n\n\n\n\nDespite the fact that we could have easily calculated the probabilities for every day, you‚Äôll never do it by hand. It could be thousands of day. Thus, you‚Äôll let the software do the work and you‚Äôll get much more results then you can and want calculate yourself, e.g.¬†confidence intervals (CIs) for everyday survival or the survival plot. The CIs are visualized per default, but we can remove them if needed via the conf.int = FALSE command. The last row in the code above displays a ‚ÄúNumber at risk‚Äù table, which we also calculated manually above. So, why did we calculate something manually at all then, if we can get everything and more from the software? Well, it was important to go through the calculation process step-by-step in order to increase your intuition about the survival analysis curve!\nInterestingly, this curve was independently described by two different scientists at the same time. Edward Kaplan and Paul Meier then published their findings together as the Kaplan-Meier (KM) estimator  in 1958.1 And survfit function fits type = \"kaplan-meier\" curve by default.\nMost of the survival data shows a lot of events in the beginning and lower number of events throughout the time. This makes the survival curve non-linear and most the survival data skewed or non-normally (not bell shaped) distributed. These are the reasons why KM method estimates median survival time instead of the mean as a measure of central tendency. The non-linearity and the ‚Äústepiness‚Äù of the KM curve make it impossible to summarize the data into one single parameter, e.g.¬†the slope, which makes the KM method non-parametric.\nInterpretation of Kaplan-Meier Curve\nThe x-axis represents time in days, and the y-axis shows the probability of surviving or the proportion of people surviving. So, the curve itself shows the exact survival probability over time. A vertical drop in the curve indicates at least one event. The height of a vertical drop shows the change in cumulative survival probability. A horizontal part of the curve represents survival duration for the certain time interval, which is terminated by the next event (and drop of the curve). The KM curve looks like a strange staircase with uneven steps, where survival probability is constant between the events, and is therefore a step function that changes value only at the time of each event. In this way each patient contributes valuable information to the calculations for as long as it is alive. Censored people are shown exactly like in the survival object, with pluses, as you can see on the day 2. However, most of the pluses look like vertical ticks, since they lie on the horizontal part of the curve, i.e.¬†days 4 and 5. The tick marks are shown by default, but could be suppressed using the argument censor = FALSE. The risk table below the plot shows the number of people at risk, which are actually all ‚Äúreally alive‚Äù people in the experiment which did not experience the event or censoring at a particular time point. Dashed line represents the median survival time which corresponds to a survival probability of 50%. And if we ignore censoring and simply estimate the median of time (for only dead people), we‚Äôll get 1.5 instead of 3, which will increase the survival provability from 48% to 70%. Again, ignoring censoring will result into overestimation of survival probability due to a survivorship bias.\n\n\nd %>% \n  filter(status == 1) %>% \n  summarize(median_survival_whithout_censoring = median(time))\n\n\n# A tibble: 1 x 1\n  median_survival_whithout_censoring\n                               <dbl>\n1                                1.5\n\nComparing survival of groups\n2 groups\nNow, let‚Äôs follow 100 people after Titanic crash instead of 10 and look at their survival using the Kaplan-Meier curve below. The numbers in the risk table are getting bigger and we‚Äôd better display the percentages at risk in brackets near the absolute values. The risk.table = \"abs_pct\" argument helps with that. The ‚Äúround‚Äù example of 100 people gives us exactly the same percentages as the absolute numbers, however with a less ‚Äúround‚Äù number or several groups the percentages would become very useful. The median survival time of people in the cold ocean is around 70 days and the confidence intervals aren‚Äôt very wide, so that we can be pretty confident in our numbers:\n\n\nset.seed(999) # for reproducible example\nd <- ggstatsplot::Titanic_full %>% \n  mutate(survived = ifelse(Survived == \"No\", 1, 0),\n         time     = runif(n=2201, min=1, max=100)) %>% \n  sample_n(100) \n\nm <- survfit(Surv(time, survived) ~ 1, data = d)\n\nggsurvplot(m, \n           conf.int = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\")\n\n\n\n\nBut can we have two survival curves on the same plot and compare them somehow? Of coarse! In fact, that is the moment where the fun starts. For instance, we can compare survival probabilities of males vs.¬†females. For this we only need to replace ~1 in the model formula with the name of a categorical variable of interest, e.g.¬†sex. Survival plot then displays a Kaplan-Meier curve for every category of your variable:\n\n\nm <- survfit(Surv(time, survived) ~ Sex, data = d)\n\nggsurvplot(m, \n           pval = TRUE, \n           conf.int = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\") \n\n\n\n\nInterpretation of groups comparison using 4 benchmarks\nthe visual comparison of curves: just by looking at two groups we can say whether there is a difference. For instance, our plot reveals that females have higher probability of survival almost thought the whole time period (x-axes). However, whether this difference is statistically significant requires a formal statistical test. We could go further and look at the numbers of the curve, for instance: at the sunny and beautiful day before the crush, the survival probability of both groups is 1.0 (or 100% of the passengers are alive). At day 50, the probability of survival of females is ca. 0.75 (or 75%) and only ca. 0.65 (or 65%) for males. At day 75, the survival is ca. 60 and ca. 30% accordingly.\ncomparison of confidence intervals (CIs): overlapping CIs show that survival of males and females is not too different and could be due to chance all the way to ca. 80 days. After 80 days CIs stop overlapping which suggests significant difference in survival at a particular time point, say 90 days.\nestimated median survival times reveals more then confidence intervals. The median survival time for each group represents the time at which the survival probability is 50%. For instance the median survival of Females is much higher (87 days) then males (65 days). Such a huge difference of 23 days sounds significant to me, because females have 23 more days to be found by some fishing boat. Here again, only test can tell and that is why the last benchmark for comparing two groups is the p-value estimated by a Log-Rank test (Peto et al, 1977), which is so important, that it deserves an extra chapter.\n\n\nm\n\n\nCall: survfit(formula = Surv(time, survived) ~ Sex, data = d)\n\n            n events median 0.95LCL 0.95UCL\nSex=Female 31     10   87.0    60.9      NA\nSex=Male   69     54   64.5    53.6    72.9\n\nLog-Rank test\nA non-parametric Log-Rank (sometimes called Mantel-Haenszel) statistical test compares median survival times of groups. Log-Rank test is similar to a (1) non-parametric Wilcoxon-Rank test, which also compares medians using ranks and it is also similar to a (2) Chi-square test, where we compare the observed number of events to the expected ones by calculating Chi-square statistic:\n\\[ X^2 = \\sum_{i = 1}^{g} \\frac{(O_i - E_i)^2}{E_i}\\]\nThe expected numbers of events are calculated for each time point and each group as compared to the previous time point. These values are then summed over all time points to give the total expected number of events in each group.\nThe non-parametric nature of the test makes no assumptions about the survival distributions. So, is survival time normal (‚Äúbell curvy‚Äù)? It can be, but does not have to! In fact, survival data are very rarely normally distributed, but are often skewed due to a typically many early events and relatively few late ones.\nThe null hypothesis of the Log-Rank test is that there is no difference in survival between the two groups. The p-value of 0.011 allows us to reject the null hypothesis and indicates a significant median difference in survival time between females and males.\nThe Log-Rank test is soo widely used for comparing two or more survival curves, that you actually have to heavily justify the usage of any other test.\nThe function survdiff() computes Log-Rank test and returns following components:\nthe number of subjects in each group.\nthe weighted observed number of events in each group.\nthe weighted expected number of events in each group.\nthe Chi-square statistic for a test of equality\nand the p-value for the difference in survival among the groups\n\n\nsurvdiff(Surv(time, survived) ~ Sex, data = d)\n\n\nCall:\nsurvdiff(formula = Surv(time, survived) ~ Sex, data = d)\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nSex=Female 31       10     19.3      4.45      6.46\nSex=Male   69       54     44.7      1.92      6.46\n\n Chisq= 6.5  on 1 degrees of freedom, p= 0.01 \n\nIs the Log-Rank test perfect? Unfortunately, no. One problem is: it does not provide an effect size which leaves us with only p-value as a measure of difference. Another problem with Log-Rank test (and Kaplan-Meier method in general) is that it does not allow confounders, it ignores other factors/variables. Fortunately, we still can compare more then two groups of a single variable üòâ.\n> 2 groups and multiple pairwise (post-hoc) Log-Rank test\nLog-Rank test can compare more then two groups and say whether there is a significant (p-value < 0.05) difference among these groups. The test results displayed in the table and KM curve below show that there is a significant difference in survival (p-value = 0.024) among groups of people in different ticket classes. However, like the most other tests (e.g.¬†ANOVA) it does not say between which groups exactly. That is why we need an additional analysis which pairwisely compares each group to each other group. Such analysis is often called a post-hoc.\n\n\nsurvdiff(Surv(time, survived) ~ Class, data = d)\n\n\nCall:\nsurvdiff(formula = Surv(time, survived) ~ Class, data = d)\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nClass=1st  14        4    11.03     4.477     5.532\nClass=2nd  13        5     8.49     1.432     1.666\nClass=3rd  37       30    21.81     3.074     4.760\nClass=Crew 36       25    22.68     0.238     0.383\n\n Chisq= 9.5  on 3 degrees of freedom, p= 0.02 \n\nm <- survfit(Surv(time, survived) ~ Class, data = d)\n\nggsurvplot(m, \n           pval = TRUE, \n           conf.int = FALSE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\",\n           ncensor.plot = TRUE,\n           break.time.by = 20,\n           risk.table.y.text.col = TRUE, risk.table.y.text = FALSE) \n\n\n\n\nThree further useful plot arguments which can help to better visualize survival data are:\nbreak.time.by = 20 break the x-axis into wishful time intervals.\nrisk.table.y.text.col = TRUE and risk.table.y.text = FALSE plots bars instead of names in text annotations of the legend of risk table\nncensor.plot = TRUE displays the number of censored subjects, which helps to understand what is the cause that the risk number becomes smaller: the event or censoring.\nInterpretation\nThe median survival is ca. 90 days for the passengers in the 1st and 2nd classes, 58 days for the 3rd class and 65 days for the crew of the Titanic, suggesting a good survival of rich people, as compared to the rest. A low p-value (p = 0.024) suggests that there is a significant difference in survival among groups. Among which, only post-hoc can tell. The results of such post-hoc Log-Rank analysis can be conducted with pairwise_survdiff function form the survminer package and are displayed below. As usual, if we have multiple comparisons, we run into a risk of making a false discovery or missing an important discovery. Thus, we have to adjust the p-values in order to reduce the probability of making an error. I personally prefer the Benjamini & Hochberg (1995) adjustment method to the famous but conservative Bonferroni method:\n\n\npairwise_survdiff(\n  formula = Surv(time, survived) ~ Class, data = d, p.adjust.method = \"fdr\"\n  )\n\n\n\n    Pairwise comparisons using Log-Rank test \n\ndata:  d and Class \n\n     1st   2nd   3rd  \n2nd  0.414 -     -    \n3rd  0.028 0.139 -    \nCrew 0.095 0.317 0.414\n\nP value adjustment method: fdr \n\nThe results of the post-hoc analysis revealed that the survival of the 1st class is significantly higher as compared to the 3rd class and the Crew. And despite the fact that the median survival of the 2nd class passengers is very similar to the first, the confidence intervals of this survival are wide and do overlap with other groups a lot (not shown to avoid clutterness). That is why the 2nd class passengers do not generally have significantly higher survival time despite much higher median survival time.\nMultiple survival curves\nAs mentioned above, the Log-Rank test can not be applied to several variables. However, we still can plot survival curves of several variables in order to get some intuition for our data. Further methods, e.g.¬†Cox models, are able to estimate difference among several variables, but they are more complex than the KM-method and thus will be covered in future posts.\n\n\nm2 <- survfit( Surv(time, survived) ~ Sex + Class, data = d )\n\nggsurv <- ggsurvplot(m2, conf.int = TRUE)\n   \nggsurv$plot +theme_bw() + \n  theme (legend.position = \"right\")+\n  facet_grid(Sex ~ .)\n\n\n\n\nThe intuition we can get from the plots above is that rich women (1st and 2nd ticket classes) and (only 2) women from the crew have 100% probability of survival, while pure women in the 3rd class will die with a similar certainty as men. Heavily overlapping CIs of men survival suggests that all man will eventually die after the Titanic accident.\nConclusions\nSurvival analysis investigates the time it takes for an event of interest to occur. Most of the univariate (single variable) survival analyses uses Kaplan-Meier plots to visualize the survival curves and Log-Rank test to compare the survival curves of two or more groups.\nAdvantages:\nThe crucial advantage of survival probability curve vs.¬†logistic regression curve is accounting for censored data, which are neither dead, nor alive. Logistic regression treats all the people who didn‚Äôt die as survived, which is wrong, simply because we don‚Äôt know the survival status of a missed person. Patients leave the study due to two main reasons, they either fill so bad, that they don‚Äôt care about your experiment anymore, or they feel much better and forget your study. Some patients may simply move to the other city without saying anything. Counting all of them as survived, as logistic regression does, would overestimate the survival probability (survivorship bias) and underestimate the hazard of death. The survival analysis is therefore more precise as compared to a logistic regression, while it still catches a non-linear trend in probabilities.\nAnother advantage is the non-parametric nature of the Kaplan-Meier method, which does not have too many assumptions. In fact the only important assumption is that censoring should be non-informative. Why? More information is always better, right? Yes! But if we know why people leave the study, we could use this information as a new variable and study it‚Äôs influence on survival. The KM method would then be inappropriate, because it will miss this information. However, often we don‚Äôt know (no-info) why people leave (are censored). And in this case the KM-method squeezes the most inference out of such non-informative data.\nDisadvantages:\nThe Kaplan-Meier ‚Äúcurve‚Äù does not actually look like a curve. Oppositely to the logistic regression Kaplan-Meier method can not be described as a smooth function (curve) by a few parameters, e.g.¬†the slope or odds-ratio. That is why we need the fool table of results or a graph.\nKaplan-Meier method can‚Äôt model numeric variables, but only categorical.\nKaplan-Meier method can‚Äôt include many explanatory variables. It‚Äôs bad, because comparing groups in terms of survival may miss the effect of other factors, known as covariates or confounders, which could potentially affect the survival time of a particular group.\nRecommendations:\nalways display statistical uncertainty by including 95% CIs or/and a p-value of the Log-Rank test. Displaying CIs at a few important time points on the plot for each treatment group may sometimes be clearer then displaying them for all time points. Non-overlapping CIs indicate significant difference between groups.\nconsider cutting the x-axis. Why? Well, the eye is naturally drawn to the right part of the plot, where the time ends. However, the end of the plot contains the least amount of information and greatest uncertainty due to just a low number of remaining participants. How far in time to extend the plot? It‚Äôs up to you.\nalways display the risk table showing the numbers of patients event-free and still in follow-up in each treatment group at relevant time points.\nWhat‚Äôs next?\nSeveral methods can address the disadvantages of the Kaplan-Meier method. Particularly, exponential parametric models provide a smooth function which is able to describe the survival curve as an actual curve in a few parameters, like slope (that‚Äôs why - parametric). And Cox-Proportional-Hazard model can be extended to several variables (in progress).\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings, videos and references\nClark, T., Bradburn, M., Love, S., & Altman, D. (2003). Survival analysis part I: Basic concepts and first analyses. 232-238. ISSN 0007-0920.\nthe whole playlist of videos from MarinStatsLectures! They are amazing! https://www.youtube.com/watch?v=vX3l36ptrTU&list=PLqzoL9-eJTNDdnKvep_YHIwk2AMqHhuJ0\nsurvminer cheat sheet: https://rpkgs.datanovia.com/survminer/survminer_cheatsheet.pdf\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#part_1:_introduction_to_survival_analysis\nOne of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com\n\nKaplan, E.L. and Meier, P. (1958) Nonparametric Estimation from Incomplete Observations. Journal of the American Statistical Association, 53, 457-481. http://dx.doi.org/10.1080/01621459.1958.10501452‚Ü©Ô∏é\n",
    "preview": "posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/thumbnail_survival_1.png",
    "last_modified": "2021-06-04T19:03:32+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-02-r-package-reviews-janitor-clean-your-data/",
    "title": "R package reviews {janitor} clean your data!",
    "description": "Data Scientists spend up to 80% of their time cleaning and preparing data for analysis. \" Happy families are all alike; every unhappy family is unhappy in its own way\" ‚Äî Leo Tolstoy. \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way\" - Hadley Wickham. Thats when \"janitor\" helps to clean the mess.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-02",
    "categories": [
      "R package reviews",
      "videos"
    ],
    "contents": "\n\nContents\nGet dirty data and look at\nit\nR demo for how to clean\nyour data\nMain functions\nclean_names()\nremove_empty() &\nremove_constant()\nget_dupes()\nround_to_fraction()\nconvert_to_date()\nrow_to_names()\n\nGenerate\n‚Äúadorable‚Äù frequency table (1-, 2-, or 3-way).\nUseful references:\n\n\n\n\nlibrary(tidyverse)          # data manipulation\nlibrary(janitor)            # data cleaning\nlibrary(readxl)             # data importing\nlibrary(kableExtra)         # beautifying tables\n\n\n\nGet dirty data and look at\nit\nIf you are reading this post, you are probably already familiar with\nthe concept of tidy data. If not, have\na look at it. And you have most likely already worked with the\nmessy (dirty) data. I did, and that is why I found the\njanitor package sooo useful!\nThe messy data displayed below can be found here.\nSome of the indicators of messyness are: strange (difficult) names,\nempty columns and rows, constant columns, which do not provide much of a\nvalue, duplicates, strange dates which do not look like dates etc..\n\n\ndirty_data <- read_excel(\"dirty_data.xlsx\")\ndirty_data %>%\n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nFirst Name\n\n\nLast Name\n\n\nEmployee Status\n\n\nSubject\n\n\nHire Date\n\n\n% Allocated\n\n\nFull time?\n\n\ndo not edit! ‚Äî>\n\n\nCertification‚Ä¶9\n\n\nCertification‚Ä¶10\n\n\nCertification‚Ä¶11\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nInstr. music\n\n\nVocal music\n\n\nNA\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nComputers\n\n\nNA\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nEnglish 6-12\n\n\nNA\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPhysical ed\n\n\nNA\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nPolitical sci.\n\n\nNA\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nNA\n\n\nVocal music\n\n\nEnglish\n\n\nNA\n\n\nR demo for how to clean your\ndata\nA short (ca. 12 min) video below shows how to clean this data and the\ncode you‚Äôll see in the video is provided below.\n\n\n\n\n\n\n\nMain functions\nclean_names()\n\nThis function removes all the non-letters and signs from the names\nand connects several words with underscores. You can ignore the\nkbl() and kable_classic_2() rows in the code below,\nthey just make the HTML table look clean, but do not really clean\nanything in our dataset.\n\n\nd <- dirty_data %>% \n  clean_names()\n\nd %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ndo_not_edit\n\n\ncertification_9\n\n\ncertification_10\n\n\ncertification_11\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nInstr. music\n\n\nVocal music\n\n\nNA\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nComputers\n\n\nNA\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nEnglish 6-12\n\n\nNA\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPhysical ed\n\n\nNA\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nPolitical sci.\n\n\nNA\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nNA\n\n\nVocal music\n\n\nEnglish\n\n\nNA\n\n\nremove_empty() &\nremove_constant()\nremove_empty() removes both empty rows and empty columns. We\nhave two empty columns and one empty row. They are just useless. Lets\nadd two constant columns to the dataset and see how we can remove all\nthis junk with janitor.\n\n\n# add two constant columns\nd <- d %>% \n  mutate(constant_column   = 42,\n         constant_column_2 = \"text\")\n\n# remove the junk\nd %>% \n  remove_constant() %>%  \n  remove_empty() %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ncertification_9\n\n\ncertification_10\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nPhysical ed\n\n\nTheater\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nPhysical ed\n\n\nTheater\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nInstr. music\n\n\nVocal music\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nPENDING\n\n\nComputers\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nPENDING\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nScience 6-12\n\n\nPhysics\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nScience 6-12\n\n\nPhysics\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nEnglish 6-12\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nPENDING\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nPhysical ed\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPolitical sci.\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nVocal music\n\n\nEnglish\n\n\nget_dupes()\nYou can hunt duplicates rows in several columns. The function also\nreturns the counts for every duplicate.\n\n\nd %>% \n  get_dupes(first_name) %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\ndupe_count\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ndo_not_edit\n\n\ncertification_9\n\n\ncertification_10\n\n\ncertification_11\n\n\nconstant_column\n\n\nconstant_column_2\n\n\nChien-Shiung\n\n\n2\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\n42\n\n\ntext\n\n\nChien-Shiung\n\n\n2\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\n42\n\n\ntext\n\n\nJason\n\n\n2\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\n42\n\n\ntext\n\n\nJason\n\n\n2\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\n42\n\n\ntext\n\n\nround_to_fraction()\nThis can be very useful if you have lots of data. I do work with\nagricultural animals and they have all kinds of scores. One of them -\nBody Condition Score (BCS) is always recorded in quarters, e.g.¬†2.25,\n2.5, 2.75 etc. So, if some colleagues try to be very smart and very\nexact, they stupidly record values like 0.8 or 3.149 instead of wanted\n0.75 and 3, you need to correct this. round_to_fraction()\nsimplifies this task enormously! Have a look at the last number below,\nbefore and after applying round_to_fraction() to a\nvariable.\n\n\n# before\nd$percent_allocated\n\n\n [1] 0.75 0.25 1.00 1.00 1.00 0.50 0.50   NA 0.50 0.50   NA   NA 0.80\n\n# after\nround_to_fraction(d$percent_allocated, denominator = 4) # digits = 3\n\n\n [1] 0.75 0.25 1.00 1.00 1.00 0.50 0.50   NA 0.50 0.50   NA   NA 0.75\n\nconvert_to_date()\nA modern Excel always tries to automate things, and I hate it! üòÇ For\ninstance you write a number into a cell and it sometimes immediately\nconverts it into date. Then you try to have a date in a cell, and it\nreturns a number. Moreover, Excel also has some strange date encoding\nsystems, which can be confused with a normal numeric columns. Luckily,\nour dirty dataset has a ‚Äúdate‚Äù word in the name of a column ‚Äúhire_date‚Äù,\notherwise we wouldn‚Äôt know that it is a date:\n\n\nd$hire_date\n\n\n [1] 39690 39690 37118 27515 41431 11037 11037    NA 32994 27919 42221\n[12] 34700 40071\n\nconvert_to_date(d$hire_date)\n\n\n [1] \"2008-08-30\" \"2008-08-30\" \"2001-08-15\" \"1975-05-01\" \"2013-06-06\"\n [6] \"1930-03-20\" \"1930-03-20\" NA           \"1990-05-01\" \"1976-06-08\"\n[11] \"2015-08-05\" \"1995-01-01\" \"2009-09-15\"\n\nrow_to_names()\nPeople often have several header columns, in order to beautifully\nexplain everything, make things accurate and don‚Äôt miss any important\ninformation. I‚Äôve been there too. But as soon as I started to work with\nsoftware, I realized that this ‚Äúbeauty‚Äù hurts. I get a lot of Excel\ndatasets like that, which is not a problem, I just delete not needed\nrows and continue working. But then I get the same table with a few\ncorrected values, even if I tell colleagues to have only one header.\nThat is why I was pleased to discover row_to_names() function.\nHave a look at the dataset ‚Äúx‚Äù below and how easy can we handle it.\n\n\nx <- data.frame(\n  X_1 = c(\"some general description\", \"Real title\", 1:3),\n  X_2 = c(\"something `very!!! important` :) \", \"Which we wont!\", 4:6))\n\nx\n\n\n                       X_1                               X_2\n1 some general description something `very!!! important` :) \n2               Real title                    Which we wont!\n3                        1                                 4\n4                        2                                 5\n5                        3                                 6\n\nx %>%\n  row_to_names(row_number = 2)\n\n\n  Real title Which we wont!\n3          1              4\n4          2              5\n5          3              6\n\nGenerate\n‚Äúadorable‚Äù frequency table (1-, 2-, or 3-way).\nThe table() function is actually cool, but as I discovered\ntabyl() in the janitor packages, I couldn‚Äôt go back.\nFirst of all, I always need to explicitly write , useNA =\n‚Äúifany‚Äù if I wanna see whether there NAs. Secondly the proportions\nhave to be called with an extra function prop.table() on top of\nthe table() function. Now, look at what tabyl()\ndoes:\n\n\n# old way\ntable(d$employee_status, useNA = \"ifany\")\n\n\n\nAdministration          Coach        Teacher           <NA> \n             1              2              9              1 \n\nprop.table(table(d$employee_status))\n\n\n\nAdministration          Coach        Teacher \n    0.08333333     0.16666667     0.75000000 \n\n# new way\ntabyl(d$employee_status) # \"show_na\" is TRUE by default\n\n\n d$employee_status n    percent valid_percent\n    Administration 1 0.07692308    0.08333333\n             Coach 2 0.15384615    0.16666667\n           Teacher 9 0.69230769    0.75000000\n              <NA> 1 0.07692308            NA\n\n# new way with two variables\nd %>% \n  tabyl(employee_status, full_time)\n\n\n employee_status No Yes NA_\n  Administration  0   1   0\n           Coach  2   0   0\n         Teacher  3   6   0\n            <NA>  0   0   1\n\nMoreover, along the counts janitor‚Äôs tabyl can also display\ntotals, formatted percentages and even all of them together by using a\nfamily of ‚Äúadorable‚Äù functions.\n\n\n#  \nd %>%\n  tabyl(employee_status, full_time) %>%\n  adorn_totals(c(\"col\", \"row\"))\n\n\n employee_status No Yes NA_ Total\n  Administration  0   1   0     1\n           Coach  2   0   0     2\n         Teacher  3   6   0     9\n            <NA>  0   0   1     1\n           Total  5   7   1    13\n\nd %>%\n  tabyl(employee_status, full_time) %>%\n  adorn_totals(\"row\") %>%\n  adorn_percentages(\"row\") %>%\n  adorn_pct_formatting() %>%\n  adorn_ns(position = \"front\") %>%               \n  adorn_title() %>% \n  adorn_title(\"combined\")\n\n\n employee_status/full_time  full_time                      \n           employee_status         No        Yes        NA_\n            Administration 0   (0.0%) 1 (100.0%) 0   (0.0%)\n                     Coach 2 (100.0%) 0   (0.0%) 0   (0.0%)\n                   Teacher 3  (33.3%) 6  (66.7%) 0   (0.0%)\n                      <NA> 0   (0.0%) 0   (0.0%) 1 (100.0%)\n                     Total 5  (38.5%) 7  (53.8%) 1   (7.7%)\n\n\n\nmtcars %>%\n  tabyl(am, cyl) %>%\n  adorn_totals(c(\"col\", \"row\")) %>% \n  adorn_percentages(\"all\") %>% \n  adorn_pct_formatting() %>%\n  adorn_ns() \n\n\n    am          4         6          8       Total\n     0  9.4%  (3) 12.5% (4) 37.5% (12)  59.4% (19)\n     1 25.0%  (8)  9.4% (3)  6.2%  (2)  40.6% (13)\n Total 34.4% (11) 21.9% (7) 43.8% (14) 100.0% (32)\n\nYou could also compare columns of two dataframes to see the\ndifference. To see more function, type ?janitor in the console\nof RStudio, scroll down and press index.\n\n\nd1 <- d %>% \n  mutate(new_column = 42, \n         second_new = \"into it\")\n\ncompare_df_cols(d, d1) %>% View()\n\n\n\nEvery single function from janitor package makes your life easier and\nmore productive for a moment, some of them a lot easier,\ne.g.¬†clean_names() and remove_empty(). But the real\npower of it accumulates over time, because you free your mind and time\nfor creative work, instead of solving problems. Thus, thousand thanks to\npackage-developer Sam Firke!\n\n\ncitation(\"janitor\")\n\n\n\nTo cite package 'janitor' in publications use:\n\n  Sam Firke (2021). janitor: Simple Tools for Examining and\n  Cleaning Dirty Data. R package version 2.1.0.\n  https://CRAN.R-project.org/package=janitor\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {janitor: Simple Tools for Examining and Cleaning Dirty Data},\n    author = {Sam Firke},\n    year = {2021},\n    note = {R package version 2.1.0},\n    url = {https://CRAN.R-project.org/package=janitor},\n  }\n\nWhat is you favorite R package?\nIf you think, I missed something, please comment on it, and I‚Äôll\nimprove this tutorial.\nUseful references:\nOne of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com\n\n\n\n",
    "preview": "posts/2021-01-02-r-package-reviews-janitor-clean-your-data/11.png",
    "last_modified": "2022-04-16T14:56:27+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/",
    "title": "How to visualize models, their assumptions and post-hocs",
    "description": "A picture is worth a thousand words! This article shows how to visualize results of 16 different models in R: from a simple linear model to a multiple-additive-non-linear-mixed-effects model. Among them are logistic, multinomial, additive and survival models with and without interactions. **Goal: minimum R code & maximum output!** We'll also go a bit beyond only model visualization. So, don't miss the bonuses üòâ.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-01",
    "categories": [
      "visualization",
      "videos",
      "models"
    ],
    "contents": "\n\nContents\nA demo for the first 7 linear models\n1. Simple linear model with one categorical predictor\n2. Simple linear model with one numeric predictor\n3. Multiple linear model with several categorical predictors\n4. Multiple linear model with several numeric predictors\n5. Multiple linear model with numeric and categorical predictors\n\nBonus 1: check all the assumption in one line of code\n6. Multiple linear model with interactions\n\nBonus 2: easy post-hocs\n7. Multiple linear model with interactions bwtween numeric predictors\n\nBonus 3: quick multiple models with ggplot\nA demo for the next 9 models: non-linear, logistic, multinomial, mixed-effects, survival‚Ä¶\n8. Multiple non-linear polynomial model with interactions\n9. Multiple non-linear Generalized Additive Models (GAM)\n10. Multiple logistic regression with interactions\n\nBonus 4: visualize the post-hoc analysis with the PairWise P-value Plot (pwpp)\n11. Multinomial logistic regression models via neural networks\n12. Multiple Linear Mixed-Effects-Model with interactions\n\nBonus 5: how to choose the best model\n13. GAMMs - Multiple Generalised Additive (non-linear) Mixed Effects Models\n14. Kaplan-Meier survival model\n15. Exponential Parametric Models\n16. Cox proportional hazard models\nReferences\n\n\nA demo for the first 7 linear models\nTwo YouTube videos in this article demonstrate the code presented below and explain all you need to know. Thus, I reduced the text to a minimum in order to decrease redundancy. If you still have a question, drop me a comment on YouTube or on this article below. I‚Äôll try to respond as quick as I can.\n\n\n\n\n\n\n\nLet‚Äôs get our first data. We‚Äôll use several dataset through out this article.\n\n\n# install.packages(\"tidyverse\") \n# install.packages(\"ISLR\") \nlibrary(tidyverse) # data wrangling\nlibrary(ISLR)      # get \"Wage\" dataset\n\n# reproducibility: the same seed grows the same tree\nset.seed(1)        \n\nd <- Wage %>%\n  sample_n(1000) %>% \n  rename(salary = wage) \n\n# have a look at the data\nglimpse(d)\n\n\nRows: 1,000\nColumns: 11\n$ year       <int> 2004, 2003, 2007, 2006, 2004, 2006, 2004, 2003, 2‚Ä¶\n$ age        <int> 59, 45, 21, 49, 59, 41, 63, 42, 46, 42, 50, 26, 5‚Ä¶\n$ maritl     <fct> 2. Married, 2. Married, 1. Never Married, 2. Marr‚Ä¶\n$ race       <fct> 1. White, 1. White, 1. White, 1. White, 1. White,‚Ä¶\n$ education  <fct> 5. Advanced Degree, 4. College Grad, 2. HS Grad, ‚Ä¶\n$ region     <fct> 2. Middle Atlantic, 2. Middle Atlantic, 2. Middle‚Ä¶\n$ jobclass   <fct> 2. Information, 2. Information, 2. Information, 1‚Ä¶\n$ health     <fct> 2. >=Very Good, 2. >=Very Good, 1. <=Good, 2. >=V‚Ä¶\n$ health_ins <fct> 1. Yes, 1. Yes, 2. No, 1. Yes, 1. Yes, 1. Yes, 1.‚Ä¶\n$ logwage    <dbl> 5.204120, 4.602060, 4.565848, 5.107210, 4.490520,‚Ä¶\n$ salary     <dbl> 182.02062, 99.68946, 96.14407, 165.20877, 89.1678‚Ä¶\n\n1. Simple linear model with one categorical predictor\nThe effects() package is the main visualization package we‚Äôll use. It can visualize results of almost any model. We‚Äôll start with the visualization of predicted values.\n\n\nm <- lm(salary ~ jobclass, d)\n\n# install.packages(\"effects\")\nlibrary(effects)    # for model visualization & more\n\nplot(allEffects(m))\n\n\n\n\n2. Simple linear model with one numeric predictor\nHere we‚Äôll add grid = TRUE for a better readability, so that we don‚Äôt need to stare on the y-axis and guess the result.\n\n\nm <- lm(salary ~ age, d)\n\nplot(allEffects(m), grid = TRUE)\n\n\n\n\n3. Multiple linear model with several categorical predictors\nWe can visualize all predictors at once, or any particular predictor from a multiple model individually.\n\n\nm <- lm(salary ~ jobclass + education, d)\n\nplot(allEffects(m))\n\n\n\nplot(predictorEffect(predictor = \"education\", mod = m))\n\n\n\n\n4. Multiple linear model with several numeric predictors\nHere we‚Äôll see how to change the appearance of confidence intervals and introduce another amazing model-visualization package - sjPlot. In this chapter we‚Äôll use the effects package for the visualization of the predicted values and sjPlot package for the visualization of the estimates with their confidence intervals.\n\n\nm <- lm(salary ~ age + year, d)\n\nplot(allEffects(m))\n\n\n\nplot(allEffects(m), confint=list(style=\"bars\"))\n\n\n\n# install.packages(\"sjPlot\")\nlibrary(sjPlot)    # for model visualization\nplot_model(m)\n\n\n\n\n5. Multiple linear model with numeric and categorical predictors\nWe can change design of the plot by determining the number of rows and columns in the plot(allEffects()). The plot_model() function can also display the numeric values of the estimates and the significance stars, which is often all we need. Besides, it looks much better than a table-looking output of the model results.\n\n\nm <- lm(salary ~ age + education, d)\n\nplot(allEffects(m))\n\n\n\n\n\n\nplot(allEffects(m), rows = 2, cols = 1)\n\n\n\n\n\n\nplot_model(m, show.values = TRUE)\n\n\n\n\nBonus 1: check all the assumption in one line of code\ncheck_model is just awesome! One of my favorite R functions! I get a ‚Äúnerdgasm‚Äù every time I use it üòÇ. The video explains it very well.\n\n\n# install.packages(\"performance\")\nlibrary(performance)    # model assumptions & performance\ncheck_model(m)\n\n\n\n\n6. Multiple linear model with interactions\nFirst of all, the allEffects() functions visualizes interactions easily! Secondly, we can put several lines on the same plot with or without confidence intervals by using argument multiline = TRUE. ‚ÄúT‚Äù instead of ‚ÄúTRUE‚Äù also works. The sjPlot package is not only able to also easily visualize interactions, but can in addition be extended with the usual ggplot2 syntax, which can greatly improve the appearance of the plot.\n\n\nm <- lm(salary ~ education * jobclass, d)\n\n# not too neat representation!\nplot(allEffects(m))\n\n\n\n# better representation\nplot(allEffects(m), lines = list(multiline = TRUE))\n\n\n\n\n\n\n# perfect representation\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, type = \"int\")+theme_blank()+theme(legend.position = \"top\")\n\n\n\n\nBonus 2: easy post-hocs\nThe emmeans package is one of my favorite for conducting post-hocs. In this chapter we only display the results in the text/table form. Later we‚Äôll also visualize post-hocs.\n\n\n# install.packages(\"emmeans\")\nlibrary(emmeans)      # for post-hocs\nemmeans(m, pairwise ~ jobclass | education, adjust = \"fdr\")$contrasts\n\n\neducation = 1. < HS Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information   0.7937 8.50 990  0.093  0.9257 \n\neducation = 2. HS Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information   0.0431 4.49 990  0.010  0.9923 \n\neducation = 3. Some College:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information  -8.7985 5.33 990 -1.651  0.0990 \n\neducation = 4. College Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information  -7.8854 4.94 990 -1.597  0.1105 \n\neducation = 5. Advanced Degree:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information -21.2809 8.24 990 -2.581  0.0100 \n\n7. Multiple linear model with interactions bwtween numeric predictors\n\n\nm <- lm(salary ~ age * health, d)\n\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, type = \"int\")+\n  theme_minimal()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\ncheck_model(m)\n\n\n\n\nBonus 3: quick multiple models with ggplot\nWe do not always need to explicitly model things in order to explore our data. A geom_smooth() function from ggplot2 package will automatically fit numeric data. Mostly with non-linear models (e.g.¬†GAM), but the argument method=‚Äúlm‚Äù can force it to take the linear one. Moreover, we could also use any formula inside of the geom_smooth(), but if we need to write the formula anyway, we‚Äôd rather produce an explicit model. However, the code presented below is quick and easy, and therefore very practical! On this note we‚Äôll enter the world of non-linear models and I recommend to watch the second video first, before checking out the code.\n\n\nggplot(d, aes(age, salary))+\n  geom_point()+\n  geom_smooth()+    # the quickest way to model numeric data\n  facet_grid(education~jobclass, scales = \"free\") # quick multiple model \n\n\n\n\nA demo for the next 9 models: non-linear, logistic, multinomial, mixed-effects, survival‚Ä¶\n\n\n\n\n\n\n\n8. Multiple non-linear polynomial model with interactions\nThe allEffects function can also easily handle polynomial non-linear models. In contrast, the plot_model function can‚Äôt. However, we still can plot the estimates.\n\n\nm <- lm(log(salary) ~ poly(age, 2) * health, d)\n\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, show.values = T)+\n  theme_bw()\n\n\n\n\n\n\ncheck_model(m)\n\n\n\n\n9. Multiple non-linear Generalized Additive Models (GAM)\nThere are two main GAM packages: gam and mgcv. They both have an identical function which we need - gam(), which produces a conflict and R might be confused about it. Thus, use gam::gam() to specify from which package exactly the gam() function should be used. The gam package has it‚Äôs own plotting function - plot.Gam and you can display all subplots on one plot by using par(mfrow = c(2, 2)). Please, don‚Äôt use ‚Äúplot(allEffects(gam1))‚Äù for GAM models, since it will produce only linear results.\ns() function indicates that we would like to use smoothing splines.\n\n\n# install.packages(\"gam\")\nlibrary(gam)\n\ngam1 <- gam::gam(salary~s(year, df = 4)+s(age, df = 5)+education + jobclass, data=d)\n\npar(mfrow = c(2, 2) )\nplot.Gam(gam1 , se=TRUE, col= \"blue\")\n\n\n\n\n10. Multiple logistic regression with interactions\nInteractions in logistic regression are not a problem for both packages. Moreover, have a look at three different displays of post-hocs from the same model and find code differences. Hint: I love the ‚Äú|‚Äù part of it!\n\n\nm <- glm(health ~ jobclass * health_ins, d, family = binomial)\n\nplot(allEffects(m))\n\n\n\nplot_model(m, type = \"int\")\n\n\n\nemmeans(m, pairwise ~ jobclass | health_ins, adjust = \"fdr\")$contrasts\n\n\nhealth_ins = 1. Yes:\n contrast                       estimate    SE  df z.ratio p.value\n 1. Industrial - 2. Information  -0.0553 0.177 Inf -0.312  0.7552 \n\nhealth_ins = 2. No:\n contrast                       estimate    SE  df z.ratio p.value\n 1. Industrial - 2. Information  -0.1830 0.241 Inf -0.760  0.4472 \n\nResults are given on the log odds ratio (not the response) scale. \n\nemmeans(m, pairwise ~ health_ins | jobclass, adjust = \"fdr\")$contrasts\n\n\njobclass = 1. Industrial:\n contrast       estimate    SE  df z.ratio p.value\n 1. Yes - 2. No    0.623 0.200 Inf 3.117   0.0018 \n\njobclass = 2. Information:\n contrast       estimate    SE  df z.ratio p.value\n 1. Yes - 2. No    0.495 0.222 Inf 2.228   0.0259 \n\nResults are given on the log odds ratio (not the response) scale. \n\n\n\nemmeans(m, pairwise ~ health_ins * jobclass, adjust = \"fdr\")$contrasts\n\n\n contrast                                     estimate    SE  df\n 1. Yes 1. Industrial - 2. No 1. Industrial     0.6232 0.200 Inf\n 1. Yes 1. Industrial - 1. Yes 2. Information  -0.0553 0.177 Inf\n 1. Yes 1. Industrial - 2. No 2. Information    0.4402 0.226 Inf\n 2. No 1. Industrial - 1. Yes 2. Information   -0.6785 0.196 Inf\n 2. No 1. Industrial - 2. No 2. Information    -0.1830 0.241 Inf\n 1. Yes 2. Information - 2. No 2. Information   0.4954 0.222 Inf\n z.ratio p.value\n  3.117  0.0055 \n -0.312  0.7552 \n  1.950  0.0768 \n -3.460  0.0032 \n -0.760  0.5367 \n  2.228  0.0518 \n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: fdr method for 6 tests \n\nBonus 4: visualize the post-hoc analysis with the PairWise P-value Plot (pwpp)\nAs promised, here is the visualization of the post-hocs. Notice the marginal means on the y axis and color-coded design of the plot, which connects the pairs the p-values for which were calculated and adjusted (x-axis).\n\n\npwpp(emmeans(m, ~ health_ins * jobclass), type = \"response\", adjust = \"fdr\"\n     )+theme_minimal()\n\n\n\n\n11. Multinomial logistic regression models via neural networks\nNo special treatment of the fancy neural networks needed. plot(allEffects()) just works!\n\n\n# get the data\nd <- foreign::read.dta(\"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\")\n\nm <- nnet::multinom(prog ~ ses + write, d)\n\n\n# weights:  15 (8 variable)\ninitial  value 219.722458 \niter  10 value 179.985215\nfinal  value 179.981726 \nconverged\n\nplot(allEffects(m), \n     lines = list(multiline = T), \n     confint = list(style = \"auto\"), rows = 2, cols = 1)\n\n\n\n\n12. Multiple Linear Mixed-Effects-Model with interactions\nThe mixed-effects models are very complex. But fortunately, all 4 main functions, namely plot(allEffects()), plot_model(), emmeans() and check_model() work flawlessly and simply deliver! The check_model() even checks the assumptions for the random effects! The random effects themselves can also be visualized, but they are rarely interpreted, so, why bother?\n\n\n# install.packages(\"lme4\")\n# install.packages(\"lmerTest\")\n\nlibrary(lme4)\nlibrary(lmerTest)\n\n# get the data\nset.seed(9)\nd <- InstEval %>% \n  group_by(service, studage) %>% \n  sample_n(100) %>% \n  mutate(dept = as.numeric(dept))\n\nm1 <- lmer(y ~ service * studage + (1|s) + (1|d), data=d)\n\nplot(\n  allEffects(m1), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m1, type = \"int\")+\n  theme_blank()+\n  theme(legend.position = \"top\")\n\n\n\n# post-hocs\nemmeans(m1, pairwise ~ service | studage, adjust = \"none\")$contrasts \n\n\nstudage = 2:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.447 0.186 787 2.404   0.0164 \n\nstudage = 4:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.183 0.186 783 0.983   0.3261 \n\nstudage = 6:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.120 0.184 785 0.650   0.5156 \n\nstudage = 8:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.344 0.185 782 1.858   0.0635 \n\nDegrees-of-freedom method: kenward-roger \n\npwpp(emmeans(m1, ~ service * studage), type = \"response\", adjust = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n# check model assumptions\ncheck_model(m1)\n\n\n\n\nBonus 5: how to choose the best model\nThe performance package, near the already insanely useful check_model() function, provides a compare_performance() function, which compares models with multiple quality indicators, e.g.¬†\\(R^2\\) or AIC. It is not only more informative as compared to the anova() function, which is often used for model comparison, but also works much better, because it displays a warning when two models shouldn‚Äôt be compared, while anova(m, m1) simply fails, when models aren‚Äôt supposed to be compared, but can be tricked by placing the mixed effects model (m1) first.\n\n\nm  <- lm(y ~ service * studage, data=d)\nm1 <- lmer(y ~ service * studage + (1|s) + (1|d), data=d)\n\ncompare_performance(m, m1)\n\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n# Comparison of Model Performance Indices\n\nName |           Model |      AIC |      BIC |  RMSE | Sigma |    R2 | R2 (adj.) | R2 (cond.) | R2 (marg.)\n----------------------------------------------------------------------------------------------------------\nm    |              lm | 2702.044 | 2744.205 | 1.295 | 1.302 | 0.015 |     0.007 |            |           \nm1   | lmerModLmerTest | 2709.212 | 2760.743 | 1.088 | 1.181 |       |           |            |      0.018\n\nanova(m1, m)\n\n\nData: d\nModels:\nm: y ~ service * studage\nm1: y ~ service * studage + (1 | s) + (1 | d)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm     9 2702.0 2744.2 -1342.0   2684.0                         \nm1   11 2688.7 2740.2 -1333.3   2666.7 17.389  2  0.0001675 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n13. GAMMs - Multiple Generalised Additive (non-linear) Mixed Effects Models\nThese kind of models are just madness, can be easily visualized though.\n\n\n# install.packages(\"mgcViz\")\nlibrary(mgcViz)\n\nm <- gammV(y ~ s(as.numeric(d), k = 3) + lectage, random=list(s=~1), data= InstEval %>% \n             slice(1:5000))\n\nplot(m, allTerms = T)\n\n\nHit <Return> to see next plot:\n\nHit <Return> to see next plot:\n\n\n14. Kaplan-Meier survival model\nSurvival models can be visualized with survminer package. Even some statistical details can be displayed. I still did not figured out how to visualize the post-hocs for survival analysis. It you know how, please let me know. Thus, I here simply provide the p-values of the post-hocs.\n\n\n# install.packages(\"survival\")\n# install.packages(\"survminer\")\nlibrary(survival)\nlibrary(survminer)\n\nset.seed(1)\nd <- lung %>% \n  filter(ph.ecog != 3) %>% \n  sample_n(100)\n\nm <- survfit(Surv(time, status) ~ ph.ecog, data = d)\n\n# simple plot\nggsurvplot(m)\n\n\n\n\n\n\n# fancy plot\nggsurvplot(m, \n           pval = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\")\n\n\n\n\n\n\n# post-hocs for survival analysis\npairwise_survdiff(\n  formula = Surv(time, status) ~ ph.ecog, data = d, p.adjust.method = \"fdr\"\n)\n\n\n\n    Pairwise comparisons using Log-Rank test \n\ndata:  d and ph.ecog \n\n  0       1      \n1 0.14954 -      \n2 0.00042 0.01507\n\nP value adjustment method: fdr \n\n15. Exponential Parametric Models\nThese models are very rarely used.\n\n\n# install.packages(\"flexsurv\")\nlibrary(flexsurv)    # for Parametric Survival Modelling\n\nex <- flexsurvreg(Surv(time, status) ~ factor(ph.ecog), data = d, dist=\"exponential\")\n\nggsurvplot(ex)\n\n\n\n\n16. Cox proportional hazard models\nCox models are more common as compared to the exponential models and can be visualized with a beautiful ggforest() plot.\n\n\nm <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data =  d)\n\nggforest(m, d)\n\n\n\n\nI hope you found this article useful. The are of coarse more interesting models out there. Thus, please let me know what kind of models you make and how you visualize them.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nReferences\nOne of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com\n\n\n\n",
    "preview": "posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/thumbnail_visualize_models.png",
    "last_modified": "2021-06-04T18:44:25+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/",
    "title": "How to create a blog or a website in R with {Distill} package",
    "description": "If you're not online, you don't exist. A personal webpage or a blog became the business card of the digital century. It shows who you are and what you are capable of. Thus: show, don't tell.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2020-12-26",
    "categories": [
      "R & the Web",
      "videos"
    ],
    "contents": "\n\nContents\nWhy should we use Distill?\nManual intermittent publishing (deploying)\n1. Install Distill package\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\n2.2. with one line of code\n\n3. Build your website\n4. Publish your blog (or a website) via Netlify\n5. Create a new blog-post\n\nAutomated continuous publishing (deploying)\n1. Install distill and usethis packages\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\n2.2. with one line of code\n\n3. Build your website\n4. Connect to your github\n4.1. Create new repository\n4.2. Stage and Commit\n4.3. Run ‚Äúusethis::use_github()‚Äù to connect a local repository to Github\n\n5. Publish your blog (or a website) via Netlify\n6. Create a new blog-post\n\nBlog configuration with \"_site.yml\"\nTheming - change desing and appearence of your blog\nPimp your content\nLinks\nPlots\nFigures\nTables\nAsides\nEquations\nFootnotes\nThumbnails\nCitations\n\nUseful ressources\n\nWhy should we use Distill?\nBecause it‚Äôs an easy, quick and free way to go online today. All you need to create a website or a blog is R, RStudio and Netlify account. Moreover, it requires very little programming. It might not look too fancy in the beginning (though totally enough for me), but with some effort you‚Äôll be able to re-design your website as you wish. Besides, Distill was originally aimed to make scientific online-publishing easier, so, it can‚Äôt be too bad ;). By the way, The blog you reading right now was created with Distill. How? That‚Äôs what this article is all about.\nIf you don‚Äôt have or don‚Äôt want to have a Github account (and continuous deployment), just read on. But if you are comfortable with Github and wanna continuously deploy via Github, jump to the chapter ‚ÄúAutomated continuous publishing (deploying)‚Äù.\nManual intermittent publishing (deploying)\nWhy should we even consider manual instead of automated? Well, for some non-programmers going with the continuous deployment via Github immediately might be very challenging and frustrating. I‚Äôve been there. We all know what is ‚Äúbetter‚Äù and ‚Äúmore time effective‚Äù in the long run, but the complexity might sometimes turn us off from doing something we would love to, resulting into ‚Äúit‚Äôs just not my cup of tea‚Äù attitude. Other do not even need the continuous deployment, because they publish rarely, e.g.¬†once per month, but would still like to be online. Going to the WordPress and co., which would most likely screw either code or code-output, is not always a solution. Fortunately, Distill and Netlify greatly reduce the complexity of going online. Thus, have a look at the quick (<5 minutes) video and then follow the step by step procedure described below and you‚Äôll be online with your blog in minutes.\n\n\n\n\n\n\n\n1. Install Distill package\n\n\ninstall.packages(\"distill\")\n\n\n\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\nOpen RStudio\nGo to File => New Project‚Ä¶ => New Directory => Distill Blog\n‚ÄúDirectory name‚Äù will be the name of the folder, where all the documents from your blog will be stored\nChoose the directory where this folder supposed to be stored\nGive your blog a name. You‚Äôll be able to rename it later\nClick ‚ÄúCreate project‚Äù\n2.2. with one line of code\n\n\ndistill::create_blog(dir = \"amazing blog\", title = \"A cool name of my blog\")\n# or\ndistill::create_website(dir = \"amazing website\", title = \"A cool name of my Website\")\n\n\n\n3. Build your website\nA new RStudio window with newly created files and folders (right bottom corner) will open itself. One of the folders \"_posts\" will contain all your posts. There are already some examples of it. But don‚Äôt bother about it now.\nFind the ‚ÄúBuild‚Äù tab and press ‚ÄúBuild Website‚Äù. You‚Äôll see the process of building.\nClick ‚ÄúOpen in Browser‚Äù and explore your website\nWe aren‚Äôt online yet. But very soon!\n4. Publish your blog (or a website) via Netlify\ngo the https://www.netlify.com/, sing up for Netlify (I used the Email way) and confirm your Email.\nIn Netlify you‚Äôll see a window with: ‚ÄúWant to deploy a new site without connecting to Git? Drag and drop your site folder here‚Äù. If you somehow don‚Äôt see it, find and press ‚ÄúTeam overview‚Äù.\ngo to the directory of your blog and find the \"_site\" folder\ndrag and drop the \"_site\" folder from your computed into this window\nwait a moment till the ‚ÄúProduction‚Äù tab produces green colored ‚ÄúPublished‚Äù and you‚Äôll get a funny named website in the left top corner, also green and starting with ‚Äúhttps://‚Äù. My was ‚Äúhttps://condescending-darwin-bc567f.netlify.app‚Äù :)\nclick on it\nCongrats, you are online!\ngo back to Netlify, click ‚ÄúSite settings‚Äù => ‚ÄúChange site name‚Äù\nrename your site (e.g.¬†better-name) and hit ‚ÄúSave‚Äù\nClick on the ‚Äúbetter-name.netlify.app‚Äù to make sure the name has changed and the site is still working\ngo back to RStudio to populate your blog\n5. Create a new blog-post\nrun the line below with the name of your choice\n\n\ndistill::create_post(\"My second blog post...this one will be really good :-)\")\n\n\n\na new partly pre-filled Rmarkdown document will open itself in RStudio\nfill it with some text and code as you usually do with Rmarkdown documents\nhit ‚ÄúKnit‚Äù. NOTE: you‚Äôll need to always ‚ÄúKnit‚Äù all changed or created blog-posts individually. It is the only way to update them. ‚ÄúBuild Website‚Äù would not re-render them for you, because it‚Äôs computationally expensive and prone to problems. However, the Distill-Website (we are doing Distill-Blog now) would. You can learn more about the difference between them here.\nThis new post exists only on your local computer, still not online, thus‚Ä¶\ngo back to Netlify and click ‚ÄúDeploys‚Äù tab, where you‚Äôll see another window with: ‚ÄúNeed to update your site? Drag and drop your site folder here‚Äù\ndrag and drop the \"_site\" folder there and wait till ‚ÄúProduction‚Äù tab produces green ‚ÄúPublished‚Äù\nclick on the ‚Äúbetter-name.netlify.app‚Äù to make sure the new blog-post appeared\nclick on your post and enjoy your creative work for a moment ;), then\ngo back to RStudio and repeat step 5\nAutomated continuous publishing (deploying)\nContinuous deployment is cool! But the path there can be a little prickly. This path may take a couple of hours or days (in my case üôà). But, once there, you quickly forget all the troubles and using Github with continuous deployment becomes your second nature. So, I think there are many people which deploy either still manually or already continuously. But not many of them in the middle (I might be wrong though). Thus, I will assume you already have installed Git on your computer, created a Github account and connected your RStudio to your Github. If not, but you wanna be there, I could recommend a single short free online book which helped me go through it: Happy Git and GitHub for the useR. You‚Äôll only need it once! If you ready to proceed, have a look at the quick (ca. 8 minutes) video and then follow the step by step procedure described below and you‚Äôll be online with your blog in minutes.\n\n\n\n\n\n\n\n1. Install distill and usethis packages\n\n\ninstall.packages(\"distill\")\ninstall.packages(\"usethis\")\n\n\n\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\nOpen RStudio\nGo to File => New Project‚Ä¶ => New Directory => Distill Blog\n‚ÄúDirectory name‚Äù will be the name of the folder, where all the documents from your blog will be stored\nChoose the directory where this folder supposed to be stored\nGive your blog a name. You‚Äôll be able to rename it later\nClick ‚ÄúCreate project‚Äù\n2.2. with one line of code\n\n\ndistill::create_blog(dir = \"amazing blog\", title = \"A cool name of my blog\")\n# or\ndistill::create_website(dir = \"amazing website\", title = \"A cool name of my Website\")\n\n\n\n3. Build your website\nA new RStudio window with newly created files and folders (right bottom corner) will open itself. One of the folders \"_posts\" will contain all your posts. There are already some examples of it. But don‚Äôt bother about it now.\nFind the ‚ÄúBuild‚Äù tab and press ‚ÄúBuild Website‚Äù. Your blog will be created in a new window.\nClick ‚ÄúOpen in Browser‚Äù and explore your website\nWe aren‚Äôt online yet. But very soon!\n4. Connect to your github\n4.1. Create new repository\nGo back to RStudio and run use_git() in order to create a new local Git repository\nthen answer two questions:\n‚ÄúIs it ok to commit them?‚Äù Don‚Äôt commit by typing 3 for ‚ÄúNo‚Äù, or ‚ÄúNope‚Äù or similar.\n‚ÄúA restart of RStudio is required to activate the Git pane Restart now?‚Äù Restart by typing 2 for ‚ÄúYes‚Äù or ‚ÄúYup‚Äù.\n\n\n\nusethis::use_git() \n\n\n\nAfter restart you‚Äôll see a new ‚ÄúGit‚Äù tab appear between the ‚ÄúBuild‚Äù and ‚ÄúTutorial‚Äù tabs. That‚Äôs gut!\nclick on the ‚ÄúGit‚Äù tab and you‚Äôll see empty boxed under ‚ÄúStaged‚Äù, lot‚Äôs of yellow question marks under ‚ÄúStatus‚Äù and the file-names under ‚ÄúPath‚Äù.\n4.2. Stage and Commit\ncheck all the boxes and press ‚ÄúCommit‚Äù button, which is (vertically) between the ‚ÄúStatus‚Äù and ‚ÄúHistory‚Äù tabs. A colourful window will pop up. This window describes all the changes you are about to make to your blog.\nFind the ‚ÄúCommit message‚Äù box and definitely describe what changes you have done (e.g.¬†‚ÄúFirst commit‚Äù), because then you‚Äôll always be able to get back to the previous version, in case something stops working. That‚Äôs what they call - a version control.\npress ‚ÄúCommit‚Äù\nwait until you see the ‚ÄúClose‚Äù button and close ‚ÄúGit commit‚Äù\nforget the other pop up window and go back to RStudio.\n4.3. Run ‚Äúusethis::use_github()‚Äù to connect a local repository to Github\n\n\nusethis::use_github() \n\n\n\nA new repository will be automatically created on your Github profile and the new browser window with your Github will pop up.\nIf you‚Äôll be asked: ‚ÄúWhich git protocol to use?‚Äù, choose the one with ‚Äúhttps‚Äù and if you‚Äôll then be asked: ‚ÄúAre title and description ok?‚Äù, agree to proceed.\nNOTE: if something (e.g.¬†Github Personal Access Token) doesn‚Äôt work, get back to the Happy Git and GitHub for the useR book and work through it if you still didn‚Äôt. You‚Äôll only need it once!\n5. Publish your blog (or a website) via Netlify\nsign into your Netlify account, if have one, if not‚Ä¶\ngo the https://www.netlify.com/, sing up for Netlify either with your Email or with your Github profile.\nclick a green box ‚ÄúNew site from Git‚Äù\nconfigure Netlify on ‚ÄúGithub‚Äù,\nchoose a newly created repository, you‚Äôll recognize the name (‚Äúmy new blog‚Äù?).\nNOTE: Make sure to set the ‚ÄúPublish Directory‚Äù to \"_site\" (could be ‚Äúdocs‚Äù if you checked some boxes while creating new project). \"_site\" (or ‚Äúdocs‚Äù) contains all the information about your blog.\nclick ‚ÄúDeploy‚Äù!\nwait a moment till the ‚ÄúProduction‚Äù tab produces green colored ‚ÄúPublished‚Äù and you‚Äôll get a funny named website in the left top corner, also green and starting with ‚Äúhttps://‚Äù. My was ‚Äúhttps://condescending-darwin-bc567f.netlify.app‚Äù :)\nclick on it\nCongrats, you are online!\ngo back to Netlify, click ‚ÄúSite settings‚Äù => ‚ÄúChange site name‚Äù\nrename your site (e.g.¬†better-name) and hit ‚ÄúSave‚Äù\nclick on the ‚Äúbetter-name.netlify.app‚Äù to make sure the name has changed and the site is still working\ngo back to RStudio to populate your blog\n6. Create a new blog-post\nrun the line below with the name of your choice\n\n\ndistill::create_post(\"My second blog post...this one will be really good :-)\")\n\n\n\na new partly pre-filled RMarkdown document will open itself in RStudio\nfill it with some text and code as you usually do with RMarkdown documents\nhit ‚ÄúKnit‚Äù. NOTE: you‚Äôll need to always ‚ÄúKnit‚Äù all changed or created blog-posts individually. It is the only way to update them. ‚ÄúBuild Website‚Äù would not re-render them for you, because it‚Äôs computationally expensive and prone to problems. However, the Distill-Website (we are doing Distill-Blog now) would. You can learn more about the difference between them here.\nThis new post exists only on your local computer, still not online, thus‚Ä¶\ngo to the ‚ÄúGit‚Äù tab in RStudio and check all the boxes\npress ‚ÄúCommit‚Äù, a new window will pop up\nadd description of your commit\nagain press ‚ÄúCommit‚Äù\nwait until you see the ‚ÄúClose‚Äù button and close ‚ÄúGit commit‚Äù pop up window\npress ‚ÄúPush‚Äù. Pushing will transfer changes in your blog from your local computer to a remote place, namely your Github repository. And since your Github repository is connected to Netlify, this changes will be online after successful push.\nwait until you see the ‚ÄúClose‚Äù button and close ‚ÄúGit push‚Äù pop up window\nclose or ignore the other pop up window\nget back to your blog ‚Äúbetter-name.netlify.app‚Äù and refresh (it may take a few seconds, so, don‚Äôt panic if the first refresh don‚Äôt work). You should see a new blog-post.\nCongrats! You now continuously deploy your online blog!\nclick on your post and enjoy your creative work for a moment ;), then\ngo back to RStudio and repeat step 6 with following routine:\nCreate or change posts\nKnit\nCommit\nPush\n\nYou don‚Äôt need to commit and push every change, only important ones. Think of this process as really saving the progress you made on your blog. Another useful thing I learned to appreciate after committing a couple of thousands of changes (which is annoying!) is to - check all the boxes (under ‚Äústage‚Äù) at once! For this:\ngo to ‚ÄúTerminal‚Äù tab in RStuio (it‚Äôs near the ‚ÄúConsole‚Äù)\ntype ‚Äúgit add -A‚Äù and press enter\ncheck one of the stage-boxes, the rest of them suppose to be then check themselves automatically\nthen press ‚ÄúCommit‚Äù\nBy the way, if you don‚Äôt want to publish your post until you really satisfied with it, you can start out as a draft:\n\n\ndistill::create_post(\"Another nice post\", draft = TRUE)\n\n\n\nOr add draft: true to the post‚Äôs metadata. When you are ready, delete draft: true.\nBlog configuration with \"_site.yml\"\nOpen \"_site.yml\". You‚Äôll see something like that:\n\nname: \"Your cool webpage\"\ntitle: \"Your cool webpage title\"\ndescription: |\n  Exploring something very important.\nbase_url: https://beta.rstudioconnect.com/content/your_thing/\nnavbar:\n  logo: images/fancy_logo.png\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n    - text: \"Rest\"\n      href: rest.html\n    - icon: fa fa-rss\n      href: index.xml\noutput: distill::distill_article\n\n\"_site.yml\" is the most important document for your website. Configure it carefully and slowly. You can add a lot of useful things: categories, google-analytics, customize the navigation bar, add references, new theme with CSS code for an individual design of your site, icons of twitter & co. and much more, which is not to important for a new site and can be added at any time.\nIf you work through the Distill web page, you‚Äôll see a lot of examples for how to design your \"_site.yml\" and ‚Äútheme.css‚Äù files. But, the best way I found to do this, is just to find the \"_site.yml\" or ‚Äútheme.css‚Äù of other Distill-blogs on Github and get inspiration from them by playing with your own code (I hope nobody is offended by this sentence due to the open source nature of R, but please let me know if it‚Äôs wrong and I‚Äôll remove this recommendation!).\nWhile you already have \"_site.yml\" file in your blog-folder, you don‚Äôt have a ‚Äútheme.css‚Äù file. To get one, read on‚Ä¶\nTheming - change desing and appearence of your blog\nYou can modify the CSS code in your theme after creating it by running the following line of code:\n\n\ndistill::create_theme(name = \"theme\") \n\n\nv Created CSS file at theme.css \no TODO: Customize it to suit your needs \no TODO: Add 'theme: theme.css' to your site or article YAML\n \nSee docs at https://rstudio.github.io/distill/website.html#theming\n\nTo activate a custom theme site-wide, add a theme key to the top-level of your \"_site.yml\" configuration file:\n\nname: \"Your cool webpage\"\ntitle: \"Your cool webpage title\"\ntheme: theme.css \n(...the rest of your _site.yml)\n\nPimp your content\nMost elements enhancing your content, like links, tables, plots, equations etc., are similar to the usual R Markdown syntax. Thus for a deeper insights go to the R Markdown: The Definitive Guide book, it‚Äôs online and free. Below I just display some quick ‚Äúhow to‚Äù examples and provide links to a more thorough online resources.\nLinks\nThe links are displayed with the help of two different brackets. First, use the square brackets to produce a [clickable word or phrase], then, directly after the square brackets, use round brackets with the URL inside, e.g.¬†(https://bookdown.org/yihui/rmarkdown/). The URL by itself would certainly also work, but it‚Äôs not as convenient as this.\nPlots\n\n\nlibrary(tidyverse)\nggplot(mtcars, aes(hp, mpg)) + \n  geom_point() + \n  geom_smooth() +\n  theme_bw()\n\n\n\n\nFigures\nYou can add external static and dynamic figures, plots, photos or diagrams by using knitr::include_graphics() function. More on figures here.\n\n\nknitr::include_graphics(\"images/your_figure.png\")\n\n\n\n\n\n\n\n\n\n\n\nTables\nMore on tables here.\n\n\n# install.packages(\"gtsummary\")\nlibrary(gtsummary)\niris %>% \n  tbl_summary()\n\n\nCharacteristic\n      N = 1501\n    Sepal.Length\n      5.80 (5.10, 6.40)\n    Sepal.Width\n      3.00 (2.80, 3.30)\n    Petal.Length\n      4.35 (1.60, 5.10)\n    Petal.Width\n      1.30 (0.30, 1.80)\n    Species\n      \n    setosa\n      50 (33%)\n    versicolor\n      50 (33%)\n    virginica\n      50 (33%)\n    \n        \n          1\n          \n           \n          Median (IQR); n (%)\n          \n      \n    \n\nAsides\nYou can include notes or even plots ‚Äúaside‚Äù (to the right) your article:\n\n\n\n\nHere the funny fact or some important info.\n\n\n\nEquations\nYou can use a Latex syntax for it:\n\n$$\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}$$\n\n\\[\n\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\n\\]\nFootnotes\nFootnotes are created in the usual RMarkdown way, namely ^[here is the content of your footnote] and here are two examples 1 and 2. Point your cursor on the footnote number or scroll to the bottom of the page to see what‚Äôs inside of the footnote.\nThumbnails\nThumbnails (or previews) are the images which are displayed along the post. They serve as an eye catcher. You can add a preview image into the post‚Äôs metadata by adding a ‚Äúpreview‚Äù field:\n\ntitle: \"Blog post on how to write blog posts\"\ndescription: |\n  Here we were out of ideas, and therefore we are proud to announce\nour new post about how to write a post. Creativity is a b**ch! ... And I love her!\npreview: images/photo-of-me-because-I-am-sooo-beautifuuulll-toniiiight.png\n\nIf you don‚Äôt provide a picture for preview, the first plot or picture from your blog-post will be used as a thumbnail by default. To override this behavior, you can add the preview = TRUE in to the code-chunk, e.g.: {r some_chunk, preview=TRUE}.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nCitations\nYou either can include a BibTex-like citation of others or make your article citable (see the very bottom of this article). ‚ÄúHow to cite‚Äù is best described in the citation section of the Distill website.\nUseful ressources\nThe best place to start is actually the Distill website itself: https://rstudio.github.io/distill.\nTom Mocks blog-post on how to build blogs with Distill helped me a lot! https://themockup.blog/posts/2020-08-01-building-a-blog-with-distill/\nOne of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com\n\nHere is the first footnote, which does not suppose to interrupt the main text!‚Ü©Ô∏é\nHere is the second‚Ü©Ô∏é\n",
    "preview": "posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/images/thumbnail.png",
    "last_modified": "2021-06-04T17:21:31+02:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  }
]
