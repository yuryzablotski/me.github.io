---
title: "R demo | Paired Samples t-Test | How to conduct, visualise and interpret"
description: |
  Can one week of training significantly improve your number of sit-ups? Well, Paired t-Test can answer this question by comparing your performance Before and After this week. So, let's learn how to produce this statistically rich plot using only one simple command, how to interpret all these results and see what happens if we use a wrong test.
author:
  - name: Yury Zablotski
    url: https://yuzar-blog.netlify.app/
date: "`r format(Sys.time(), '%B %d, %Y')`"
categories:
  - videos
  - statistics
preview: thumbnail.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
# draft: true
bibliography: /Users/zablotski/Documents/library.bib
#csl: american-political-science-association.csl
biblio-style: apalike
link-citations: yes
linkcolor: blue
#draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
library(tidyverse)
library(BSDA)
```

## This post as a video

I recommend to watch a video first, because I highlight things I talk about. It's ca. 6 minutes long. 

```{r, eval=T, echo=F}
vembedr::embed_youtube("QRS2z4ZmGTQ")
```

## Previous topics

[One Sample tests](https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/) would help.

## Get the data

```{r eval=FALSE}
# install.packages("tidyverse")  # for everything ;)
library(tidyverse)

# install.packages("BSDA")       # for Fitness data
library(BSDA)

View(Fitness)
```

{BSDA} package provides a {Fitness} dataset, with the sit-up performance of 9 people **before** and **after** one week of training.

```{r}
# make wide format
d <- Fitness %>% 
  pivot_wider(
    id_cols     = subject, 
    names_from  = test, 
    values_from = number) %>% 
  mutate(difference =  After - Before)

View(d)
```


To see whether training makes any difference, we'll **calculate this difference** by subtracting performance **Before** from performance **After** and compare our **mean difference** to zero. If training didn't help, our **mean difference** would be equal to zero, which becomes our Null Hypothesis (H~0~). If training made a difference, our **mean difference** would be lower or, hopefully, higher then zero, which becomes our alternative hypothesis (H~Alt~). And since this difference is more important then paired samples themselves, we only need to check the normality of the difference, not of both samples. Checking normality is important for choosing a correct test, otherwise we could get wrong result. We'll see what happens if we choose a wrong test in a moment. Until then...

## Check normality of the difference

```{r}
shapiro.test(d$difference)
```

... a big p-value of the Shapiro-Wilk normality test indicates that our difference is **normally distributed**. That's why we need a **parametric paired t-test**, to compare two paired samples. If the difference would have been **not-normally distributed**, we would have taken a **non-parametric Wilcoxon test**, which you can learn about from [this article](https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/). 

For **paired** tests, the data **needs to be sorted**, so that the first observation of the **before** group, pairs with the first observation of the **after** group. If our data is sorter, we are ready to compute the test.

## Compute Paired Samples t-Test 

And the best way to compute our test (in my opinion) is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:

- first, **our data** - Fitness, then
- **x** - as the grouping variable,
- **y** - are the numbers of sit-ups and
- the **type** of statistical approach. Since our data was normally distributed, we choose a **parametric** test, and {ggwithinstats} automatically takes **Paired t-Test** .

Such simple command results in this statistically rich and publication ready plot! Now, let's interpret the results.

```{r}
# install.packages("ggstatsplot")
library(ggstatsplot)

set.seed(1)   # for Bayesian reproducibility
ggwithinstats(
  data = Fitness,
  x    = test, 
  y    = number, 
  type = "parametric"
)

ggsave(filename = "paired_t_test.jpg", plot = last_plot(), width = 6, height = 4)
```



## Interpret the result

- **Students t-statistics** is the **measure of similarity** between compared samples measured in units of standard error. The further *t-value* is from zero, the more different are the samples. But **t-value** by itself can not say how far from zero is far enough, to conclude that this difference is significant. That's why we need a **p-value**.


$$t = \frac{our.mean - expected.mean}{standart.error} = \frac{our.mean - expected.mean}{ \frac{standart.deviation} {\sqrt sample.size} }$$ 

- our **P-value** of 0.025 tells us that our difference IS significant. It shows a moderate evidence against the null hypothesis (H~0~) in favor of the alternative hypothesis (H~Alt~). Particularly, our group of 9 students will make 2 sit-ups more on average after one week of training. But is a difference of only 2 sit-ups large? P-value can not tell that. A P-value only tells you that there is an effect from training, but not how strong this effect is. 

![](p_value_interpretation.png)

- Fortunately, {ggwithinstats} provides **Hedges' g** as the measure of the **Effect Size**, which shows how large the effect of training is. **Hedges' g** is interpreted in the same way as **Cohen's d** Effect size, but outperforms **Cohenâ€™s d** for small samples, like 9 in our example. Our effect size of 0.83 **is large**, and tells us that the increase in performance of only 2 sit-ups is actually - a lot. But it doesn't seem like a lot. So, we have to double check it! 

![](effect_size_hedges_d.png)

- For that {ggwithinstats} provides the **Bayesian Difference** between our samples with 95% Highest Density Intervals, which is conceptually similar to the difference of 2 sit-ups we see on the plot, and the **Bayes Factor**, which is conceptually similar to the **p-value**.

- Our **Bayes Factor** (Jeffreys, 1961) of -1.13 indicates a **substantial evidence for the alternative hypothesis** - that training actually did make a difference, and we now make substantially more sit-ups then **Before** ... which IS in line with the frequentists statistics on the top of the plot. 

![](bf_interpretation.png)


- The {interpret_hedges_g} function from {effectsize} package helps to interpret this effect size and even provides the reference for interpretation if you ask R about it: `?interpret_hedges_g`. 

```{r}
# install.packages("effectsize")
library(effectsize)

interpret_hedges_g(0.83)

?interpret_hedges_g
```



## What happens if we choose a wrong test?


```{r}
ggwithinstats(
  data = Fitness,
  x    = test, 
  y    = number, 
  type = "nonparametric"
)

ggsave(filename = "paired_t_test.jpg", plot = last_plot(), width = 6, height = 4)
```

There are two ways things can go wrong. First, if we are lazy to check the normality, we'll go straight to the **non-parametric Paired Wilcoxon-Test**. Here, a p-value almost failed to reject the Null Hypothesis when we use the threshold of 0.05 to determine significance. And the effect size is lower, which means our study would be underpowered. Let me show you what I mean. 

```{r}
# install.packages("pwr")
library(pwr)

pwr.t.test(n = 9, d = 0.83, sig.level = 0.05, type = "paired")
pwr.t.test(n = 9, d = 0.81, sig.level = 0.05, type = "paired")
```

The effect size from Wilcoxon test has 2% lower power, which increases the chances to miss an important discovery. And in our example with a p-value close to the significance threshold we almost missed it.

But that's not all!!! We could be even more wrong if we took a **NOT-PAIRED t-Test**, because we would get completely opposite result, namely, that the effect of training is small and not significant. This wrong result could even seem plausible, since the difference of only 2 sit-ups is sooo small. 


```{r}
ggbetweenstats(
  data = Fitness,
  x    = test, 
  y    = number, 
  type = "parametric", 
  var.equal = T
)

interpret_hedges_g(0.3)

ggsave(filename = "usual_t_test.jpg", plot = last_plot(), width = 6, height = 4)
```


So, how can such a small difference be significant in a paired test then??? Well, it lies in the nature of the paired test - which does not compare averages of two samples, but compares a **mean difference** between those samples to zero. And if we look at the difference, which shows individual performance of students, we'll see that 7 out of 9 improved their performance, some of them by a lot, so that, the group in general was successful, and our significant p-value actually makes sense.

## Proof of the concept about the difference between two samples

```{r}
gghistostats(data = d, x = difference, test.value = 0, binwidth = 1)

ggsave(filename = "one_sample_t_test.jpg", plot = last_plot(), width = 6, height = 4)
```

Interestingly, if we test our mean difference against zero using **one-sample t-Test** and compare the t-statistics, p-value and the effect size to our **Paired t-Test**, we'll see identical result! 

Thus, our fancy **Paired Samples test** is actually **One-Sample test** on the difference. And if you wanna know more about **one-sample tests** and how to interpret all these numbers, check out [this article](https://yuzar-blog.netlify.app/posts/2021-12-20-one-sample-t-test-do-your-results-make-sense-or-how-to-compare-your-work-to-the-work-of-others/).



```{r}
# old way to do the tests
# install.packages("broom")
library(broom)

bind_rows(
  t.test(d$difference) %>% tidy(),
  t.test(d$After, d$Before, paired = T) %>% tidy()
) 
```



## One-Tailed (or One-Sided) Parametric Two-Samples Paired t-Tests

We can also ask the question, whether performance-change is positive or negative by performing **One-Tailed (or One-Sided) Parametric Two-Samples Paired t-Tests**. (I hate that name! :) 


```{r}
t.test(d$After, d$Before, paired = TRUE, alternative = "less")
t.test(d$After, d$Before, paired = TRUE, alternative = "greater")
```

Low *p-value* (p = 0.012) of the **greater-sided** test confirms that the **performance increases** after one week of training. The *p-value* of the **less-sided** test screams that your performance will not decrease with the probability of 99% (p = 0.99). In fact only one student (N = 6), which ironically was the best Before training week, decreased his/her performance by 2 sit-ups and was overtaken by two students After the week. That's the price of arrogance :)


## What's next, or Don't use Paired t-Test if

- samples are independent. In this case apply[Student's or Welsh t-test](https://yury-zablotski.netlify.com/post/two-sample-t-test-compare-your-work-to-others/)
- samples are small (n<30) and not-normally distributed. In this case use [paired two-samples Wilcoxon-test](https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/). 

- If you need to compare more then two samples, first check whether they are normally distributed, and if they are, go to [ANOVA](https://yury-zablotski.netlify.com/post/one-way-anova/), but if they aren't, go to *Kruskal Wallis*


---

If you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.

**Thank you for learning!**
