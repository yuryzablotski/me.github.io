---
title: "Master Simple Linear Regression with Numeric Predictor in R"
description: |
  Simple linear regression demonstrates how one numeric predictor affects a numeric outcome. For example, it can reveal whether age actually translates to higher paychecks. So, let's learn (1) how to build a linear regression in R, (2) how to check ALL model assumptions with a ONE simple and intuitive command, (3) how to visualize and interpret the results, and much more.
author:
  - name: Yury Zablotski
    url: https://yuzar-blog.netlify.app/
date: "`r format(Sys.time(), '%B %d, %Y')`"
categories:
  - videos
  - statistics
  - visualization
preview: thumbnail_lmnp.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
bibliography: /Users/zablotski/Documents/library.bib
# csl: american-political-science-association.csl
biblio-style: apalike
link-citations: yes
linkcolor: blue
#draft: true

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyverse)    
# library(patchwork)    
# library(sjPlot)       
# library(gtsummary)
# library(quantreg)
# library(ISLR)
# library(broom)
# library(effectsize)
# library(emmeans)
# library(flextable)
library(ISLR)

sjPlot::set_theme(theme_test())
```

# This post as ca. 12 minutes video

```{r, eval=T, echo=F}
vembedr::embed_youtube("yjdONltFwjM") 
```






# Clean the data, otherwise: shit in shit out!


```{r echo=FALSE}
#ggstatsplot::ggscatterstats(Wage, age, wage, type = "p")
ggplot(Wage, aes(age, wage))+
  geom_point()+
  geom_smooth(method = "lm")
```

The data in this picture comes from the Wage dataset in the ISLR package. It clearly shows that real-world data often contains contaminations üí©. Therefore, it‚Äôs crucial to look at the data first before rushing into modeling. For instance, in this scenario, I would clean the data by removing the high-income individuals earning over 200K per year, because they are outliers, and totally not because I am jealous! üòâ Additionally, I‚Äôd exclude older individuals who are nearing retirement, as their salaries tend to decline. The data-cleaning step is vital and often overlooked. Remember the phrase: ‚ÄòGarbage in, garbage out‚Äô? If we skip data cleaning, our model‚Äôs performance could be seriously compromised, and we'll see it in a moment. For now let's take 100 random people form the industry and build our linear model in R.

```{r}
library(tidyverse)      # laod & thank me later ;)
library(ISLR)           # provides Wage dataset
theme_set(theme_test()) # beautifies plots

set.seed(1)             # for reproducibility
d <- Wage %>% 
  filter(wage < 200 & age < 60 & jobclass == "1. Industrial") %>% 
  sample_n(100)
```


# Build linear equation


To build our linear model, we‚Äôll use the intuitive ‚Äòlm‚Äô (show pack of cigarettes ; —Å —Ä–µ–∫–ª–∞–º–æ–π –∏–º–ø–æ—Ç–µ–Ω—Ü–∏–∏ :) function. Within ‚Äòlm‚Äô, we only need two arguments: the formula and the data. Here‚Äôs how it breaks down:

- On the left side of the formula, we'll place the variable we're interested in predicting. Unfortunately, this variable goes by several names: response variable, outcome, dependent variable, target, and more. It can get confusing!

- On the fright side, we'll place the predictor variable. Predictors also have various synonyms: independent variable, explanatory variable, regressor or covariate (in linear regression), feature (in Machine Learning), and even risk factor (in epidemiology).

Ugh, so many names for the same thing! Drives me nuts. Makes stats seem way harder than it actually is.


```{r}
# build the model
m <- lm(formula = wage ~ age, data = d)
```


# Check all model assumptions visually

After building the model, we need to make sure the assumptions are satisfied. Otherwise, we couldn‚Äôt trust our model! The "check_model()" function from the {performance} package is so intuitive and powerful that, once you‚Äôve used it, you cannot unlearn it! Believe me! I use it **every time** I model.

```{r fig.height = 9, fig.width=9}
# check model assumptions visually
library(performance)   # extra video on my channel
check_model(m)
```

Looking at the assumptions, I could say that the model fits the data pretty well! The linearity assumption looks fine; the line is not completely straight, but there is no dramatic non-linearity to see. The variance is homogeneous. There are no influential points or outliers. And finally, the residuals are normally distributed since they are inside the confidence intervals, which means our data is normally distributed.

Now, when we model uncleaned data, the model fit would be far from reality, and we‚Äôll even get a warning that our model misses the maximum values ‚Äî you know, those rich folks I am totally not jealous about ;). Besides, the normality of residuals would also be compromised, so I would stop trusting my model. Therefore, it‚Äôs really important to clean the data before modeling.

```{r warning=TRUE}
m2 <- lm(formula = wage ~ age, data = Wage)
library(patchwork)
check_posterior_predictions(m2)
a <- check_posterior_predictions(m2) %>% plot() 
b <- check_normality(m2) %>% plot()

a + b
```


Oh, by the way, if you have a lot of data, it‚Äôs important to check assumptions visually, like we just did, instead of conducting statistical tests. For a lot of data, the tests would often show significant results, meaning that assumptions seemingly would not be satisfied, while they are actually satisfied. Here is an example of the Shapiro-Wilk normality test, which magically finds non-normally distributed residuals, even though they are totally fine!

```{r}
# don't trust statistical tests to much
check_normality(m)
```


Now, since the assumptions of our model are satisfied, we can visualize model results, and by results, I mean visualize model predictions.

# Visualize predictions

For that, we‚Äôll use another very intuitive function called "plot_model()" from the {sjPlot} package and provide three arguments:

- the model name,
- the type of predictions ‚Äî we‚Äôll use ‚Äúeffect‚Äù to get an effect plot, and
- the name of a predictor we want to visualize.

```{r}
# visualize predictions
library(sjPlot)    # extra video on my channel

plot_model(m, type = "eff", terms = "age") # show.data = T
```

And if we really want to, we could easily **show the data** on our plot:

```{r}
plot_model(m, type = "eff", terms = "age",  show.data = TRUE)
```

And voil√†, our plot shows that salaries increase with age. This plot is nice, but it misses two important details: it doesn‚Äôt exactly show how quickly the salary grows, and it doesn‚Äôt demonstrate whether this increase is significant.

# Visualize estimates

The "plot_model()" function solves both problems, when we use "show.values = TRUE" instead of "type = "eff".

```{r}
# visualize estimates
plot_model(m, show.values = TRUE, terms = "age")
```

Namely, it tells us that we have a 1080\$ increase in salary per year, and this increase is highly significant because the estimate doesn‚Äôt cross zero. And while the significance stars are often enough, sometimes we need an exact p-value. To obtain this, we‚Äôll use the "tab_model()" function from the same {sjPlot} package. This function produces a nice, publication-ready table that not only provides exact p-values but also reveals the equation of our linear model:

```{r}
tab_model(m)
```


This equation describes the straight-line relationship between the x-axis and the y-axis:

$$ y = Œ± + Œ≤x $$
Or, in our case, between "age" and "salary":

$$ wage = Estimate_{(Intercept)} + Estimate_{age} * age $$

... and it helps us to interpret our model. Specifically, an increase in age of one year results in an average salary increase of 1080\$. It‚Äôs important to note that this increase can vary ‚Äî sometimes it‚Äôs as low as 540\$, and other times as high as 1620\$, depending on people‚Äôs job roles. The model indicates that there is a 95% chance that the salary increase falls within this interval. That's why it's called - the 95% Confidence Interval.
  
If this average annual increase of 1080\$ occurs consistently, we obtain the slope of the line, represented by the beta (Œ≤) coefficient in the model formula. Put simply, the slope indicates the average change in "y" for every one-unit increase in "x". As with any slope, it needs a starting point. And the best start of any slope is usually zero. When our line crosses the y-axis (zero on the x-axis), we find the intercept (Œ±) in the model output. In our case, this implies an unrealistic scenario where our salary would be 57,500\$ at birth, which doesn't make any sense, and that's why the intercept of a model is often not interpreted. 

The true value of the formula lies in its ability to predict future salaries. By plugging any "age" value (x) into the model equation, we can estimate the corresponding "salary" value (y). This means we can ask the model to tell us how much we'll earn when we're 40 or 50. 



# Make specific predictions

For that we'll simply replace our alpha with the value of the Intercept 57.5 and the age with 40 or 50: 

57.50 + 1.08 * 40 = 100.7

57.50 + 1.08 * 50 = 111.5


Assuming our lives remain relatively stable, at 40 years old, we could be earning over 100,000\$, and by 50, our paychecks could exceed 111,000\$. Essentially, we've just forecast our future salaries based on the available data. Naturally, R provides several convenient functions for making such predictions, the most well-known being the "predict" function. This function allows you to input your model and a data frame containing the variable you want predictions for. However, I personally find this function somewhat unintuitive.

```{r}
# get particular predictions
predict(m, data.frame(age = c(40, 50, 60)) )
```

A much better choice is the "emmeans" function from the very powerful {emmeans} package. It not only provides predictions for any age you desire, but also reports 95% confidence intervals and offers a wide range of additional capabilities.

```{r}
library(emmeans)   # extra 2 videos on my channel ;)
emmeans(m, ~ age, at = list(age = c(40, 50, 60)))
```

Asking for predictions beyond 60 years old isn't recommended, as we intentionally limited our data to include only individuals up to 60. Here, it's crucial to differentiate between interpolation and extrapolation. Interpolation means - predicting values within existing data points, and it is very useful. However, extrapolation, which means going beyond the data range, is risky and unreliable. Predicting salary at birth (age zero) is a perfect example for a nonsensical extrapolation, because at the age of zero we can't earn anything. 


# Get effect sizes

Now, that we understand how to interpret our model, the next crucial question we need to address is - how good our model is. Your clients or scientific reviewers will definetely ask this question. What they typically want to know is - how well our model fits the data, which is summarized by the coefficient of determination, denoted as $R^2$.

$R^2$ quantifies how much variance in the data our model explains. It ranges from 0 to 1, with a simple rule: the higher the value, the better the fit. However, interpreting $R^2$ is context-dependent. For instance:

- in physics, a model with $R^2$ near 1 is usually desirable, while
- in biology, even an $R^2$ of 0.2 might indicate a good model fit.

Despite this context sensitivity, some guidelines proposed by smart individuals (such as Cohen) exist. And we can ask the {effectsize} package to interpret any value of $R^2$ for us.

```{r}
# get effect sizes
library(effectsize)
?interpret_r2

interpret_r2(0.139, rules = "cohen1988") 
# rules = "cohen1988" is a default
```


In our case, the $R^2$ value of 0.139 indicates a moderate relationship between age and salary. But why moderate and not strong? Well, that could be because age is most likely not the most crucial predictor for salary üòâ. I mean, when I sit on the sofa, do nothing, and just get older, my salary won‚Äôt grow! So, what is an important predictor? Education is! 

```{r}
ggplot(Wage, aes(x = age, y = wage, color = education))+
  geom_smooth(method = "lm")
```

Specifically, folks without a High School diploma start at 70,000\$ as early as 18 years old, while those with an ‚ÄúAdvanced Degree‚Äù begin with a salary of approximately 140,000\$ at the age of 25, after completing their studies. However, I'll cover it in a separate video in this series, where I demonstrate how to interpret a model with a categorical predictor. Until then if you‚Äôre enjoying the video so far, please, consider hitting the like button!

# Report model results

Finally, the last thing I found difficult was **accurately describing the model with sufficient detail** for others to understand and reproduce my results. If you have similar problem, the {report} package solves it ‚Äî even if you only use one function to literally ‚Äúreport‚Äù your model.

Namely, the ‚Äúreport‚Äù function:

- identifies the type of model you used,
- shows how well the model fits the data ($R^2$),
- interprets effect sizes,
- describes whether predictors are significant and which direction the slopes go and
- even reports how 95% confidence intervals (CIs) and p-values were calculated.

I personally found it very useful, but I‚Äôd love to know what you think? So, feel free to share your thoughts in the comments section below. 

```{r}
library(report)   # extra video on my channel
report(m)
```

# What's next?

Moreover, the {report} package can help you correctly describe most classic statistical tests and models. It can even describe your entire dataset, so make sure to check it out.

```{r}
report(d)
```



-----------------------------

If you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.

**Thank you for learning!**






