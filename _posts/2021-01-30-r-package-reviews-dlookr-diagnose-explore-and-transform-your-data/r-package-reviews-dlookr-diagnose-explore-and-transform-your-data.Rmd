---
title: "R package reviews {dlookr} diagnose, explore and transform your data"
description: |
  Raw data need to be diagnosed for existing problems, explored for new hypotheses and repaired in order to increase data quality and output. The {dlookr} package makes these steps fast and easy. {dlookr} generates automated reports and performs compex operations, like imputing missing values or outliers, with simple functions. Moreover, {dlookr} collaborates perfectly with {tidyverse} packages, like {dplyr} and {ggplot2} to name just a few!
author:
  - name: Yury Zablotski
    url: https://yury-zablotski.netlify.app/
date: 01-30-2021
categories:
  - EDA
  - videos
  - data wrangling
  - R package reviews
  - visualization
  
preview: dlookr_thumbnail.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```


## This post as a video

If you are more of a visual person, your can watch this video, which covers ca. 90% of this post.

```{r, eval=T, echo=F}
vembedr::embed_youtube("M7eNYbd4n1Y")
```



```{r}
library(tidyverse) # for almost everything ;)
library(flextable) # for beautifying tables
library(dlookr)    # for the main event of the evening ;)

citation("dlookr")
```


## Chapter 1: DIAGNOSE your data

### Generall diagnosis

We can think of **diagnosing** our data similarly to **diagnosing** a disease - we just try to figure out what is wrong. First of all, we can check whether a **type** of variables is correct. For instance, if we see a **text** or a **factor** for a clearly numeric variable - Temperature, we would be alarmed and would know that we need to fix it. Then, we can immediately see which variable has **missing values** and **how many**. Lastly, we can see how many **unique values** do we have in every variable, which is particularly useful for categorical variables, for instance, we could immediately see if some of values were misspelled and are now in several instead of only one category.


```{r}
diagnose(airquality) %>% flextable()
```

### Categorical variables

Speaking of categorical variables: using **diagnose_category()** function we can **diagnose** all categorical variables from our dataset at once. Such **diagnosis** reveals **names** of categories, their **counts** and **percentages** and even **ranking number** from the biggest category to the smallest.


```{r}
diagnose_category(diamonds) %>% flextable()
```

### Numeric variables

Diagnosing all numeric variables at once is similarly easy. **diagnose_numeric()** function calculates not only most common descriptive statistics, like **minimum, first Quartile, average, median, third Quartile** and **maximum**, but also gives you the number of **zeros, negative values** and even the **number of potential outliers** for every numeric variable.

```{r}
diagnose_numeric(diamonds) %>% flextable()
```

As mentioned before, {dlookr} happily collaborates with two of the most used {tidyverse} packages: {dplyr} and {ggplot2}, which is simply amazing!

```{r}
diagnose_numeric(diamonds) %>% 
  filter(minus > 0 | zero > 0) %>% 
  select(variables, median, zero:outlier) %>% 
  flextable()
```

### Outliers

By the way **outliers**, how would we **diagnose** them? Of coarse we would **count** them and get their **percentages** in every variable. {dlookr} does it too. Moreover, it calculates three different averages: the **mean** of every variable **with outliers, without outliers** and the **mean of the outliers themselves**. In this way we can see how strong the influence of outliers for every variable is. For instance the variable "depth" in "diamonds" data has over 2500 outliers. That's a lot! However, the **means with and without outliers** are almost identical. Besides, the **average of the outliers themselves** is very similar to the original average of the whole data. In contrast, the variable "price" with over 3500 outliers is heavily influenced by them. The **average of the outliers** is almost 5 times higher, than the **average without them**. That's deep enough diagnoses of outliers already. And if that weren't enough, {dlookr} can visualize the distribution of data with and without outliers. Let's plot them!

```{r}
diagnose_outlier(diamonds) %>% flextable()
```


First of all we'll quickly find the variables (or columns) with outliers using **find_outliers()** function. It gives you the index of the column. For example in "iris" dataset the second column contains outliers. Using the argument **index = FALSE** provides the name of the column, so that you can select only columns with outliers and plot them by a simple **plot_outlier()** function. 

If we don't specify any columns, **plot_outlier()** function will plot each numeric variable from our dataset. This plot displays the distribution of data **with and without outliers** in the form of box plots and histograms, so that we directly **see how our data changes if we remove outliers**.

```{r}
find_outliers(iris)
find_outliers(iris, index = F)

iris %>% 
  select(Sepal.Width) %>% 
plot_outlier()

airquality %>% 
  select(find_outliers(airquality)) %>% 
plot_outlier()

plot_outlier(ISLR::Default)
```

If we remove outliers, they kind of become **missing values**, despite the fact that they weren't missing in the beginning. And if we want to diagnose our data properly, we need to deal with **missing values** too. And as always, the best way to deal with any data problem is to **visualize it**. 

### Missing Values

Well amazingly, {dlookr} not only can visualize missing values, but can also do this in **three different ways**. The first one - is the **Pareto chart**. 

```{r fig.height=5}
plot_na_pareto(airquality)
```

In a **Pareto Chart counts and proportions of missing values** are represented in descending order by **bars**, and the **cumulative total** is represented by the **line**. It even tells you what the amount of missing values **means**, namely, missing around 24% of observations is still **OK**, while missing more then 40% of values would be **bad**. If you have a lot of variables and wanna display only the ones with missing values, use **only_na = TRUE** argument. The only problem with pareto plot is that we don't know whether missing values in different columns belong to the same observation.


```{r fig.height=5}
airquality %>% 
  plot_na_pareto(only_na = TRUE)

airquality %>% 
  plot_na_pareto(only_na = TRUE, plot = FALSE) %>% flextable()
```

But that's where the second type of visualization of missing values comes into play - **plot_na_hclust()** - which shows you how missing values are **distributed** and whether there is any **overlapping** of variables between then. The only thing which is troublesome here is that overlapping itself could be difficult to see if some variables have very few and some variable have a lot of missing values.

```{r}
plot_na_hclust(airquality)
```

Third method of plotting missing values solves this problem. **plot_na_intersect()** function visualizes the **combinations of missing values across columns**. The **x-axis** shows the variables with missing values, while the counts of missing values are shown on the top of the plot as bars. The **y-axis** represents the **combination of variables and their frequencies**. For instance, we see, that two of the observations are missing in both variables, Ozone and Solar. 

```{r}
plot_na_intersect(airquality)	
```

### Reporting

So, let's close the **Diagnosis** chapter with the **diagnose_report()** function, which combines most of what we just learned (but not all!) into one **PDF** or **HTML document** in seconds. Just run this line of code and explore the document. But the way - **exploring** - is the second thing {dlookr} package absolutely nails! So, let's get straight to it.

```{r eval=F}
diagnose_report(airquality) # pdf or html
```



## Chapter 2: EXPLORE your data

### Describtive statistics

```{r}
# all numeric variables
describe(iris) %>% flextable()
```


**describe()** function provides **descriptive statistics of ALL numeric variables in your dataset at once**, if you don't specify any. Among them are **average, standard deviation, standard error, interquartile range, skewness and many quantiles**. However, **describe()** function becomes even more useful by **collaborating with {dplyr}** package. Particularly, we can **group the data by any categorical variable** and get **descriptive stats for each separate category**.


```{r}
# per category
iris %>% 
  group_by(Species) %>% 
  select(Sepal.Length) %>% 
  describe() %>% flextable()
```



### Normality Test



```{r}
normality(iris) %>% flextable()
```


Checking normality of data is an routine task of data scientists before analysing data. **normality()** function performs a **Shapiro-Wilk normality test on ALL numeric variables at once**, if non are specified. (When the number of observations is greater than 5000, then 5000 observations are randomly selected.) The ability of this function to work with {dplyr} helps here too, since we often need to **check normality of groups** before comparing them. The code you see below consists of **just 4 words**, but conducts 12 normality tests, namely for 3 categories in 4 numeric variables. Now, imagine a dataset with 100 numeric variables and 100 categories you wanna check the normality for. It would still take only 4 words to conduct 10.000 tests!

```{r}
iris %>% 
  group_by(Species) %>% 
  normality() %>% 
  flextable()
```

For comparison, here is the simplest example of the code I used for the same task before {dlookr}. Here I would need to:

- transform the data first
- "nest" the values
- use "map" function two times, which has nothing to do with "normality"
- use strange symbols like "~" (*tilde*) and ".x$"
- "tidy" up the result
- "unnest" the result and even 
- get rid of some useless columns to get the same table we have seen above

Don't get me wrong, I am a huge fan of {dplyr}! But some tasks can be **simplified even beyond {dplyr}** and that's just **beautiful**.

```{r }
iris %>% 
  pivot_longer(cols = 1:4) %>% 
  nest(value) %>% 
  mutate(
    test   = map(data, ~ shapiro.test(.x$value)),
    tidied = map(test, broom::tidy)
  ) %>% 
  unnest(tidied, .drop = TRUE) %>% 
  select(-data, -test) %>% 
  flextable() %>% 
  width(j = "method", width = 2)
```



Moreover, **plot_normality()** function **visualizes the normality** of numeric data and two most common transformations of data, namely **log transformation & square root**, in case the normality assumption wasn't met. Particularly, we see:

- Histogram of original data
- Quantile-Quantile plot of original data
- Histogram of log transformed data and finally
- Histogram of square root transformed data

Now we can even see whether transformation improves something or not.


```{r}
airquality %>%
  plot_normality(Ozone)
```

### Correlation

In order to quickly **check the relationship between numeric variables** we can use **correlate()** function. If we don't specify any target variables, **Pearson's correlation between ALL variables** will be calculated pairwisely. But let's have a look at only one variable first - Ozone. Nice, right? But **plot_correlate()** function is even more useful, because it **visualizes these relationships**. We can of course determine the **method of calculations**, be it a default "pearson", or a non-parametric "kendall" or "spearman" correlation. The **shape** of each subplot shows the **strength of the correlation**, while the **color** shows the **direction**, where blue is **positive** and red is **negative correlation**.

```{r}
correlate(airquality, Ozone)

plot_correlate(iris, method = "kendall")
```

Here again, by using some {dplyr} code, we can quickly check as many correlations as we want. 

```{r}
diamonds %>%
  filter(cut %in% c("Premium", "Ideal")) %>% 
  group_by(cut) %>%
  plot_correlate(method = "spearman")
```

### Relation

{dlookr} can also check out **other kinds of relationships between two particular**, for example between two categorical, **variables**. You just need to specify the response (or target) variable with a function **target_by()** and the predictor with the function **relate()**. 

#### Categorical to categorical

For instance, the influence of predictor "clarity" on our target "cut" in "diamonds" dataset, will be analyzed by a **Chi-Square Test for independence**. Plotting them will produce a **mosaic plot** with frequencies of both categorical variables. 

```{r}
diamonds %>% 
  target_by(cut) %>%      # similar to "group_by" only for one variable
  relate(clarity) %>% 
  summary() 


diamonds %>% 
  target_by(cut) %>%      
  relate(clarity) %>% 
  plot()
```


#### Categorical to numeric

If our target variable is categorical and our predictor is numeric, **simple descriptive stats per category of a target variable and for the numeric variable without categorization and density plots** will be displayed.



```{r}
# iris %>% 
#   target_by(Species) %>%      
#   relate(Sepal.Length) %>% 
#   flextable()

iris %>% 
  target_by(Species) %>%      
  relate(Sepal.Length) %>% 
  plot()

```




#### Numeric to categorical

If the response variable is numeric, while the predictor is categorical or numeric, **simple linear regression** will be applied. Using **summary()** or **tab_model()** would give you the **model output**, whith **coefficients and p-values**. Plotting the categorical predictor would produce a pretty boring **box-plot**, while visualization of the numeric variable would produce a nice **model fit with confidence intervals**.

```{r}
iris %>% 
  target_by(Sepal.Length) %>%      
  relate(Species) %>% 
  sjPlot::tab_model()

iris %>% 
  target_by(Sepal.Length) %>%      
  relate(Species) %>% 
  plot()
  #sjPlot::plot_model(type = "pred")
```
#### Numeric to numeric

```{r}
airquality %>% 
  target_by(Ozone) %>%      
  relate(Temp) %>% 
  summary() 

airquality %>% 
  target_by(Ozone) %>%      
  relate(Temp) %>% 
  plot()
```


### Reporting

Similarly to the diagnose report, **eda_report()** will produce a **report** containing most of the exploratory data analysis {dlookr} provides. Moreover, if we here specify the **target variable**, we'll get much richer report. Let's have a look at the **HTML output format** this time. In this you'll see the descriptive stats, the normality checks, correlation analysis and more, so, feel free to explore it by yourself.

```{r eval=F}
airquality %>%
  eda_report(
    target        = Temp, 
    output_format = "html", 
    output_file   = "EDA_airquality.html")
```


## Chapter 3: TRANSFORM your data

In the last part of this post we'll see how we can easily fix the problem with missing values and outliers. This part is my favorite! To be honest with you, missing values and outliers just suck! They are like parasites on my data and I just wanna get rid of them. Now, thanks to {dlookr}, I can do it easily and very effective! Check this out.


### Imputation

**Imputation** simply means **replacing a missing value** with a value which makes sense. We can impute both numeric and categorical values.

#### 1. numeric

- imputation can be done with a **single value**, like 
  - mean 
  - median or 
  - mode, or 
- by a **machine learning algorithm**, like
  - knn - K-nearest neighbors
  - rpart - Recursive Partitioning and Regression Trees or
  - mice - Multivariate Imputation by Chained Equations (random seed must be set)

**imputate_na()** function needs 4 arguments: 
- data
- name of the variable with missing values
- name of the predictor variable and 
- the **method** of imputation

Let's use the "airquality" dataset again, and **fill out 37 missing values** in the variable "Ozone" via the predictor variable "Temperature" using the **average** as a **method of imputation**.

If we save the output of the function **imputate_na()** in some meaningful name, for example "bla", we can **first** have a look at the **summary**. The summary compares some metrix of the *Original* and *New* datasets, *before* and *after* the imputation accordingly. First, we see that the standard deviation and the standard error got smaller, which is great! Secondly, let's plot both datasets and compare them. The plot reveals that **distribution** of a new dataset changed dramatically, which is **not great at all**, because we just made up some data which deviates far away from the original data, and the original data is our best bet on reality. So, the distribution before and after imputation suppose to be similar.

```{r}
# mean
bla <- imputate_na(airquality, xvar = Ozone, yvar = Temp, method = "mean")
summary(bla)
plot(bla)
```


Hmm, but does imputation then makes sense at all? Well, let's use one of the **machine learning methods**, for example **K-nearest neighbors**, and look at the result. This plot looks much better, because the distribution did not really change, but some aspects of it became a bit more emphasized. 

```{r}
# “knn” : K-nearest neighbors
blap <- imputate_na(airquality, Ozone, Temp, method = "knn")
plot(blap)
```

But does this mean, that imputation by a single value is useless? Well I've heard this opinion before. But I found that usefulness of imputation strongly depends on the dataset and the best way to impute **in my opinion** is just to **compare all imputation methods with each other and take the best**, even if it is a single value method. If we run the whole chunk of code below, we'll be able to visually choose among methods. I think that “rpart” - Recursive Partitioning and Regression Trees method, does the best job for the variable "Ozone" in "airquality" dataset.

Needless to say, the plots can be easily pimped with some **ggplot2** syntax, as you can see in the code.


```{r}
# mean
plot(imputate_na(airquality, xvar = Ozone, yvar = Solar.R, method = "mean"))

# median
plot(imputate_na(airquality, Ozone, Temp, method = "median"))

# mode
plot(imputate_na(airquality, Ozone, Temp, method = "mode"))

# “knn” : K-nearest neighbors
plot(imputate_na(airquality, Ozone, Temp, method = "knn"))

# “rpart” : Recursive Partitioning and Regression Trees
plot(imputate_na(airquality, Ozone, Temp, method = "rpart"))+
  theme_classic()+
  theme(legend.position = "top")

# “mice” : Multivariate Imputation by Chained Equations
plot(imputate_na(airquality, Ozone, Temp, method = "mice", seed = 999))+
  theme_minimal()+
  theme(legend.position = "top")
```

#### 2. categorical

Only three methods are available for imputation of categorical variables:

- mode - mode
- rpart - Recursive Partitioning and Regression Trees and
- mice - Multivariate Imputation by Chained Equations (random seed must be set)

Let's take the **diamonds** dataset, make it a bit smaller, add some NAs and use the **mice imputation**. The plot of the results compares the frequencies of each category, so that we can see which values were imputed. The summary command would give you the **exact counts and proportions of categories before and after imputation**.

And already familiar **plot_na_pareto()** function can ensure that we have missing values in the old "cut" variable and no missing values in the "new_cut" variable.

```{r}
d <- diamonds %>% 
  sample_n(1000) 

d[sample(seq(NROW(d)), 50), "cut"] <- NA

d2 <- imputate_na(d, cut, price, method = "mice", seed = 999)

plot(d2)

summary(d2)
```


```{r fig.height=5}
d %>% 
  mutate(new_cut = d2) %>% 
  select(cut, new_cut) %>% 
  plot_na_pareto()
```


#### 3. Imputation of outliers

We have seen that {dlookr} package can effectively **find outliers and impute missing values**. If the outliers are nasty, they can be treated like missing values. Therefore, if desired, the outliers can be easily replaced by the  **imputate_outlier()** function. This function supports 4 following imputation methods:

- mean
- median
- mode and
- capping - which **imputes the upper outliers with 95th percentile, and the bottom outliers with 5th percentile**. Here, I would also recommend to **compare all 4 methods** before deciding which method is the best. And if we look at the plots, I think a simple average wins this time.

```{r}
bla <- imputate_outlier(diamonds, carat, method = "mean")
plot(bla)

bla <- imputate_outlier(diamonds, carat, method = "median")
plot(bla)

bla <- imputate_outlier(diamonds, carat, method = "mode")
plot(bla)

bla <- imputate_outlier(diamonds, carat, method = "capping")
plot(bla)
```



### Binning

**Categorization**, or *binning()*, transforms a numeric variable into a categorical one. The following types of binning are supported:

- quantile, which categorizes using quantiles 
- equal - categorizes to have equal length segments
- pretty - categorizes into moderately good segments (whatever it it :)
- kmeans - categorization using K-means clustering
- bclust - categorization using clustering technique with **bagging**, where **bagging** is an abbreviation for **Bootstrap Aggregating** - a machine learning meta-algorithm designed to improve predictive performance.

The **bclust method** sounds most fancy, so, let's try this one first. The distribution of the Ozone variable provides 3 intuitive groups:

- the parabola from 0 to 60
- short flat area from 60 to 80 and
- steady decline till the end of the plot

...thus, let's set the number of **bins** (or categories) to 3 and name the categories with the argument "labels". Plotting the result shows that our first intuition was correct, the parabola was singled out into a category. However, the third category is kind of small, and has only  observation, which we can see using **summary()** command. And we wanna know the **exact numbers were the borders of categories were drawn**, we just don't use argument "labels" in the **binning()** command:

```{r}
DataExplorer::plot_density(airquality$Ozone)

# with names
set.seed(4444)
bin <- binning(airquality$Ozone, type = "bclust", nbins = 3,
              labels = c("cat1", "cat2", "cat3"))

plot(bin)

summary(bin)

# without names
binning(airquality$Ozone, type = "bclust", nbins = 3)
```

If we **check out all the other categorizations, we can choose the one we feel the best about.** I would take the **kmeans**, because it has relatively similar counts of data in every category, while still describing the distribution with three traits of the plot we have seen, namely parabola at the beginning of the plot, plateau in the middle and decline till the end.

```{r}
plot(binning(airquality$Ozone, type = "quantile", nbins = 3,
              labels = c("cat1", "cat2", "cat3")))

plot(binning(airquality$Ozone, type = "equal", nbins = 3,
              labels = c("cat1", "cat2", "cat3")))

plot(binning(airquality$Ozone, type = "pretty", nbins = 3))

plot(binning(airquality$Ozone, type = "kmeans", nbins = 3,
              labels = c("cat1", "cat2", "cat3")))
```
Using a simple {dplyr} syntax we can easily **add a new categorized Ozone variable into our dataset**.

```{r}
airquality %>% 
  mutate(
    binned_Ozone = binning(airquality$Ozone, type = "quantile", nbins = 3, labels = c("cat1", "cat2", "cat3"))
    ) %>% head() %>% flextable()
```



### Standardization and Resolving Skewness

**transform()** function performs both **Standardization and Resolving Skewness**.

#### 1. Standardization

Standardization is the process of **putting different variables on the same scale**, so that we can better compare and model them. In order to standardize variables, we first calculate the mean and standard deviation for a variable. Then, for each observed value of the variable, we subtract the mean and divide by the standard deviation. {dlookr} provides two methods for standardization, namely:

- zscore - z-score transformation is default: $$ \frac{x - mean}{sigma} $$
- minmax - minmax transformation: $$ \frac{x - min}{max - min} $$


```{r}
bla <- transform(airquality$Solar.R)

plot(bla)
```




#### 2. Resolving Skewness

...and 6 methods for resolving skewness:

- log - transformation, which is great, but has problem with zeros.
- log+1 - transformation, which is amazing for variables that contain zeros.
- sqrt - square root transformation.
- 1/x - transformation
- x^2 - squared transformation and
- x^3 - power of three transformation

**log+1** is my favorite, because it **handles zeros**, which otherwise produce "Infinities" with a simple **logarithm**.

```{r}
find_skewness(mtcars, index = F)

transform(mtcars$hp, method = "log+1") %>% 
  plot(ylim=c(0,10))
```


### Reporting

Similarly to **diagnosis** and **exploratory reporting**, we can produce a **transformation report** using a single intuitive command.

```{r eval = F}
transformation_report(airquality, target = Temp)
```


## Conclusion

I hope {dlookr} package provides some value for you. It certainly made my life easier, that's why I wanted to share it with you. I am super curious what R package you find the most useful? Please, let me know in the comments section below. All right, that's it for today. If you liked this post, check out the other ones on my blog, or subscribe to my [YouTube](https://www.youtube.com/channel/UCcGXGFClRdnrwXsPHi7P4ZA) channel. 

**Thanks for learning ;) **

---------------

If you think, I missed something, please comment on it below, and I’ll improve this tutorial.

# Useful ressources

- https://github.com/choonghyunryu/dlookr
