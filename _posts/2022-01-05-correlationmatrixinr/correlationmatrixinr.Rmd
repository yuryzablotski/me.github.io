---
title: "R demo | Correlation Matrix | Danger or opportunity?"
description: |
  Having several numeric variables, we often wanna know which of them are correlated and how. Correlation Matrix seems to be a good solution for it. But drawing conclusions from plain correlation coeffitients and p-values is dangerous, if we don't visualize the data. Let's learn a better way to produce a correlation matrix.
author:
  - name: Yury Zablotski
    url: https://yuzar-blog.netlify.app/
date: "`r format(Sys.time(), '%B %d, %Y')`"
categories:
  - videos
  - statistics
preview: thumbnail.jpg
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
# draft: true
bibliography: /Users/zablotski/Documents/library.bib
#csl: american-political-science-association.csl
biblio-style: apalike
link-citations: yes
linkcolor: blue
# draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```


## This post as a video

I recommend to watch a video first, because I highlight things I talk about. It's less then 5 minutes long. 

```{r, eval=T, echo=F}
vembedr::embed_youtube("ffhQil7KhSo")
```

## Previous topics

[Correlation Blogpost](https://yuzar-blog.netlify.app/posts/2021-12-29-correlationinr/) would help.



## Visualize distribution!

```{r}
# install.packages("dlookr")
library(dlookr)
plot_correlate(iris)
```


```{r eval=FALSE}
# install.packages("fastStat")
library(fastStat)
cor_sig_star(iris[, 1:3])
```
A {chart.Correlation} function from {PerformanceAnalytics} package does both (1) provides correlation coefficients with significance and (2) plots the data. This plot immediately uncovers **4 reasons why we can't trust a plain correlation matrix**. 

```{r}
# install.packages("PerformanceAnalytics")
library("PerformanceAnalytics")
chart.Correlation(iris[, 1:3])
```

### 1. Linearity assumption

First, the data is not-linearly distributed, while correlation only makes sense for linear relationships. If fact, we could draw three straight lines in this plot, while our negative correlation coefficient assumes only one of them. 

### 2. Grouping variables & Simpson's paradox

Secondly, some data seemed to be clustered into groups. Indeed, the "Iris" dataset, which you already have in R, has a "Species" column with 3 different Species of iris plant. So, putting all three species in one basket feels wrong.

![](iris.jpg)
A {ggpairs} command from {GGally} package accounts for a grouping variable and conducts correlation analysis for  all variables and groups at once, while also plotting a correlation line on top of every group.

```{r}
# install.packages("tidyverse")
library(tidyverse) # for "aes()" & later for "%>%" 

# install.packages("GGally")
library(GGally)
ggpairs(iris, 
        columns = 1:3, 
        aes(colour=Species),
        lower = list(continuous = "smooth"),
        upper = list(continuous = wrap("cor", 
                         method = "pearson")))
```

These plots reveal that two of the negative coefficients before splitting into groups show the opposite of what is actually true, namely, the correlation within every group **IS** positive.

Moreover, NOT-significant-negative-correlation between Sepal.Length and Sepal.Width turned out to be a significant positive correlation for all three species! If I would have accepted the not-significant negative correlation, I would have made a Type II Error, in other words, I would have missed an important discovery. So, no Nobel Price for me.

Vice Versa, a highly significant correlation between Petal.Length and Sepal.Length for all three Iris species, is not significant at all for "Setosa." Here I would have made a Type I Error, meaning, I would have discovered nonsense. 

The phenomenon, where grouped data shows the opposite correlation scenario as compared to ungrouped data is known as **Simpson's Paradox, also called Yule-Simpson effect**.

But, that's not all! The third problem of not visualizing the data is that we have no idea about the normality of distribution, so we actually don't know which Correlation method to use, Pearson, Spearman or something else.

### 3. Normality assumption

But if we look at histograms from {chart.Correlation} function, we immediately see that two of variables, Sepal.Length and Petal.Length, are not-normally distributed. Shapiro-Wilk Normality Tests confirm this intuition with low p-values. 

```{r}
# install.packages("flextable")
library(flextable)

iris %>% 
  normality() %>% 
  mutate(across(is.numeric, ~round(., 3))) %>%
  regulartable()
```

So, we could have applied a non-parametric Spearman or Kendall correlation methods, but if we check the normality for every Species, which are clustered in groups, we'll see that ALL OF THEM are perfectly normally distributed, so, we should use a parametric Pearson correlation analysis to ensure the highest statistical power! Otherwise, we'll again either miss a discovery, or discover complete nonsense.

```{r}
iris %>% 
  group_by(Species) %>% 
  select(1,3) %>% 
  normality() %>%  
  mutate(across(is.numeric, ~round(., 3))) %>%
  regulartable()
```

### 4. Correct for multiple comparisons

The last thing which makes using plain correlation matrix statistically dangerous is that you have multiple comparisons without adjusting p-values for them, which again, increases the probability to find nonsense. A {grouped_ggcorrmat} function from {ggstatsplot} package helps with that. Within this function your can specify only 3 argument to get all the results:

- first, the data - "Iris",
- then "type" of correlation, we'll go with a "parametric" Pearsons method since we know that our data is linearly and normally distributed and 
- the grouping variable, which is "Species" in our case

Here you'll see:

- how strong the correlation is,
- in which direction it goes and
- whether correlation is significant or not after p-values were adjusted for multiple comparisons.

```{r fig.width=10, fig.height=10}
# install.packages("ggstatsplot")
library(ggstatsplot)

grouped_ggcorrmat(
  iris,
  type = "parametric",
  grouping.var = Species, 
  plotgrid.args = list(nrow = 2)) # looks better
```



## What's next?

- As you can see, while correlation matrix is an useful Exploratory Tool, we shouldn't trust the numbers too quickly. We need to (1) visualize the data, (2) check the assumptions of normality and (3) linearity, (4) check for grouping variables and (5) correct for multiple comparisons. And since some data might be suitable for parametric analysis, while some not, it's usually better to make a deeper dive into every single correlation between only two numeric variables, which you can learn [from this short video](https://youtu.be/eXczd0ewVgE), where I showed how to produce this statistically rich plot using only one command and how to interpret all these results.

- Start to learn about linear regression, you won't regret it ;)

---

If you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.

**Thank you for learning!**
















