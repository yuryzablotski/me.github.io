---
title: "Transform Your Data Like a Pro with {tidyr} and Say Goodbye to Messy Data!"
description: |
  Every data scientist dreams of creating beautiful visualizations, conducting complex modeling, and diving into machine learning methods. However, most of the time, messy data hinders our ability to do really cool stuff. Thus, tidying up the data is the key to unlocking your full potential. Unfortunately, reshaping data in Excel can be a tedious and error-prone task. Do you remember the time when you needed to quickly transform columns to rows or rows to columns, split or combine columns, or handle missing values? With the {tidyr} package, you'll be able to transform your data quickly, accurately, and efficiently, preparing yourself for the stuff that really matters ;) 
author:
  - name: Yury Zablotski
    url: https://yuzar-blog.netlify.app/
date: "`r format(Sys.time(), '%B %d, %Y')`"
categories:
  - videos
  - statistics
  - data wrangling
  - R package reviews
preview: dplyr_3_thumbnail.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
bibliography: /Users/zablotski/Documents/library.bib
#csl: american-political-science-association.csl
biblio-style: apalike
link-citations: yes
linkcolor: blue
#draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
library(magick)
```



    
    “Happy families are all alike; every unhappy family is unhappy in its own way.” 
    – Leo Tolstoy

    “Tidy datasets are all alike, but every messy dataset is messy in its own way.” 
    – Hadley Wickham


![](tidy-1.png)

This picture shows three simple rules which make a dataset tidy[^1]:

[^1]: Amazing and Free Book by Garrett Grolemund and Hadley Wickham: [“R for Data Science”](https://r4ds.had.co.nz/)

1. Each column is a variable
2. Each row is an observation
3. Each cell is a single value

But it’s surprising, how rare these three rules are followed.
    

# This post as a video

To maximize the effect of this post you should definitely work through [Data Wrangling Vol. 1](https://yuzar-blog.netlify.app/posts/2023-01-31-datawrangling1/) and [Data Wrangling Vol. 2](https://yuzar-blog.netlify.app/posts/2023-02-07-datawrangling2/) before. And I also recommend to watch a video first, because I highlight things I talk about. It's ca. 13 minutes long. 

```{r, eval=T, echo=F}
vembedr::embed_youtube("DotnsqCoa7I") 
```


```{r}
library(tidyverse)
```




# Reshape data: make then wider or longer

```{r echo=FALSE}
image_read("tidyr-pivoting.gif")
```

## pivot_wider & spread

Imagine the situation where various types of information are combined into one column, but you need to "spread" them out into separate columns, because the information they convey cannot coexist, like in columns `type` and `count`. The "cases" and "population" values have nothing in common and therefore, don't belong in the same column. So, we have to broaden our table by taking the column "names from" the "type" and the "values from" the "count" column. Simply put, **we need to make our table wider**. And while it's easy to manually copy and paste the data for two categories, you wouldn't want to do that with 100 categories.


![](tidy-8.png)

```{r}
table2 %>% 
  pivot_wider(names_from = type, values_from = count)
```


A "pivot_wider" is amazing, but if you work with real-world data (which is often dirty), you'll certainly encounter two common problems. The first one is that "pivot_wider" will eventually produce missing values. For this case, "pivot_wider" has an useful argument that fills in the missing values.

```{r}
x <- tibble(A = c("a", "b", "c"), B = c("one", "two", "three"), C = c(1, 2, 3) )

x %>% pivot_wider(names_from = B, values_from = C)

x %>% pivot_wider(names_from = B, values_from = C, values_fill = list(C = 0))
```

The second problem is that widening a table can lead to several values being put into one cell, resulting in a list of values that violates the third principle of tidy data - each cell is a single measurement. 

![](tidy_data.jpg)

To address this problem, you can utilize the "values_fn" argument, which enables you to fill this cell with an aggregation function such as "mean". If there are still missing values after aggregation (because there was nothing to aggregate), you can fill them with a value of your choice using the "values_fill" argument.

```{r}
mtcars %>% 
  pivot_wider(
    id_cols = c(vs, am), 
    names_from = cyl, 
    values_from = mpg)

mtcars %>% 
  pivot_wider(
    id_cols = c(am, vs), 
    names_from = cyl, 
    values_from = mpg, 
    values_fn = list(mpg = mean),
    values_fill = list(mpg = 9999) )
```

A "pivot_wider" is the more intuitive next-generation version of the "spread" command. However, "spread" is still commonly used, so it remains in the package. Knowing how "spread" works can help in understanding older code.

```{r}
table2 %>% 
  spread(key = type, value = count) 
```

## pivot_longer & gather

Now, imagine this: you've got a bunch of data collected over a span of years. Usually, these surveys organize the measurements by year and put them in separate columns. So, you'd have a column for each specific year.

```{r}
table4a
```


However, storing data in this manner limits the information we can extract from it. By separating the measurements into different columns based on year, we lose the valuable variable - "time", which is imprisoned in the table header (imagine liquid metal T-1000 before passing prison bars), unable to be fully utilized.

![](tidy-9.png)

For such a case, "pivot_longer" takes multiple columns and combines them into just two columns - one with column names and one with survey values. This way, the information about time is freed up (T-1000 passing prison bars) and can be used for things like plotting or modeling.

```{r}
table4a %>% 
  pivot_longer(cols = c("1999", "2000"))
```

So in this example, we kept it simple and just used two years. But let's say you have a lot more data, like hundreds of columns. No problem, just use a "colon" between the first and last column names to quickly cover them all. And if you want to make things **even clearer**, you can use the "names_to" and "values_to" arguments to give the new columns better names, like "year" and "number of cases" for example.

```{r}
table4a %>% 
  pivot_longer(cols = c("1999" : "2000"), names_to = "year", values_to = "number of cases")
```


And the "pivot_longer" command is basically the new and improved version of the retired "gather" command. But don't worry, "gather" is still around and people still use it, so it's good to know how it works too.

```{r}
table4a %>% 
  gather(2:3, key = "year", value = "value")
```

```{r echo=FALSE}
image_read("tidyr-spread-gather.gif")
```

# Split or combine cells



## unite

![](tidy-18.png)

The real-world data can be a bit messy (footbal star messy), which sometimes requires us to **combine several columns into one**. For example, separating a century and a year into different columns doesn't make much sense, does it? That's where the "unite" command comes in handy.

With "unite", we can do a few things. First, we give a name to the new column. Second, we specify which columns to combine. Third, we determine what separator to use between the values. Fourth, we can decide whether to remove the input columns from the output data (but I like to keep them in to make sure everything is working properly). And finally, we can choose to remove any missing values.

For example, we could unite a "century" and a "year" into a "new_year" column (imagine a picture of Santa here). After we're confident that our code is working properly, we can set the "remove" argument to TRUE and say goodbye to those old, separate columns. 



```{r}
table5 %>% 
  unite(
    col = "new_year", 
    c("century", "year"), 
    sep = "", 
    remove = FALSE, # remove = TRUE, 
    na.rm = TRUE)
```

Isn't that cool? However, it's too early to celebrate just yet, because in the very next "rate" column, we face an opposite problem where two variables "cases" and "population" are trapped within the same column. Luckily, the "separate" command offers an intuitive solution to this issue.

## separate

![](tidy-17.png)

Let me break this down for you. If you spot a separator within your data, you can utilize the "sep" argument to inform the "separate" command where to split, and you can specify the names of new columns you want to create. 

```{r}
table5 %>% 
  unite(col = "year", c("century", "year"), sep = "") %>% 
  separate(col = rate, sep = "/", into = c("cases", "population"))
```

However, the best part is - "separate" is quite intelligent and can detect split-points that aren't characters or letters automatically. This means that even if your dataset is super messy (football star messy), and different separators such as underscores, slashes, special characters or empty spaces are present, "separate" can still split it like a boss.

```{r}
blah <- tribble(
  ~ID, ~month,
  1,   "a_c",
  2,   "d/c",
  3,   "7 & 8",
  4,   "9 10"
)

blah %>% 
  separate(month, into = c("month 1", "month 2")) 
```




But wait, there's more. There are actually **two unique ways to separate** a messy column:

1. First, we can create several columns out of one, like we just did. But that's only half of the story. Because we have some useful tricks that can make your life even easier. For instance, this output shows that two new columns are of the "character" type, put simply - **text**, because they inherited it from the original column. But I'd like them to be what they actually are - **numbers**. For this we can allow the "separate" command to "convert" them to a better format of it's choice:



```{r}
table3 %>% 
  separate(col = rate, into = c("cases", "population"), convert = TRUE)
```

We can also split a column without any separators by specifying where the values should be separated. 


```{r}
table1 %>% 
  separate(col = country, into = c("begin", "end"), sep = 5) 
```

**But we have to be careful**. Because, if we have too many pieces of text inside of a column, the automatic separation may leave out **very important** text. 

```{r}
blah <- tribble(
  ~sex, ~joke,
  "She",   "Darling, tell me something dirty!",
  "He",   "Ahhhhh .... kitchen?"
)

blah %>% 
  separate(col = joke, into = c("part 1", "part 2"))
```

To prevent this, we can use the "extra" option to "merge" the rest of the text together. And finally, to check the quality of separation, we again should use the "remove = FALSE" option.

```{r}
blah %>% 
  separate(
    col    = joke, 
    into   = c("part 1", "part 2"),
    extra  = "merge", 
    remove = FALSE) 
```


2. So, we now learned how to create several columns out of one. The second way to separate is to **create several rows** out of one. This is useful when messy data violates the third rule of a data set and contains multiple measurements in a single cell. It's kind of "pivot_longer" on steroids (Arni), because it makes a clean table from a single messy column, instead of multiple tidy columns.

```{r}
blah <- tribble(
  ~season,    ~month,
  "Winter",   "Dez & Jan + Feb",
  "Spring",   "Mar | Apr ? Mai"
)

blah %>% 
  separate_rows(month) 
```


# Nested data

**But get ready for a plot twist!** While storing several values in one cell is bad, storing several columns in one cell can actually be incredibly useful, because it helps to organize complex data and allows for more effective work with multiple sub-tables at once, using functions like "map". For example, we can create multiple models and store their results for further use with just a few lines of code. I have also created a [tutorial on multiple models](https://youtu.be/tQ8dC0oLTnA), so feel free to check it out later. But for now let's see how to create a nested data frame.


![](nest_data.png)

## Create nested data

Creating a nested data frame is as easy as just using "group_by" and "nest" functions to move the groups into a list-column. We can also specify the columns we want to "nest", which can help with large datasets.  But what if we already have a nested data frame and need to access the data inside it?

```{r}
storms %>%
 group_by(name) %>%
 nest()

storms %>%
  group_by(name) %>%
 nest(data = c(year:long, category:hurricane_force_diameter))
```



## Reshape nested data

![](unnest_longer.png)

Well, similarly to "separate_rows" which **unpacks** the single cell into several rows, {tidyr} package provides the "unnest_longer" function, which **unpacks nested data** into multiple rows. 

```{r}
starwars %>%
  select(name, films) %>%
  filter(name %in% c("Luke Skywalker", "C-3PO", "R2-D2")) %>%
  unnest_longer(films)
```


Moreover, we can easily turn each element of a list-column into a regular column via "unnest_wider" function. But interestingly, that creates missing values, which uncovers a new problem.


```{r, layout="l-body-outset"}
starwars %>%
  select(name, films) %>%
  filter(name %in% c("Luke Skywalker", "C-3PO", "R2-D2")) %>%
  unnest_wider(films, names_sep = "_") %>% 
  flextable::regulartable()
```


# Expand and complete tables

## expand

The problem is that sometimes we think we have all the data we need, but in reality, we don't. Let's take, for example, a study on cars with different transmissions or gears. Before we dive in, we need to make sure we're aware of how many unique values each variable has.

```{r}
xtabs(~ am, data = mtcars)
xtabs(~ gear, data = mtcars)
```

Having two categories from the transmission variable ("am"), and three from the gearbox ("gear"), you'd think we'd have six different combinations of cars, right? But if we look at the combination of both variables, we actually end up with only four distinct combinations.

```{r}
mtcars %>% distinct(am, gear)
```

And here's where things get tricky. Even if we have a massive dataset, there are still some combos we might be missing. In our case we have zero cars with an automatic transmission and five gears, and not a single car with a manual transmission and three gears. 

```{r}
xtabs(~ am + gear, data = mtcars)
```

This **is a problem**, because **these missing values are kinda sneaky - they're implicit**, meaning we don't even realize we're missing them. And that's the dangerous moment when we start to think we have all the data we need, when in reality, we don't. Fortunately, "expand" function provides a solution for it, by finding all possible combinations of categorical variables we should have:

```{r}
mtcars %>% expand(am, gear)
```

But wouldn't it be great if we could add those missing combos to our dataset, even if we don't have any data for them yet? Well, yes! Why? Because then we could **estimate** what their values could **probably** be.

```{r}
d <- mtcars %>% 
  complete(cyl, am, gear) %>% 
  arrange(desc(is.na(mpg)))

d
```



## complete

And if you think that **estimates are too uncertain**, consider this: **even a rough idea is often better than no information at all**. That's why it's **essential** to fill in missing data to uncover new opportunities. Because, even when we have **only three** categorical variables from the "mtcars" dataset, we're missing **eight combinations**. Imagine how much would you miss, if you have 100 categorical variables with several categories each? This highlights the importance of dealing with missing values.

Luckily for us {tidyr} package provides two ways to deal with missing values. The first is rather radical: we could simply remove all rows containing missing values, doesn't matter implicit or explicit, using "drop_na" function. But that could eliminate most of our data, because missing values are very common, and we would have missed the opportunity to uncover something new and get a Nobel price.

```{r}
d %>% 
  drop_na()
```


The second way is to "fill up" missing values with something useful. What is useful? Well, the average would do the job. So, let's "fill" the first three columns "mpg, disp and hp" with their averages:


```{r}
mtcars %>% 
  complete(cyl, am, gear, 
           fill = list(mpg  = mean(mtcars$mpg), 
                       disp = mean(mtcars$disp), 
                       hp   = mean(mtcars$hp))) %>% 
  arrange(desc(is.na(wt)))
```

Hmm, the average may be better then nothing, but it's far from perfect, right? But that's not the point. The point is that you can first "discover" your implicit missing values and then turn them into opportunity by filling them out with some sophisticated machine learning method, like chained random forest, which is unfortunately way outside of the scope of today's topic. But if you want to learn more about imputing missing values with fancy machine learning algorithms, check out [this video](https://youtu.be/Akb401i32Oc).


**Thank you for reading!**

# Further readings and references

- [cheat sheet on {tidyr}](https://github.com/rstudio/cheatsheets/blob/main/tidyr.pdf)

- and the [official page of {tidyr}](https://tidyr.tidyverse.org/)



