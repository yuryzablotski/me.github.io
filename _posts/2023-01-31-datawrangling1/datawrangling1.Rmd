---
title: "Top 10 Must-Know {dplyr} Commands for Data Wrangling in R!"
description: |
  The {dplyr} is one of the most useful R packages outthere. For me R is {dplyr} and {tidyverse}. So, here we'll use the most frequently used command.
author:
  - name: Yury Zablotski
    url: https://yuzar-blog.netlify.app/
date: "`r format(Sys.time(), '%B %d, %Y')`"
categories:
  - videos
  - statistics
  - data wrangling
preview: dplyr_1_thumbnail.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
bibliography: /Users/zablotski/Documents/library.bib
#csl: american-political-science-association.csl
biblio-style: apalike
link-citations: yes
linkcolor: blue
#draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```


# This post as a video

I recommend to watch a video first, because I highlight things I talk about. It's only 8 minutes long.

```{r, eval=T, echo=F}
vembedr::embed_youtube("XcK4chr2jws") 
```


# Why do we need it? It's effective!

With just 10 commands at your fingertips, you can **select**, **rename** and create new columns (**mutate**), **arrange** or **filter** out rows, **group** rows together and then **summarize** those groups and be able to combine all these data wrangling techniques in a single **pipe**line like never before. And the best part? These commands are very easy to master, as most of them are common English verbs like "select" or "summarise". These verbs will allow you to solve the vast majority of your data manipulation challenges. So, let's get into it.

We'll start with loading the `tidyverse` package. First, because `dplyr` package is part of it and secondly, `tidyverse` contains ready to use datasets. So, let's discover the art of data manipulation with a quick **glimpse** on the first command, which name is in fact "glimpse".
 
```{r}
# install.packages("tidyverse")
library(tidyverse)
```


## 1. `glimpse` 

```{r}
glimpse(table2)
```

`glimpse` gives you a quick look at your data. It shows all variables, their first values, and their types, whether integer or ordered categorical. It also reveals the number of observations (rows) and variables (columns). The beauty of `glimpse` is that it doesn't overload your screen if your data is large. It presents only the crucial information. To **view** the whole dataset, you can actually use the `view` command.

## 2. `view(table2)`

One useful feature of such table view is the ability to **arrange** values by clicking on the header's variable name.

## 3. `arrange`

By the way, you can also arrange values while working with it. For example, "table2" is arranged alphabetically by countries. However, the whole table can be easily rearranged using one or several columns.

```{r}
table2

table2 %>% 
  arrange(year)

table2 %>% 
  arrange(year, type)
```

"Oh God, what is this strange `%>%` symbol?!," you might ask. Well, it's a highly useful one! But I'd prefer to demonstrate its use before explaining it. Until then, let's **select** the most useful columns using the... drumroll... **select** command.

## 4. `select`

```{r}
table1 %>% 
  select(country, population)
```

Useless columns will be automatically discarded. But what if you've got 300 (picture of the movie 300 ;) useful columns and just want to eliminate a couple? Rather than typing out all 300 names in the **select** command, which can be cumbersome, prone to typing mistakes, and hard on your fingertips ;) , you can simply add a minus sign in front of the unwanted column names to **deselect** them.

```{r}
table1 %>% 
  select(-cases, -year)
```

Selecting columns is great. But what if we need only specific rows, such as only those from the year 2000?

## 5. `filter`

To achieve this, we can use the **filter** command with a double equal sign. Keep in mind that the double equal sign means "equal to", while the single equal sign assigns a value to a name and won't allow for filtering. The arguments here determine the conditions that must be true to keep the row, for example we want to see only rows with the year 2000.

```{r}
table1 %>% 
  filter(year == 2000)
```

However, if you want to eliminate any undesired observations, for example to see everything except the year 2000, the `!=` (exclamation mark equals) operator is your friend. It stands for "not equal to." Cool, write?

```{r}
table1 %>% 
  filter(year != 2000)
```

But you know what's even cooler? We can do both **select** and **filter** at the same time by using this funky symbol `%>%`, called the **pipe operator**.

```{r}
table1 %>% select(year, population) %>% filter(year == 2000)
```



## 6. `%>%` - **pipe operator**

The pipe operator, for me, is just another way of saying **then**. It allows you to transform your data in a step-by-step manner, by doing one thing first, **then** another, and so on. For example, this code takes the table1 dataset, **then** selects two columns, **then** filters only the year 2000. The operator is called the **pipe operator** because it **pipes** the values on the left side into the transformed values on the right side. This replaces the less intuitive `f(x)` with the much more readable `x %>% f`. The left-to-right nature of `x %>% f` is easier to write and understand, as it follows the natural way we read, while `f(x)` reads in a counterintuitive inside-out manner, making it harder for people to grasp, except for programmers (funny picture of a programming nerd). Thus, improving the readability and understanding of code is a great advantage of this operator. Additionally, you can chain multiple simple steps to achieve a complex result easily and making the process faster (we'll see it in a moment). I have to admit, it didn't like the pipe in the beginning, but now I absolutely love it! ðŸ¥° 

The good thing is, the **pipe** will quickly become invisible, and your fingers will type it without your brain having to think about it. And the best part? There's even a keyboard shortcut for it - `Shift-Cmd/Ctrl-M`! Give it a try.

So far, so good. Removing some useless columns or rows is useful. But what if we want to create new columns? Well, we do it in a similar way evolution does, through **mutations**.

## 7. `mutate` 

The **mutate** command literally means - **creating a new column**. We can either fill the new column with whatever we want or use existing (parent) columns to calculate new (offspring) ones.

```{r}
table1 %>% 
  mutate(fill_it = "with a smile ;)") %>% 
  mutate(popul_per_case   = population / cases)
```

Not only can you give your column any name you desire while creating it, but you can also change its name whenever you want using the 'rename' command. 

## 8. `rename`

```{r}
table1 %>% 
  mutate(fill_it = "with a smile ;)") %>%
  rename(much_better_name = fill_it)
```

With the eight commands we just learned, you'll have your "perfectly" transformed dataset. But having data alone isn't much use, especially if there's a lot of it. So, it's common to want to **summarize** the data in some way - like counting, finding the sum or average.

## 9. `summarise`


To do this, you simply choose a column to **summarize** and a function to apply to it, like 'mean'.

```{r}
table1 %>% 
  summarise(average = mean(population))
```


With `dplyr` package, you can take things a step further and summarize different columns in different ways. For example, use `sum()` to get an overall summary of all values in the column and `n()` to count the number of rows.


```{r}
table1 %>% 
  summarise(avr_popul = mean(population),
            all_cases = sum(cases),
            how_many  = n())
```

Now it's getting exciting, because you can generate new numbers (making descriptive statistics) and get a comprehensive view of your dataset using plain English as computer code. However, a single-row overview of our data is not very useful since we have data from two different years and three countries. It would be much more helpful to know the averages (or any other summary statistics) for each year or each country. And that's where the final, and perhaps the most useful, `group_by` command comes into play... Hallelujah!

## 10. `group_by`

```{r}
table1 %>% 
  group_by(year) %>% 
  summarise(avr_popul = mean(population),
            all_cases = sum(cases),
            how_many  = n())

table1 %>% 
  group_by(country) %>% 
  summarise(avr_popul = mean(population),
            all_cases = sum(cases),
            how_many  = n())
```


And it gets even better - you can **group_by** multiple variables at once! Let's take the mtcars dataset as an example and find out how many cars with 4, 6, or 8 cylinders (cyl) and different gearboxes (am) are in the dataset. And let's get the average fuel efficiency (mpg) and average horsepower (hp) for each combination of cylinders and gearboxes. It's like hitting two birds with one stone!

```{r}
mtcars %>% 
  group_by(cyl, am) %>% 
  summarise(avr_mpg = mean(mpg),
            avr_hp  = mean(hp),
            count   = n())
```
So, using **group_by** and **summarize** together is much more powerful than using them separately. And this goes for any function we've learned today. The more functions you string together using the pipe-operator %>%, the more advanced your data manipulation becomes. For example, don't look at this chunk of code as a massive block, but rather read one line at a time and you'll have no trouble understanding it. Once you grasp this piece of code, you'll be able to easily create your own code in no time.

```{r}
fancy_table = diamonds %>% 
  select(-x, -y, -z, -table) %>%
  # "c" means concatenate == use several categories
  filter(color == c("E", "I"), clarity == "SI2") %>%  
  mutate(price_carat = price / carat) %>% 
  rename(new_name = depth) %>% 
  group_by(cut, color, clarity) %>% 
  # remove NAs, otherwise you'll get NAs
  summarise(avg_price = mean(price, na.rm = TRUE),
            count     = n()) 

fancy_table
```

You can also name the resulting table and save it as an object for future use, by using the assignment operator, `=` or `<-`. It's cool, right? But what if you need even more descriptive stats, like standard error, median, IQR, skewness, and so on, for ALL numeric variables in your dataset, and for ALL of your groups simultaneously? Sounds like a lot of work, right? But hold on! You can actually do it with a single intuitive word, **describe**, which delivers **ALL the descriptive stats you can think of for ANY combination of groups and for EVERY variable in your dataset**. And that's just the tip of the iceberg of what the `dplyr` package can do. Want to know even more powerful functions? Check out my review of the `dlookr` package.

```{r eval=FALSE}
mtcars %>% 
  group_by(cyl, am) %>% 
  dlookr::describe(-cyl, -am)
```


```{r echo=FALSE}
library(flextable)
mtcars %>% 
  group_by(cyl, am) %>% 
  dlookr::describe(-cyl, -am) %>% 
  regulartable() 
```


# Conclusion

80% of working with data is pre-processing, such as cleaning, transforming, and wrangling. Although this work is rarely seen in the final result, like a model or a scientific paper, it is often the most important part. The phrase "Garbage in, garbage out" emphasizes the importance of having clean and accurate data. Making pre-processing intuitive and enjoyable can prevent wrong conclusions, boredom, and pain, and make the world a better place.

[^1]: Amazing and Free Book by Hadley Wickham: "R for Data Science" https://r4ds.had.co.nz/

These 10 verbs will cover most of your daily data manipulation needs, and make your code easy to read and understand for your colleagues and even for your future self. This simple, powerful, and clear data pre-processing will allow for the use of statistical and machine learning models, resulting in valuable inferences and predictions. In the era of big data, there are too many unused or chaotic data that goes to waste. Data is the most valuable resource in the 21st century, and it's important to handle it sustainably. Let's make it useful.

# What's next?


- [Data Wrangling Vol. 2: Â´The big 6Â´ and their babies](https://yury-zablotski.netlify.com/post/2019-09-22-data-wrangling-2/data-wrangling-2/)

---

If you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.

**Thank you for learning!**





