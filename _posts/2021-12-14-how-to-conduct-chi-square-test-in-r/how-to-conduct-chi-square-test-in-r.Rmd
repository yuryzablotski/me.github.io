---
title: "R demo | Chi-Square Test | how to conduct, visualize & interpret | + pairwise post-hoc tests"
description: |
  Chi-Square Test checks the independence between two categorical variables, where variables can have two or more categories. Need to do Chi-Square test? It can actually be done with only one line of code. There is no better way than {ggbarstats} function from {ggstatsplot} package ðŸ“¦. In this short blog-post you'll learn how to conduct, visualize and interpret Chi-Square test & pairwise post-hoc tests in R.
author:
  - name: Yury Zablotski
    url: https://yuzar-blog.netlify.app/
date: 12-20-2021
categories:
  - videos
  - statistics
preview: thumbnail.jpg
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
# draft: true
bibliography: /Users/zablotski/Documents/library.bib
#csl: american-political-science-association.csl
biblio-style: apalike
link-citations: yes
linkcolor: blue
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```


## This post as a video

I recommend to watch a video first, because I highlight things I talk about. It's ca. 5 minutes long.

```{r, eval=T, echo=F}
vembedr::embed_youtube("8Tj0-yMPO64")
```

## Previous topics

Understanding [hypothesis testing](https://yuzar-blog.netlify.app/posts/2021-06-04-my-second-blog-postthis-one-will-be-really-good/) and [p-values](https://yuzar-blog.netlify.app/posts/2021-07-31-p-value-intuitive-explanation/) would be very helpful. 

## How to conduct Chi-Squared test in R

If you have installed and loaded {ggstatsplot} package, you can use {ggbarstats} function to conduct and visualize Chi-Square test of independence between two categorical variables, where variables can have two or more categories. Within this function we need to specify only four arguments: 

- **our data**, e.g. let's take `mtcars`, which you already have in R, so you don't need to look for it
- **x** - as one of your categorical variables, for example *Transmission* of the car (with 0 being automatic, and 1 being manual transmission)
- **y** - would be your second variable, let's take the *Number of cylinders* (4, 6 or 8) and
- **the label** argument - which displays **both** numbers and percentages of observations in each category.

This simple command results in a statistically rich and publication ready plot! Now, let's interpret the results.


```{r eval=FALSE}
install.packages("ggstatsplot")
```

```{r}
library(ggstatsplot)

ggbarstats(
  data  = mtcars, 
  x     = am, 
  y     = cyl, 
  label = "both", 
)
```


## Interpretation

- **Chi-Square statistics** was previously used to manually calculate p-value, but nowadays, since p-values are always calculated by computers, we can safely ignore it

- **P-value** in our test can be seen as the probability of independence between two variables, low p-value (usually p < 0.05), like in our example, indicates that number of cylinders and transmission of cars *are dependent on each other*. In other words - *there is a relationship between them*.

Indeed, the plot shows that the number of cars using automatic transmission (am = 0) increases with increasing number of cylinders. The opposite is true for cars with manual transmission (am = 1), their frequency declines as number of cylinders increases.

So, we can conclude, that *the relationship between transmission and number of cylinders exists*. However, p-value doesn't say *how strong this relationship is*.

- That's why we have **V Cramer** value with its 95% confidence intervals as **the effect size** next to p-value. Our effect size of 0.46 indicates a **relatively strong relationship**, which supports the conclusion made by the p-value. The confidence intervals do not make much sense though, since *V Cramer* goes from 0 to 1 anyway.

![](v_cramer.png)

- However, `ggbarstats` also provides a second **Bayesian V Cramer effect size**, which delivers much more useful 95% Highest Density Intervals. The interpretation of the Bayesian effect size is the same, so the relationship between our variables is **relatively strong**.

- If that's not enough, we can look at the **Bayes Factor** (Jeffreys, 1961), which tests both null and alternative hypotheses at the same time. Bayes Factor of - 2.82 in our example indicates a **strong evidence for the alternative hypothesis** - that the relationship exists, which IS in line with the frequentists statistics on the top of the plot. 


![](bf_interpretation.png)


- We can also see **Proportion Tests** for transmissions in each cylinder. They show whether proportions inside every cylinder differ. Our Null Hypothesis ($H_0$) here is that there are equal proportions of different transmissions in a particular category of a cylinder. Which is the case for cylinders 4 and 6. While, our Alternative Hypothesis ($H_alt$) is that the proportions differ, which is the case for the cylinder 8.

## Pairwise Proportion Tests ... or post-hoc tests

If you find a significant relationship between variables and you have more then two categories in any of your variables, like in our example, you might be interested to compare proportions of cylinders with each other, namely 4 with 6, 4 with 8 and 6 with 8. Such simple pairwise comparisons is often called with an unnecessary fancy name - **post-hoc tests**.

The easiest was to make pairwise proportions tests is to use {pairwise_prop_test} function from {rstatix} package. Thus, first, install and load {rstatix} package, then use {table} function for a contingency table of your variables. And finally, simply apply {pairwise_prop_test} function to your contingency table. The results show two kinds of p-values, normal and adjusted for multiple comparisons. Always use the adjusted ones. So, we see that there is a significant association between cylinders 4 and 8, where cylinder 4 has more cars with manual transmission, while cylinder 8 has more cars with automatic transmission. 

```{r eval=FALSE}
install.packages("rstatix")
```


```{r}
library(rstatix)
```


```{r eval=FALSE}
contingency_table <- table(mtcars$cyl, mtcars$am)
contingency_table
pairwise_prop_test(contingency_table) 
```


```{r echo=F, message=F, warning=F}
library(flextable)
pairwise_prop_test(table(mtcars$cyl, mtcars$am)) %>% 
  regulartable() %>% 
  autofit()
```



## What's next?

- first of all, never forget to cite this amazing package!

```{r}
citation("ggstatsplot")
```


- learn more about [Chi-Squared Test](https://yury-zablotski.netlify.app/post/chi-square-2/)

- have a look at the [Chi-Squared Test Goodness of Fit](https://yury-zablotski.netlify.app/post/goodness-of-fit/)

---

If you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.

**Thank you for learning!**

## Further readings and watchings

- Jeffreys, H. 1961. Theory of Probability. 3rd ed. Oxford: Oxford University Press.








